{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ECS708A_Lab2_test.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"9DM79tIB-F9N","colab_type":"text"},"source":["# <font color='gray'> Lab 2: Classification I</font>"]},{"cell_type":"markdown","metadata":{"id":"lieJbcwd1GIp","colab_type":"text"},"source":["## Introduction \n","\n","The aim of this lab is to get experience with **classification problems**, the concepts of **optimization**, **regularization**, and **feature relevance**. \n","\n","- This lab constitutes your second course-work activity.\n","- A report answering the <font color = 'red'>**questions in</font><font color = \"maroon\"> red**</font> only should be submitted by the 22nd of March. \n","- The report should be a separate file in **pdf format** (so **NOT** *doc, docx, notebook* etc.), well identified with your name, student number, assignment number (for instance, Assignment 2), module code. \n","- No other means of submission other than the appropriate QM+ link is acceptable at any time (so NO email attachments, etc.)\n","- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n","\n"]},{"cell_type":"markdown","metadata":{"id":"eqztdP8iJKh9","colab_type":"text"},"source":["## 0. Classification I"]},{"cell_type":"code","metadata":{"id":"f1mLGgg_gHpv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hl7-UiW7JRZJ","colab_type":"text"},"source":["The\t point\t of\t this\texercise\tis\t to\t understand:\n","\n",">i)\t**classification problems**;<br>\n",">ii)\tthe concepts of **optimization** in training of a parametric model, specifically, `Logistic Regression (MaxEnt)`;<br>\n",">iii) introduction to a non-parametric classifier, specifically, `KNN`.\n"]},{"cell_type":"markdown","metadata":{"id":"FWjueQMB8Xzo","colab_type":"text"},"source":["## 1. `MaxEnt`/`Logistic-Regression` Classifier"]},{"cell_type":"markdown","metadata":{"id":"ByusQROHN0Af","colab_type":"text"},"source":["In the following exercises, we will examine some **Python** code for **KNN** and **MaxEnt** to get a deeper understanding of the concepts and the strengths and limitations of each of these two classifiers."]},{"cell_type":"markdown","metadata":{"id":"4TUFkLv0ETFa","colab_type":"text"},"source":["0.   Load the following datasets (they are available in QM+): `lab2_0_test.csv`, and `lab2_0_train.csv` (i.e., *test* and *training*). \n","  *Recall that you first need to upload them to this google colab VM from the pan  on the left (using `UPLOAD`).*"]},{"cell_type":"code","metadata":{"id":"Gh3yv2UM9_1b","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","df_train = pd.read_csv(\"lab2_0_train.csv\")\n","df_test = pd.read_csv(\"lab2_0_test.csv\")\n","\n","print('The first few rows of the train dataset (to get a feeling about the data):\\n')\n","print(df_train.head())\n","print('-'*50+'\\nSummary information about the train dataset:\\n')\n","print(df_train.info())\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRMdi4DUFFvA","colab_type":"text"},"source":[" 1. Let's plot the training dataset in the feature space (a.k.a. attribute space, or predictor space). Note that the `x` and `y` axes represent each of our two attributes (so we are dealing with a two-dimensional problem) and the class that each instance belongs to is denoted by a <font color = 'blue'>blue circle</font> (label 0) versus a <font color = 'red'>red cross</font> (label 1). Note that, of course, the choice of which class we label as 0 versus 1 is arbitrary, as long as we choose it once, and stick with it!"]},{"cell_type":"code","metadata":{"id":"g53l_hTG2Xz1","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","# %config InlineBackend.figure_format = 'retina'\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# split the data in df_train based on their class labels\n","df_train_Label_0 = df_train.loc[df_train['Label'] == 0]\n","df_train_Label_1 = df_train.loc[df_train['Label'] == 1]\n","\n","# plot the data\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","ax.scatter(df_train_Label_0['Attribute_1'], \n","            df_train_Label_0['Attribute_2'], \n","            c = 'b', marker = 'o', label = 'Label 0')\n","ax.scatter(df_train_Label_1['Attribute_1'], \n","            df_train_Label_1['Attribute_2'], \n","            c = 'r', marker = 'x', label = 'Label 1')\n","\n","# set a title and labels\n","ax.set_title('Plot of the $training$ dataset', fontsize = 18)\n","ax.set_xlabel(\"Attribute 1\", fontsize = 14)\n","ax.set_ylabel(\"Attribute 2\", fontsize = 14)\n","\n","# set dimensions of x and y axes \n","ax.set_xlim(-1, 1)\n","ax.set_ylim(-1, 1)\n","ax.grid(alpha=0.2)\n","\n","# set legend\n","ax.legend(fontsize=14)\n","\n","# show the plot\n","plt.show()\n","\n","\n","# # an alternative plotting library:\n","\n","# import altair as alt\n","\n","# df_train['Label']= df_train['Label'].astype('category')\n","\n","# alt.Chart(df_train).mark_point(size=60).encode(\n","#     x = 'Attribute_1', \n","#     y = 'Attribute_2', \n","#     shape = 'Label',\n","#     color = 'Label',\n","#     tooltip=['Attribute_1','Attribute_2'],\n","# ).interactive()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zmH2uUVatNv","colab_type":"text"},"source":["---\n","> **Q0:** Is the data linearly separable?"]},{"cell_type":"markdown","metadata":{"id":"PkmezLFsPF67","colab_type":"text"},"source":["> **A0:** (*Recall that you can use this space to write your answer by double-click on this cell. This is only for your own note-taking. The answers to the red exercises -- hence, **NOT** this one, for instance! -- should be written in a separate document and submitted through QM+ as a single **pdf** file, as for Lab 1.*) \n","---"]},{"cell_type":"markdown","metadata":{"id":"P_eh2coGa7ge","colab_type":"text"},"source":["### The basics of the `MaxEnt` classifier."]},{"cell_type":"markdown","metadata":{"id":"4Ml0Ufu8Xxzw","colab_type":"text"},"source":["Recall that a logistic regression makes use of the `logistic` function, which is also called `logistic sigmoid` or `expit` function:\n","$$\n","f(t) =\\frac{e^t}{e^t + 1} = \\frac{1}{1 + e^{-t}}\n","$$\n","It takes a real value $t$, which will be associated with the distance of a point to the decision boundary, and returns a value between 0 and 1, which can be interpreted as a probability.\n","\n","So let's see this function first! \n","\n","We can use the `expit` function from the scipy library:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"SA3fMHxaIlJE","colab_type":"code","colab":{}},"source":["from scipy.special import expit\n","\n","# generate evenly spaced numbers over a specified interval\n","t = np.linspace(-10, 10, 1000)\n","\n","# plot expit function of the generated data\n","plt.plot(t, expit(t), linewidth = 3)\n","\n","# set plot parameters\n","plt.title('Expit fuction (from the scipy library)', fontsize=16)\n","plt.xlabel('t', fontsize=16)\n","plt.ylabel('f(t)', fontsize=16)\n","plt.grid(alpha = 0.2)\n","plt.ylim(0, 1)\n","\n","# show the plot\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NeTVU1y1dQnQ","colab_type":"text"},"source":["Or we can even define it ourselves!\n"]},{"cell_type":"code","metadata":{"id":"Km-i7-lQjXH3","colab_type":"code","colab":{}},"source":["# define our own expit function\n","def my_expit(t):\n","  \"\"\"\n","  Description\n","  ============\n","  The expit function, also known as the logistic function, \n","  is defined as expit(x) = 1/(1+exp(-x)).\n","  \n","  Parameters\n","  ==========\n","  A numpy array\n","  \n","  Return\n","  ======\n","  A numpy array  \n","  \"\"\"\n","  return  1. / (1. + np.exp(-t))\n","\n","# generate evenly spaced numbers over a specified interval\n","t = np.linspace(-10, 10, 1000)\n","\n","# plot expit function of the generated data\n","plt.plot(t, my_expit(t), linewidth = 3)\n","\n","# set plot parameters\n","plt.title('Expit fuction (directly defined by ourselves)', fontsize=16)\n","plt.xlabel('t', fontsize=16)\n","plt.ylabel('f(t)', fontsize=16)\n","plt.grid(alpha = 0.2)\n","plt.ylim(0, 1)\n","\n","# show the plot\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fadGrphZp3-","colab_type":"text"},"source":["---\n","> **Q1:** Inspecting the plot, what is the value of the expit function at zero? What is its value at large positive inputs? What is its value at large negative inputs? Check your answer with its algebraic relation $f(x) = \\frac{1}{1 + e^{-x}}$."]},{"cell_type":"markdown","metadata":{"id":"QULjtn_ja4Vx","colab_type":"text"},"source":[">**A1:** \n","---"]},{"cell_type":"markdown","metadata":{"id":"jqGd5RSftVz0","colab_type":"text"},"source":["Now, recall how logistic regression uses the above logistic function: We have a vector of weights, $\\boldsymbol{w}=[w_0, w_1, w_2,\\ldots,w_p]$. These constitute the parameters of the model. \n","\n","Given a new instance with attributes $\\boldsymbol{x}=[1, x_1, x_2, \\ldots, x_p]$, we first compute the inner product of the two, which is a real number, and pass it to the logistic function:\n","\n","$$\n","Pr(label = 1) = \\frac{1}{1+e^{-\\boldsymbol{w}^T.\\boldsymbol{x}}} = \\frac{1}{1+e^{-[w_0+w_1x_1+\\ldots+w_px_p]}}\n","$$\n","\n","\n","The following is the implementation of this function:\n","\n"]},{"cell_type":"code","metadata":{"id":"muM07y3kILXN","colab_type":"code","colab":{}},"source":["  def logisticRegressionBinaryClassifier(X, w):\n","    \"\"\"\n","    Description\n","    ===========\n","    The logistic regression for a binary classifier.\n","    \n","    Parameters\n","    ==========\n","    X: A matrix containing the sample data.\n","       The first column should be all ones.\n","    w: weights of each attributes\n","    \n","    Return\n","    ======\n","    A numpy array\n","    \"\"\"\n","    return expit(np.matmul(X, w.T))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hz9WTLBcXS7i","colab_type":"text"},"source":["In our example, we are dealing with a 2-dimensional problem, since we have only two attributes. Therefore, a logistic classifier is specified by three parameters: $w_0$, $w_1$ and $w_2$:\n","\n","- $w_0$ is the intercept;\n","- $w_1$ is the weight of attribute 1;\n","- $w_2$ is the weight of attribute 2.\n","\n","which we put in an array of weights $\\boldsymbol{w}$, i.e., $\\boldsymbol{w} = [w_0, w_1, w_2]$.\n"]},{"cell_type":"markdown","metadata":{"id":"fQzAkWGt1FfS","colab_type":"text"},"source":["### Visualising the predictions of a given `MaxEnt` classifier."]},{"cell_type":"markdown","metadata":{"id":"7_fk4ppweYsB","colab_type":"text"},"source":["Note that the output of the logistic function is not directly a class label, but rather a **probability**. But how can we visualise this probability?\n","\n","\n","We are going to use colour: the background colour shows the probability that our classifier assigns to that point in the attribute space (a.k.a. feature space, or predictor space). The colour bar next to figure shows what value each colour represents. \n","\n","\n","The following code shows the output for an example values of weights.\n","**Note** that the current values chosen for the weights of the classifier is terrible! Because it is predicting red for the samples from the blue class (circles), and blue for the ones from the red class (crosses)!. \n","\n","*Note: If you have an issue identifying colours, please contact me (i.e., the instructor).*\n","\n"]},{"cell_type":"code","metadata":{"id":"64T9lVrml4W2","colab_type":"code","colab":{}},"source":["# generate evenly spaced numbers over a specified interval\n","att1 = np.linspace(-1, 1, 100)\n","att2 = np.linspace(-1, 1, 100)\n","\n","# weights needed to calculate the probability that the label of a given sample is 1\n","# to have a good classifier, we need to find good weights\n","# weights -> (w0, w1, w1)\n","# probability = 1 / (1 + e ^ -(w0 + w1*x1 + w2*x2))\n","weights = np.array([0, -5, 0])\n","\n","# generate the meshgrid of generated numbers\n","att1, att2 = np.meshgrid(att1, att2)\n","\n","# create a matrix of possible values for x1 and x2\n","Xfull = np.c_[att1.ravel(), att2.ravel()]\n","\n","# the first column of matrix is multiplied by w0, so it should be 'one' for all\n","Xfull = np.c_[np.ones(Xfull.shape[0]), Xfull]\n","\n","# find the probability for each combination of x1 and x2 \n","probas = logisticRegressionBinaryClassifier(Xfull, weights)\n","\n","# draw the color map for the calculated probabilities\n","# (this is added on top of the previous figure, which was showing the samples)\n","imshow_handle = ax.imshow(probas.reshape((100, 100)), \n","                          cmap = 'jet', \n","                          interpolation='none', \n","                          extent = (-1, 1, -1, 1), \n","                          origin = 'lower', \n","                          alpha = 0.9)\n","# ax.contour(att1, att2, probas.reshape((100, 100)), levels=5)\n","\n","# draw the color bar if it has not been drawn yet\n","if len(fig.axes) < 2: \n","  cbar = fig.colorbar(imshow_handle, ax = ax, orientation = 'vertical')\n","  cbar.minorticks_on()\n","\n","fig"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7jV_sabyfBbV","colab_type":"text"},"source":["---\n","### <font color='maroon'>**Exercise 1:** Try (by varying the values of weights in the code), and describe the effect of the following actions on the classifier (with a brief explanation): <ins>[0.5 mark]</ins>\n","- changing the first weight dimension, i.e., $w_0$?\n","- changing the second two parameters, $w_1$, $w_2$?\n","- negating all the weights?\n","- scaling the second two weights up or down (e.g. dividing or multiplying both $w_1$ and $w_2$ by 10)?\n","  </font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"F_6vM7GJhn45","colab_type":"text"},"source":["> **Q3:** Knowing the above, can you find a weight array that makes a reasonably good classifier for this dataset?"]},{"cell_type":"markdown","metadata":{"id":"9GRq76YThxHk","colab_type":"text"},"source":["> **A3:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"MJPTB0oZ2N4h","colab_type":"text"},"source":["The following code computes the accuracy of the predictor (specified by a given weight) when applied to some labelled samples: "]},{"cell_type":"markdown","metadata":{"id":"vXcQ7BuA3mif","colab_type":"text"},"source":["---\n","> **Q4:** Recall that the output of the logistic function is a probability. But in order to compute the accuracy, we need the predicted labels (to compare with the actual labels). How can we convert the probability to a label? \n","First, think of the answer yourself, then, inspecting the following code, find out which line is doing that.\n"]},{"cell_type":"markdown","metadata":{"id":"OoG1gTp84oWL","colab_type":"text"},"source":["> **A4:**\n","---"]},{"cell_type":"code","metadata":{"id":"0VmwAqNVS-T3","colab_type":"code","colab":{}},"source":["def computeAccuracy(X, w, Y):\n","  \"\"\"\n","  Description\n","  ===========\n","  Compute the accuracy of the given weight (w).\n","  \n","  Parameters\n","  ==========\n","  X: A matrix containing the sample data.\n","     The first column should be all ones.\n","  w: Weights (w0, w1, w2)\n","  Y: An array of corresponding labels of each sample in 'X'.\n","  \n","  Return\n","  ======\n","  The accuracy percentage of a given weight.\n","  \"\"\"\n","  \n","  # compute the logistic regression of a given weight and samples\n","  prediction_probab = logisticRegressionBinaryClassifier(X, w)\n","  \n","  # convert probabilities to a binary class\n","  threshold = 0.5\n","  prediction_binary = [int(probab > threshold) for probab in prediction_probab]\n","\n","  # compute the accuracy of the given weights with comparing\n","  # predicted classes to actual ones.\n","  number_correct_classified_instances = 0\n","  for index in range(len(Y)):\n","    if prediction_binary[index] == Y[index]:\n","      number_correct_classified_instances += 1\n","  return number_correct_classified_instances/len(Y)\n","    \n","\n","# Prepare sample data for computing the accuracy of the weight\n","Xtrain = df_train[['Attribute_1', 'Attribute_2']].values\n","Xtrain = np.c_[np.ones(Xtrain.shape[0]), Xtrain]\n","df_train_labels = df_train['Label'].values \n","\n","weights = np.array([0, -5, 0])\n","print('Train accuracy 1: {:.2f}'.format(computeAccuracy(Xtrain, weights, df_train_labels)))\n","\n","# this is for the exercise\n","print('Train accuracy 2: {:.2f}'.format(computeAccuracy(Xtrain, -weights, df_train_labels)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U6vh5C6q25Di","colab_type":"text"},"source":["---\n","> **Q5:** Note that there are two outputs printed. What are them? Why did the accuray increase?"]},{"cell_type":"markdown","metadata":{"id":"ZrrZf0jo5nEU","colab_type":"text"},"source":[">**A5:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"S0by0xbl5UyQ","colab_type":"text"},"source":["### Training the logistic regression model: Optimisation - Exhaustive Search"]},{"cell_type":"markdown","metadata":{"id":"dQS_k3NpTR1h","colab_type":"text"},"source":["In this section, we are going to investigate how training takes place, i.e., how we find the best weights of the logistic regression model. "]},{"cell_type":"markdown","metadata":{"id":"Kyrn437z_xot","colab_type":"text"},"source":["Recall from the class that a suitable measure of goodness of a classifier that returns probabilities (like logistic regression) is the likelihood function:\n","\n","$$\n","L(\\boldsymbol{w})=\\prod_{y_i=0}\\left(1-p(\\boldsymbol{x}_i; \\boldsymbol{w})\\right) \\prod_{y_i=1}p(\\boldsymbol{x}_i; \\boldsymbol{w})\n","$$\n","\n","This is the likelihood that the labels in the training dataset is produced if the model is the true model (assuming instances are independent). Intuitively, if the true label is 1, we have a term $p(x)$ and if the true label is zero, we have a term of $1-p(x)$. So the best model has the highest value for the likelihood function.\n","\n","To avoid prevision errors (since we are multiplying many probabilities), and also get a better behaving function, we take the (negative) of the log of the above product, which is called the `negative-log-likelihood` function:\n","\n","$$\n","-\\log L(\\boldsymbol{w})=-\\sum_{y_i=0}\\log\\left(1-p(\\boldsymbol{x}_i; \\boldsymbol{w})\\right) -\\sum_{y_i=1}\\log p(\\boldsymbol{x}_i; \\boldsymbol{w})\\\\\n","= -\\sum_{y_i}[(1-y_i)\\log\\left(1-p(\\boldsymbol{x}_i; \\boldsymbol{w})\\right) +y_i\\log p(\\boldsymbol{x}_i; \\boldsymbol{w})]\n","$$\n","\n","Note that the \"negative log likelihood\" is now a \"loss\" function, so  for the best model, we have to minimise it. "]},{"cell_type":"markdown","metadata":{"id":"sBtuC2QyH1wB","colab_type":"text"},"source":["The following function computes the negative-log-likelihood given the weight parameters of a logistic regression and a labelled dataset. (There is a also a LAMBDA for regularisation!)"]},{"cell_type":"code","metadata":{"id":"wqkWbvMup9Rm","colab_type":"code","colab":{}},"source":["def negLogLikelihood(X, w, Y, LAMBDA = 0.01):\n","  \"\"\"\n","  Description\n","  ===========\n","  Compute negative log likelihood of a dataset under logistic regression \n","  classifier.\n","  \n","  Parameters\n","  ==========\n","  X: A matrix containing the sample data.\n","     The first column should be all ones.\n","  w: Weights, e.g. w = [w0, w1, w2]\n","  Y: An array of corresponding labels of each sample in 'X'.\n","  LAMBDA: coefficient of the 'regularisation' penalty. \n","         Default value is set to '0.01'.\n","\n","  Return\n","  ======\n","  neg_log_likelihood: it is the loss value of the current classifier specified\n","  by a weight vector and given labelled dataset.\n","  \"\"\"\n","  \n","  prediction_probab = logisticRegressionBinaryClassifier(X, w)\n","  neg_log_likelihood = -np.sum(Y * np.log(prediction_probab) +\n","                             (1 - Y) * np.log(1 - prediction_probab)) + LAMBDA * np.matmul(w, w.T)\n","  \n","  return neg_log_likelihood"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FFDZFNStI0-w","colab_type":"text"},"source":["Let's try it on our training dataset for an example weights:"]},{"cell_type":"code","metadata":{"id":"X_w_ozVRJs9Y","colab_type":"code","colab":{}},"source":["Xtr = df_train[['Attribute_1','Attribute_2']].values\n","Xtr = np.c_[np.ones(Xtr.shape[0]), Xtr]\n","Ytr = df_train['Label'].values\n","\n","weights = np.array([0, -5, 0]) # weights of the logistic regression classifier\n","\n","print('Negative Log Likelihood 1: {:.3f}'.format(negLogLikelihood(Xtr, weights, Ytr)))\n","print('Negative Log Likelihood 2: {:.3f}'.format(negLogLikelihood(Xtr, -weights, Ytr)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-LMI0eWGJ5Gr","colab_type":"text"},"source":["---\n","> **Q6:** Note that there are two outputs printed. What are they? How and why has the value chanegd? "]},{"cell_type":"markdown","metadata":{"id":"dNHxeClTJ22B","colab_type":"text"},"source":["> **A6:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"d2PXbGUaMlS4","colab_type":"text"},"source":["So far we have a loss function (negative-log-likelihood) that we can use to find the best weights of our model. But how? A trivial approach is  to\n","implement an exhaustive search algorithm, that tries different combinations of the weights and finds the one that gives the lowest value of the loss function:"]},{"cell_type":"code","metadata":{"id":"_XU5cG5gPPW6","colab_type":"code","colab":{}},"source":["from itertools import product\n","\n","Xtr = df_train[['Attribute_1','Attribute_2']].values\n","Xtr = np.c_[np.ones(Xtr.shape[0]), Xtr]\n","Ytr = df_train['Label'].values\n","\n","\n","lw0 = np.linspace(-1, 1, 20)\n","lw1 = np.linspace(-1, 1, 20)\n","lw2 = np.linspace(-1, 1, 20)\n","\n","all_possible_weights = product(lw0, lw1, lw2)\n","lowest_loss = np.Infinity\n","\n","np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n","for i, weights in enumerate(all_possible_weights):\n","  current_loss = negLogLikelihood(Xtr, np.array(weights), Ytr)\n","  if current_loss < lowest_loss:\n","    best_weights = np.array(weights)\n","    lowest_loss = current_loss\n","    print('a new best found = {:8.5f} --- '.format(current_loss), end='')\n","    print('accuracy: {:.2f} --- '.format(computeAccuracy(Xtr, best_weights, Ytr)), end='')\n","    print('weights = {}'.format(best_weights))\n","\n","    \n","print('\\nThe best weight vector found using exhaustive search is: {}'.format(best_weights))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"23SLUYLnNjl0","colab_type":"text"},"source":["---\n","### <font color='maroon'>**Exercise 2:** (a) How many times did your loop execute? (b) Report your classifier weights that first get you at least 75% train accuracy. <ins>[0.25 + 0.25 = 0.5 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"dmaSMAsUIEEX","colab_type":"text"},"source":["---\n",">**Q7:** For a challenging question: You should notice that there is a consensus between the general trend in the loss function and the accuracy measure (both are improving). But there are some small deviations from this general trend: there are cases where the loss function improves but not the accuracy, although both of them are measured on the train data. Can you provide any justification why this could happen at all?"]},{"cell_type":"markdown","metadata":{"id":"QXkpwFmDJDpB","colab_type":"text"},"source":[">**A7:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"DMh-qUwAT45y","colab_type":"text"},"source":["In order to (literally!) see the progress of how the exhaustive search finds the best weights, we are going to make an animation. Execute the following code, but keep in mind that it may take up to a minute to finish baking your animation!"]},{"cell_type":"code","metadata":{"id":"laCtDpKkPh0I","colab_type":"code","colab":{}},"source":["%matplotlib notebook\n","\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","import matplotlib.animation as animation\n","import matplotlib.gridspec as gridspec\n","\n","\n","\n","att1 = np.linspace(-1, 1, 50)\n","att2 = np.linspace(-1, 1, 50)\n","\n","att1, att2 = np.meshgrid(att1, att2)\n","\n","Xfull = np.c_[att1.ravel(), att2.ravel()]\n","Xfull = np.c_[np.ones(Xfull.shape[0]), Xfull]\n","\n","# plt.style.use('classic')\n","\n","lw0 = np.linspace(-2, 2, 4)\n","lw1 = np.linspace(-2, 2, 4)\n","lw2 = np.linspace(-2, 2, 4)\n","\n","\n","all_possible_weights = [np.array(w) for w in product(lw0, lw1, lw2)]\n","figg = plt.figure(figsize=(15, 15))\n","# figg = plt.figure(constrained_layout=True)\n","\n","axx0 = figg.add_subplot(231)\n","axx1 = figg.add_subplot(232)\n","axx2 = figg.add_subplot(233)\n","axx3 = figg.add_subplot(212)\n","\n","weights_bar = axx0.bar(['w0', 'w1', 'w2'],[lw0[-1], lw1[-1], lw2[-1]])\n","axx0.set_ylim((-6.1,6.1))\n","axx1.scatter(df_train_Label_0['Attribute_1'], \n","            df_train_Label_0['Attribute_2'], \n","            c = 'b', marker = 'o', )\n","axx1.scatter(df_train_Label_1['Attribute_1'], \n","            df_train_Label_1['Attribute_2'], \n","            c = 'r', marker = 'x', )\n","axx1.set_xlim(-1, 1)\n","axx1.set_ylim(-1, 1)\n","axx1.grid(alpha = 0.2)\n","probas = logisticRegressionBinaryClassifier(Xfull, np.array([lw0[-1], lw1[-1], lw2[-1]]))\n","im_handle = axx1.imshow(probas.reshape(att1.shape), cmap='jet', \n","                          interpolation='none', extent=(-1, 1, -1, 1), \n","                          origin='lower', alpha=0.9)\n","\n","line, = axx2.plot([], [], linewidth=1, marker='.', )\n","loss_measures = []\n","axx2.grid()\n","\n","axx2.set_xlim((0, len(all_possible_weights)))\n","axx2.set_ylim((0, 200))\n","\n","\n","axx3.set_aspect('equal')\n","axx3.scatter(df_train_Label_0['Attribute_1'], \n","            df_train_Label_0['Attribute_2'], \n","            c = 'b', marker = 'o', )\n","axx3.scatter(df_train_Label_1['Attribute_1'], \n","            df_train_Label_1['Attribute_2'], \n","            c = 'r', marker = 'x', )\n","im_handle_of_best = axx3.imshow(probas.reshape(att1.shape), cmap='jet', \n","                          interpolation='none', extent=(-1, 1, -1, 1), \n","                          origin='lower', alpha=0.9)\n","\n","lowest_loss = np.Infinity\n","best_weights = np.array([lw0[-1], lw1[-1], lw2[-1]])\n","\n","textstr = '\\n'.join((\n","    r'$loss=%.2f$' % (lowest_loss,),\n","    r'$weights=[%.2f,%.2f,%.2f]$' % (best_weights[0], best_weights[1], best_weights[2]),\n","    r'$Train accuracy=%.2f$' % (0), ))\n","\n","# these are matplotlib.patch.Patch properties\n","props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n","\n","# place a text box in upper left in axes coords\n","textbox3 = axx3.text(1.05, 0.95, textstr, transform=axx3.transAxes, fontsize=14,\n","        verticalalignment='top', bbox=props)\n","\n","\n","def update_weights(weights):\n","  global lowest_loss, best_weights\n","  # updating the weights bar plot:\n","  for i, bar in enumerate(weights_bar):\n","    bar.set_height(weights[i])\n","    \n","  # updating the probability colormap plot:\n","  Yfull = np.matmul(weights, Xfull.T)\n","  probas = my_expit(Yfull)\n","  im_handle.set_data(probas.reshape(att1.shape))\n","  \n","  # updating the loss measures plot\n","  current_loss = negLogLikelihood(Xtr, weights, Ytr)\n","  loss_measures.append(current_loss)\n","  line.set_data(range(len(loss_measures)), loss_measures)\n","  \n","  # update the best classifier so far:\n","  if current_loss < lowest_loss:\n","    best_weights = weights\n","    lowest_loss = current_loss\n","    im_handle_of_best.set_data(probas.reshape(att1.shape))\n","    textstr = '\\n'.join((\n","    r'$loss=%.2f$' % (lowest_loss,),\n","    r'$weights=[%.2f,%.2f,%.2f]$' % (best_weights[0], best_weights[1], best_weights[2]),\n","    r'$Train\\ accuracy=%.2f$%%' % (computeAccuracy(Xtrain, best_weights, df_train_labels)), ))\n","    textbox3.set_text(textstr)\n","    \n","\n","print('Please wait while the animation is being rendered ... ')\n","anim = animation.FuncAnimation(figg, update_weights, \n","                               frames=all_possible_weights,\n","                               interval =100,\n","                               repeat=False,\n","                               blit=False)\n","\n","HTML(anim.to_html5_video())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XmY1eIASWdW2","colab_type":"text"},"source":["---\n","> **Q7:** Describe what each of these 4 figures represent."]},{"cell_type":"markdown","metadata":{"id":"gi2M7yYXWzBZ","colab_type":"text"},"source":[">**A7:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"-KT1m6-TW3-T","colab_type":"text"},"source":["---\n","> **Q8:** Explain why in the curve in the third figure is so jagged and jumps up and down!"]},{"cell_type":"markdown","metadata":{"id":"WcUHDj4NYCJU","colab_type":"text"},"source":["> **A8:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"8uCb2IHbYQKD","colab_type":"text"},"source":["---\n",">**Q9:** How many different weights does this exhaustive search try here? Try to improve the output of the algorithm by increasing the number of possible weights. Note that this will effect your running time."]},{"cell_type":"markdown","metadata":{"id":"_qEC4RzsZCOE","colab_type":"text"},"source":["> **A9:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"CaHAqrJbYPkg","colab_type":"text"},"source":["Let's compute te accuracy of the best found model:"]},{"cell_type":"code","metadata":{"id":"0pS7K8ry9xeV","colab_type":"code","colab":{}},"source":["# Accuracy of the computed optimized weights on the testing dataset.\n","Xtest = df_test[['Attribute_1', 'Attribute_2']].values\n","Xtest = np.c_[np.ones(Xtest.shape[0]), Xtest]\n","df_test_labels = df_test['Label'].values\n","print('Test accuracy: {:.2f}'.format(computeAccuracy(Xtest, best_weights, df_test_labels)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"boVhFlYscOT6","colab_type":"text"},"source":["### Training the logistic regression model: Optimisation - Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"W9R1jhZGKzFZ","colab_type":"text"},"source":["The previous algorithm is inefficient. Let's look for a better way to optimise the classifier. In particular, instead of just try all different values of weights blindly, we are going to do the following:\n","\n","- start from an initial value of the weights;\n","- then in each step, update the weights along the direction ``suggested by'' the derivative (technically, the gradient) of the loss function.  \n","- stop depending on some stopping criteria (will be part of your questions!)"]},{"cell_type":"markdown","metadata":{"id":"4RWMZ_cqrvcg","colab_type":"text"},"source":["So we need a function that computes the gradient of our loss function at a given point. Here is it:"]},{"cell_type":"code","metadata":{"id":"U4-dIKS_abpj","colab_type":"code","colab":{}},"source":["def negLogLikelihoodGradient(X, w, Y, LAMBDA = 0.01):\n","  \n","  \"\"\"\n","  Description\n","  ===========\n","  Compute the gradient of the negative log likelihood loss function for a\n","  dataset under logistic regression classifier.\n","  \n","  Parameters\n","  ==========\n","  X: A matrix containing the sample data.\n","     The first column should be all ones.\n","  w: Weights e.g. w = [w0, w1, w2]\n","  Y: An array of corresponding labels of each sample in 'X'.\n","  LAMBDA: coefficient of the 'regularisation' penalty. \n","         Default value is set to '0.01'.\n","\n","  Return\n","  ======  \n","  gradient_neg_log_likelihood: it is the gradient of the loss function with\n","  respect to the weights, i.e., the vector of d[p(X,Y|w)]/dw, computed at a \n","  given w for a given training dataset X.\n","  \"\"\"\n","    \n","  prediction_probab = logisticRegressionBinaryClassifier(X, w)\n","  \n","  Xprob = Y - prediction_probab\n","  gradient_neg_log_likelihood = -np.matmul(X.T, Xprob) + 2. * LAMBDA * w\n","  \n","  \n","  return gradient_neg_log_likelihood\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wO7zGwI72t2q","colab_type":"text"},"source":["Here is an example of computing the gradient:"]},{"cell_type":"code","metadata":{"id":"smiP6oYz2o2W","colab_type":"code","colab":{}},"source":["weights = np.array([0, -1, 1])\n","\n","# Prepare sample data for computing the accuracy of the weight\n","Xtrain = df_train[['Attribute_1', 'Attribute_2']].values\n","Xtrain = np.c_[np.ones(Xtrain.shape[0]), Xtrain]\n","df_train_labels = df_train['Label'].values  \n","\n","negLL = negLogLikelihood(Xtrain, weights, df_train_labels)\n","gradientLL = negLogLikelihoodGradient(Xtrain, weights, df_train_labels)\n","print ('loss function (negative-log-likelihood): {:.2f}'.format(negLL))\n","print ('gradient of the loss function:\\t\\t {}'.format(gradientLL))\n","print('Train accuracy:\\t\\t\\t\\t {:.2f}'.format(computeAccuracy(Xtrain, weights, df_train_labels)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KtUJtQ9120Qs","colab_type":"text"},"source":["Now, lets update the weights using the gradient:"]},{"cell_type":"code","metadata":{"id":"gwUv7J_M22Ns","colab_type":"code","colab":{}},"source":["step_size = 0.1\n","\n","# update the weights by moving a small step in the opposite direction of \n","# the gradient:\n","weights = weights - step_size * gradientLL\n","\n","negLL = negLogLikelihood(Xtrain, weights, df_train_labels)\n","gradientLL = negLogLikelihoodGradient(Xtrain, weights, df_train_labels)\n","\n","print ('loss function (negative-log-likelihood): {:.2f}'.format(negLL))\n","print ('gradient of the loss function at this new point: {}'.format(gradientLL))\n","\n","print('Train accuracy: {:.2f}'.format(computeAccuracy(Xtrain, weights, df_train_labels)))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"USQSamJ3Fmw5","colab_type":"text"},"source":["---\n","> **Q10:** Compare the new value of the loss function with its value in the previous step. Describe the change and explain why this happened."]},{"cell_type":"markdown","metadata":{"id":"sKBW98tRGcMK","colab_type":"text"},"source":[">**A10:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"lggX9dbSLcnk","colab_type":"text"},"source":["---\n","### <font color='maroon'>**Exercise 3:** (a) Notice that we used the negative of the gradient to update the weights. Explain why. (b) Given the gradient at the new point, determine (in terms of \"increase\" or \"decrease\", no need to give values), what we should do to each of $w_0$, $w_1$ and $w_2$ to further improve the loss function. <ins>[0.5 + 0.5 = 1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"uK2WIDQRE_7h","colab_type":"text"},"source":["---\n","### <font color='maroon'>**Exercise 4:** Suppose we wanted to turn this step into an algorithm that sequentially takes such steps to get to the optimal solution. What is the effect of \"step-size\"? Your answer should be in terms of the trade-offs, i.e., pros and cos, in choosing a big value for step-size versus a small value for step-size. <ins>[0.25 + 0.25 = 0.5 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"dhtmwrzpKpac","colab_type":"text"},"source":["---\n","### <font color='maroon'>**Exercise 5:** Answer this question either by doing a bit of research, or looking at some code, or thinking about them yourself: (a) Argue why choosing a constant value for `step_size`, whether big or small, is not a good idea. A better idea should be to change the value of step-size along the way. Explain whether the general rule should be to decrease the value of step-size or increase it. (b) Come up with the stopping rule of the gradient descent algorithm which was described from a high level at the start of this section. That is, come up with (at least two) rules that if either one of them is reached, the algorithm should stop and report the results. <ins>[0.5 + 0.5 = 1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"1F7G-UKz5Jmp","colab_type":"text"},"source":["Instead of implementing the gradient descent algorithms ourselves, we are going to use the `minimize` function from the `optimization` module implemented as part of the `scipy` package. It takes a (callable) loss function, its gradient (also as a callable function, and an initial solution, along with some other parameters, like tolerance, and outputs the final solution found as optimal. "]},{"cell_type":"code","metadata":{"id":"mbDyDHdw5MCf","colab_type":"code","colab":{}},"source":["import scipy.optimize\n","np.set_printoptions(formatter={'float': '{: 0.10f}'.format})\n","\n","\n","# Initial guess of weights\n","initial_weights = np.array([1, -1, 1])\n","\n","# Prepare sample data for computing the accuracy of the weight\n","Xtrain = df_train[['Attribute_1', 'Attribute_2']].values\n","Xtrain = np.c_[np.ones(Xtrain.shape[0]), Xtrain]\n","df_train_labels = df_train['Label'].values\n","\n","# The loss function to be minimized (the objective function)\n","def obj_func(w):\n","  return negLogLikelihood(Xtrain, w, df_train_labels)\n","\n","# The gradient of loss function.\n","def grad_func(w):\n","  return negLogLikelihoodGradient(Xtrain, w, df_train_labels)\n","\n","# In order to store the path that the optimisation takes to get to the optimal \n","# point, we create the following callback function\n","# source for this one: \n","# http://louistiao.me/notes/visualizing-and-animating-optimization-algorithms-with-matplotlib/\n","def make_minimize_cb(path=[]):\n","    \n","    def minimize_cb(xk):\n","        # note that we make a deep copy of xk\n","        path.append(np.copy(xk))\n","\n","    return minimize_cb\n","  \n","path_ = [initial_weights] # putting the first solution here.\n","\n","# use scipy to calculate the optimal values for weights.\n","\n","# use scipy to calculate the optimal values for weights.\n","# the intermediate points will be appended to the path\n","res = scipy.optimize.minimize(fun = obj_func, x0 = initial_weights, \n","                              jac = grad_func, tol=1e-6, \n","                              callback=make_minimize_cb(path_))\n","\n","path = np.array(path_)[:,1:].T\n","\n","\n","print('computed optimal weights:\\t\\t {}'.format(res.x))\n","\n","negLL = negLogLikelihood(Xtrain, res.x, df_train_labels)\n","gradientLL = negLogLikelihoodGradient(Xtrain, res.x, df_train_labels)\n","print ('optimal loss (negative-log-likelihood):\\t {:.4f}'.format(negLL))\n","print ('gradient at the optimum point:\\t\\t {}'.format(gradientLL))\n","print('Train accuracy:\\t\\t\\t\\t {:.2f}'.format(computeAccuracy(Xtrain, res.x, df_train_labels)))\n","\n","# Accuracy of the computed optimized weights on the testing dataset.\n","Xtest = df_test[['Attribute_1', 'Attribute_2']].values\n","Xtest = np.c_[np.ones(Xtest.shape[0]), Xtest]\n","df_test_labels = df_test['Label'].values\n","print('Test accuracy:\\t\\t\\t\\t {:.2f}'.format(computeAccuracy(Xtest, res.x, df_test_labels)))\n","\n","\n","print('\\n\\nAnd here are all the intermediate solutions the algorithm visited to get to the optimal solution:\\n')\n","for w in path_:\n","  print('[w_0, w_1, w_2] = {}'.format(w))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kIHyOJr1ESae","colab_type":"text"},"source":["---\n","> **Q11:** Note that the above code also prints -- in the 3rd line -- the value of the gradient at the final \"optimal\" point.  What do you expect this gradient to be, if the point is truly the optimal point? Does it match what you expect? "]},{"cell_type":"markdown","metadata":{"id":"Yua_SGMBQT0C","colab_type":"text"},"source":[">**A11:** \n","---"]},{"cell_type":"markdown","metadata":{"id":"ogpZgaorTVvr","colab_type":"text"},"source":["---\n","> **Q12:** Note that the `minimize` function (line 39 of the code), takes a parameter called `tol` as well. What is used for? Try different meaningful values for it and describe the effect."]},{"cell_type":"markdown","metadata":{"id":"aQPCyOUzTfA-","colab_type":"text"},"source":[">**A12:** \n","---"]},{"cell_type":"markdown","metadata":{"id":"e0gsKbwVTaSs","colab_type":"text"},"source":["---\n","> **Q13:** The `minimize` function takes an initial value as the starting point of the optimization, i.e., an initial guess for the solution, as well (passed as `x0` argument). Try different values for it and describe the effect."]},{"cell_type":"markdown","metadata":{"id":"UNdK5P_cTgbk","colab_type":"text"},"source":[">**A13:** \n","---"]},{"cell_type":"markdown","metadata":{"id":"ro-ZABbkQamz","colab_type":"text"},"source":["Now, in order to visually see the progress made along the way using the gradient descent algorithm, and get a better comparison, let's recreate the animation we did for the exhaustive search approach:"]},{"cell_type":"code","metadata":{"id":"bNgeqvYKJzxz","colab_type":"code","colab":{}},"source":["%matplotlib notebook\n","\n","import matplotlib.pyplot as plt\n","\n","att1 = np.linspace(-1, 1, 50)\n","att2 = np.linspace(-1, 1, 50)\n","\n","att1, att2 = np.meshgrid(att1, att2);\n","\n","Xfull = np.c_[att1.ravel(), att2.ravel()]\n","Xfull = np.c_[np.ones(Xfull.shape[0]), Xfull]\n","\n","figg = plt.figure(figsize=(15, 15))\n","\n","axx0 = figg.add_subplot(231);\n","axx1 = figg.add_subplot(232);\n","axx2 = figg.add_subplot(233);\n","axx3 = figg.add_subplot(212);\n","\n","initial_weights = np.array([1,1,1]);\n","path_ = [initial_weights] \n","res = scipy.optimize.minimize(fun = obj_func, x0 = initial_weights, \n","                              jac = grad_func, tol=1e-6, \n","                              callback=make_minimize_cb(path_))\n","\n","\n","weights_bar = axx0.bar(['w0', 'w1', 'w2'], initial_weights);\n","axx0.set_ylim((-6.1,6.1));\n","axx1.scatter(df_train_Label_0['Attribute_1'], \n","            df_train_Label_0['Attribute_2'], \n","            c = 'b', marker = 'o', );\n","axx1.scatter(df_train_Label_1['Attribute_1'], \n","            df_train_Label_1['Attribute_2'], \n","            c = 'r', marker = 'x', );\n","axx1.set_xlim(-1, 1);\n","axx1.set_ylim(-1, 1);\n","axx1.grid(alpha = 0.2);\n","probas = logisticRegressionBinaryClassifier(Xfull, initial_weights)\n","im_handle = axx1.imshow(probas.reshape(att1.shape), cmap='jet', \n","                          interpolation='none', extent=(-1, 1, -1, 1), \n","                          origin='lower', alpha=0.9);\n","\n","line, = axx2.plot([], [], linewidth=1, marker='.', );\n","loss_measures = []\n","axx2.grid();\n","\n","axx2.set_xlim((0, len(all_possible_weights)));\n","axx2.set_ylim((0, 200));\n","\n","\n","axx3.set_aspect('equal');\n","axx3.scatter(df_train_Label_0['Attribute_1'], \n","            df_train_Label_0['Attribute_2'], \n","            c = 'b', marker = 'o', );\n","axx3.scatter(df_train_Label_1['Attribute_1'], \n","            df_train_Label_1['Attribute_2'], \n","            c = 'r', marker = 'x', );\n","im_handle_of_best = axx3.imshow(probas.reshape(att1.shape), cmap='jet', \n","                          interpolation='none', extent=(-1, 1, -1, 1), \n","                          origin='lower', alpha=0.9);\n","\n","lowest_loss = np.Infinity\n","best_weights = initial_weights\n","\n","# the values here are just dummy place-holders!\n","textstr = '\\n'.join((\n","    r'$loss=%.2f$' % (np.Inf,),\n","    r'$weights=[%.2f,%.2f,%.2f]$' % (0, 0, 0),\n","    r'$Train accuracy=%.2f$' % (0), ));\n","\n","# these are matplotlib.patch.Patch properties\n","props = dict(boxstyle='round', facecolor='wheat', alpha=0.5);\n","\n","# place a text box in upper left in axes coords\n","textbox3 = axx3.text(1.05, 0.95, textstr, transform=axx3.transAxes, fontsize=14,\n","        verticalalignment='top', bbox=props);\n","\n","\n","def update_weights(weights):\n","  global lowest_loss, best_weights\n","  # updating the weights bar plot:\n","  for i, bar in enumerate(weights_bar):\n","    bar.set_height(weights[i]);\n","    \n","  # updating the probability colormap plot:\n","  Yfull = np.matmul(weights, Xfull.T)\n","  probas = my_expit(Yfull)\n","  im_handle.set_data(probas.reshape(att1.shape));\n","  \n","  # updating the loss measures plot\n","  current_loss = negLogLikelihood(Xtr, weights, Ytr)\n","  loss_measures.append(current_loss)\n","  line.set_data(range(len(loss_measures)), loss_measures);\n","  \n","  # update the best classifier so far:\n","  if current_loss < lowest_loss:\n","    best_weights = weights\n","    lowest_loss = current_loss\n","    im_handle_of_best.set_data(probas.reshape(att1.shape));\n","    textstr = '\\n'.join((\n","    r'$loss=%.2f$' % (lowest_loss,),\n","    r'$weights=[%.2f,%.2f,%.2f]$' % (best_weights[0], best_weights[1], best_weights[2]),\n","    r'$Train\\ accuracy=%.2f$%%' % (computeAccuracy(Xtrain, best_weights, df_train_labels)), ));\n","    textbox3.set_text(textstr);\n","    \n","\n","print('Please wait while the animation is being rendered ... ')\n","anim = animation.FuncAnimation(figg, update_weights, \n","                               frames=path_,\n","                               interval =100,\n","                               repeat=False,\n","                               blit=False);\n","display(HTML(anim.to_html5_video()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jA6LcKK9QywN","colab_type":"text"},"source":["---\n","> **Q12:** Note that the third figure, unlike in exhaustive search, is no longer jagged (like saw-tooth!), and instead has a consistent trend downward. Why? Also explain why it has a sharp trend downward at the beginning but then it sort of \"plateaus\"  and the reductions are no longer as significant."]},{"cell_type":"markdown","metadata":{"id":"Zf7XO7BnRZwq","colab_type":"text"},"source":["> **A12:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"hWuUqidkoiWU","colab_type":"text"},"source":["\n","Although the above graphics were helpful, it is not exactly showing us the mechanism of the gradient descent. In what follows, we will try to visualise the actual gradient descent in action. However, we have an issue: the loss function has three arguments, $w_0$, $w_1$ and $w_2$, so to visualise the loss function, we would need 4 visual dimensions! (why?) \n","\n","Because we can really visualise only 3 spatial dimensions, something has to be sacrificed here: we are going to set the value of one of the parameters as fixed, and only vary the other two. For instance, in the following code, we are fixing the value of $w_0$ at its optimal (which we already computed in from the previous steps). So we can now pretend that our loss function has only two parameters $w_1$ and $w_2$, and hence, we can now plot it in 3D:"]},{"cell_type":"code","metadata":{"id":"Jdw_UDc7_ctQ","colab_type":"code","colab":{}},"source":["from mpl_toolkits import mplot3d\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib.colors import LogNorm\n","%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","\n","def loss(w1, w2):\n","  return negLogLikelihood(Xtrain, np.array([res.x[0], w1, w2]), df_train_labels)\n","\n","np_loss = np.frompyfunc(loss, 2, 1)\n","\n","def loss_gradient(w1, w2):\n","  return negLogLikelihoodGradient(Xtrain, np.array([res.x[0], w1, w2]), df_train_labels)[1:]\n","\n","np_loss_gradient = np.frompyfunc(loss_gradient, 2, 1)\n","\n","minima_ = np.array([res.x[1],  res.x[2]])\n","\n","\n","\n","xmin, xmax, xstep = -2.5, 12.5, .2\n","ymin, ymax, ystep = -2.5, 12.5, .2\n","x, y = np.meshgrid(np.arange(xmin, xmax + xstep, xstep), np.arange(ymin, ymax + ystep, ystep))\n","z = np_loss(x, y).astype(float)\n","\n","fig1 = plt.figure(figsize=(8, 5))\n","ax1 = plt.axes(projection='3d', elev=50, azim=-50)\n","print(type(z))\n","print(z.ndim)\n","ax1.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, \n","                edgecolor='none', alpha=.7, cmap=plt.cm.jet)\n","\n","ax1.scatter3D(*minima_, loss(*minima_), marker='*', color='y', s=150)\n","\n","\n","ax1.set_xlabel('$w_1$', fontsize=14)\n","ax1.set_ylabel('$w_2$', fontsize=14)\n","ax1.set_zlabel('L',)\n","\n","ax1.set_xlim((xmin, xmax))\n","ax1.set_ylim((ymin, ymax))\n","\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPUwV7BgS-K9","colab_type":"text"},"source":["So, let us see the gradient descent on this function:"]},{"cell_type":"code","metadata":{"id":"la-KaGHVN-MO","colab_type":"code","colab":{}},"source":["from matplotlib import animation\n","from IPython.display import HTML\n","\n","initial_weights = np.array([0, -1, 1])\n","\n","  \n","path_ = [initial_weights]\n","\n","# use scipy to calculate the optimal values for weights.\n","res = scipy.optimize.minimize(fun = obj_func, x0 = initial_weights, \n","                              jac = grad_func, tol=1e-6, callback=make_minimize_cb(path_))\n","\n","path = np.array(path_)[:,1:].T\n","\n","\n","# gradient = np_loss_gradient(x, y)\n","# pick_first = np.vectorize(lambda x: x[0])\n","# pick_second = np.vectorize(lambda x: x[1])\n","\n","# dz_dx = pick_first(gradient)\n","# dz_dy = pick_second(gradient)\n","\n","fig2, ax2 = plt.subplots(figsize=(10, 6))\n","\n","ax2.contour(x, y, z, levels=np.logspace(0, 3, 50), norm=LogNorm(), cmap=plt.cm.jet)\n","ax2.quiver(path[0,:-1], path[1,:-1], path[0,1:]-path[0,:-1], path[1,1:]-path[1,:-1], scale_units='xy', angles='xy', scale=1, color='k')\n","ax2.plot(*minima_, 'r*', markersize=18)\n","\n","ax2.set_xlabel('$w_1$')\n","ax2.set_ylabel('$w_2$')\n","\n","ax2.set_xlim((xmin, xmax))\n","ax2.set_ylim((ymin, ymax))\n","\n","plt.show()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0NxiWPOtX3dE","colab_type":"text"},"source":["---\n","> **Q14:** Explain what you see. In particular, explain what are the \"contour lines\", what is the point demarcated by a red asterisk, what are the black arrows, what space is this in?"]},{"cell_type":"markdown","metadata":{"id":"OMMmMKrQX5V_","colab_type":"text"},"source":[">**A14:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"MdKAFrxJXJug","colab_type":"text"},"source":["And let's animate the steps!:"]},{"cell_type":"code","metadata":{"id":"VUiPze88uQ3t","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(figsize=(10, 6))\n","\n","ax.contour(x, y, z, levels=np.logspace(0, 3, 50), norm=LogNorm(), cmap=plt.cm.jet)\n","ax.plot(*minima_, 'r*', markersize=18)\n","\n","line, = ax.plot([], [], 'b', label='Gradient-Descent', lw=2, marker='o')\n","point, = ax.plot([], [], 'bo')\n","\n","ax.set_xlabel('$x$')\n","ax.set_ylabel('$y$')\n","\n","ax.set_xlim((xmin, xmax))\n","ax.set_ylim((ymin, ymax))\n","\n","ax.legend(loc='upper left')\n","\n","def init():\n","    line.set_data([], [])\n","    point.set_data([], [])\n","    return line, point\n","  \n","def animate(i):\n","    line.set_data(*path[::,:i])\n","    point.set_data(*path[::,i-1:i])\n","    return line, point\n","\n","anim = animation.FuncAnimation(fig, animate, init_func=init,\n","                               frames=path.shape[1], interval=120, \n","                               repeat_delay=5, blit=True)\n","\n","\n","HTML(anim.to_html5_video())\n","\n","\n","# fig = plt.figure(figsize=(8, 5))\n","# ax = plt.axes(projection='3d', elev=50, azim=-50)\n","\n","# ax.plot_surface(x, y, z, norm=LogNorm(), rstride=1, cstride=1, edgecolor='none', alpha=.8, cmap=plt.cm.jet)\n","# ax1.scatter(*minima_, loss(*minima_), marker='*', color='r', s=200)\n","\n","# # line, = ax.plot([], [], [], 'b', label='Gradient-Descent', lw=2)\n","# point = ax.scatter([], [], [], 'bo')\n","\n","# ax.set_xlabel('$x$')\n","# ax.set_ylabel('$y$')\n","# ax.set_zlabel('$z$')\n","\n","# ax.set_xlim((xmin, xmax))\n","# ax.set_ylim((ymin, ymax))\n","\n","# def init():\n","# #     line.set_data([], [])\n","# #     line.set_3d_properties([])\n","# #     point.set_data([], [])\n","#     point.set_3d_properties([])\n","#     return point\n","  \n","# def animate(i):\n","# #     line.set_data(path[0,:i], path[1,:i])\n","# #     line.set_3d_properties(np_loss(*path[::,:i]))\n","#     point.set_data(path[0,i-1:i], path[1,i-1:i])\n","#     point.set_3d_properties(np_loss(*path[::,i-1:i]))\n","#     return point\n","  \n","# anim = animation.FuncAnimation(fig, animate, init_func=init,\n","#                                frames=path.shape[1], interval=60, \n","#                                repeat_delay=5, blit=True)\n","\n","# HTML(anim.to_html5_video())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"liIGMZj_J2It","colab_type":"text"},"source":["---\n","### <font color='maroon'>**Exercise 6:**Provide one advantage and one disadvantage of using exhaustive search for optimisation, especially in the context of training a model in machine learning. <ins>[0.25+0.25 = 0.5 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"e293xH9OvMno","colab_type":"text"},"source":["## 3. Logistic Regression (MaxEnt) versus KNN"]},{"cell_type":"markdown","metadata":{"id":"g6pknHYv1nFs","colab_type":"text"},"source":["Load a new datasets (test and train), from our other csv files (that you uploaded here!):"]},{"cell_type":"code","metadata":{"id":"NS1w_NHKvPcY","colab_type":"code","colab":{}},"source":["df_testt = pd.read_csv('lab2_1_test.csv')\n","df_trainn = pd.read_csv('lab2_1_train.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BjHg9goAAqrq","colab_type":"text"},"source":["Let's plot the training dataset. It classifies the 2D training dataset with two classes shown (i.e., Labels: $0$ and $1$)."]},{"cell_type":"code","metadata":{"id":"33Fi3UbNArQa","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","# %config InlineBackend.figure_format = 'retina'\n","\n","# import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# split data in df_train to two class labels\n","df_trainn_Label_0 = df_trainn.loc[df_trainn['Label'] == 0]\n","df_trainn_Label_1 = df_trainn.loc[df_trainn['Label'] == 1]\n","\n","# plot data\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","ax.scatter(df_trainn_Label_0['Attribute_1'], \n","            df_trainn_Label_0['Attribute_2'], \n","            c = 'b', marker = 'o', label = 'Label 0')\n","ax.scatter(df_trainn_Label_1['Attribute_1'], \n","            df_trainn_Label_1['Attribute_2'], \n","            c = 'r', marker = 'x', label = 'Label 1')\n","\n","# set a title and labels\n","ax.set_title('Plot $training$ dataset', fontsize=14)\n","ax.set_xlabel(\"Attribute 1\", fontsize=12)\n","ax.set_ylabel(\"Attribute 2\", fontsize=12)\n","\n","# set dimensions of x and y axes \n","ax.set_xlim(-4, 4)\n","ax.set_ylim(-4, 4)\n","ax.grid(alpha = 0.2)\n","\n","# set legend\n","ax.legend(fontsize=12)\n","\n","# show the plot\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7d3Cfcvu34h4","colab_type":"text"},"source":[" Let's run `Logistic Regression` (`MaxEnt`) to classify this dataset."]},{"cell_type":"code","metadata":{"id":"9RXdbFlQ8H0L","colab_type":"code","colab":{}},"source":["import scipy\n","\n","# Initial guess of weights\n","initial_weights = np.array([0, -1, 1])\n","\n","# Prepare sample data for computing the accuracy of the weight\n","Xtrainn = df_trainn[['Attribute_1', 'Attribute_2']].values\n","Xtrainn = np.c_[np.ones(Xtrainn.shape[0]), Xtrainn]\n","df_trainn_labels = df_trainn['Label'].values\n","\n","# The loss function to be minimized (the objective function)\n","def obj_func(w):\n","  return negLogLikelihood(Xtrainn, w, df_trainn_labels)\n","\n","# The gradient of loss function.\n","def grad_func(w):\n","  return negLogLikelihoodGradient(Xtrainn, w, df_trainn_labels)\n","\n","# use scipy to calculate the optimize values for weights.\n","res = scipy.optimize.minimize(fun = obj_func, x0 = initial_weights, \n","                              jac = grad_func, tol=1e-6)\n","print('Computed optimized weights: ', res.x)\n","\n","# Accuracy of the computed optimized weights on training dataset.\n","negLL =  obj_func(res.x)\n","gradientLL = grad_func(res.x)\n","\n","print ('llh:\\t', negLL)\n","print ('dLLh:\\t', gradientLL)\n","print('Train accuracy: {:.2f}%'.format(computeAccuracy(Xtrainn, res.x, df_trainn_labels)))\n","\n","# Accuracy of the computed optimized weights on the testing dataset.\n","Xtestt = df_testt[['Attribute_1', 'Attribute_2']].values\n","Xtestt = np.c_[np.ones(Xtestt.shape[0]), Xtestt]\n","df_testt_labels = df_testt['Label'].values\n","print('Test accuracy: {:.2f}%'.format(computeAccuracy(Xtestt, res.x, df_testt_labels)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f73ihgr2W_55","colab_type":"text"},"source":["---\n","> **Q15:** Explain why `Logistic Regression (MaxEnt)` is not a good choice of a classifier for this dataset\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZZb4LzUQY30A","colab_type":"text"},"source":["> **A15:** \n","---\n"]},{"cell_type":"markdown","metadata":{"id":"mn_NnF3E52QJ","colab_type":"text"},"source":["Now, instead of `Logistic Regression (MaxEnt)`, let's classify with `KNN`. "]},{"cell_type":"markdown","metadata":{"id":"rOO7KOgTNTGB","colab_type":"text"},"source":["First, let's see the decision regions of the KNN classifer generated from this training dataset for a few different values of K. \n","\n","The plots should appear in different tabs. If they don't try running the cell twice!"]},{"cell_type":"code","metadata":{"id":"YP65hZaJWtVn","colab_type":"code","colab":{}},"source":["# Don't forget to run this cell twice to get the tabs. \n","# If you don't get the tab view, it is fine, you can still analyse the outputs!\n","\n","from google.colab import widgets\n","from sklearn.neighbors import KNeighborsClassifier\n","from matplotlib.colors import ListedColormap\n","from itertools import chain\n","\n","\n","# source for this part inspired from: \n","# https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n","\n","\n","# Create color maps\n","cmap_light = ListedColormap(['#AAAAFF','#FFAAAA'])\n","# cmap_bold = ListedColormap(['#0000FF', '#FF0000'])\n","\n","\n","K_values = list(range(1, 10, 2))\n","K_values.append(40)\n","\n","X = Xtrainn[:,1:] # we don't need the all ones columns any more!\n","\n","h = .02  # step size in the mesh\n","x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n","y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                     np.arange(y_min, y_max, h))\n","\n","\n","tb = widgets.TabBar([str(K) for K in K_values])\n","for K in K_values:\n","  with tb.output_to(str(K), select= (K < 2)):\n","    \n","    fig = plt.figure(figsize=(7, 7))\n","    ax = fig.add_subplot(111)\n","    ax.set_aspect('equal')\n","    \n","    \n","    clf = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean')\n","    clf.fit(X, df_trainn_labels)\n","    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n","\n","    # Put the result into a color plot\n","    Z = Z.reshape(xx.shape)\n","    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n","\n","\n","    ax.scatter(df_trainn_Label_0['Attribute_1'], \n","                df_trainn_Label_0['Attribute_2'], \n","                c = 'b', marker = 'o', label = 'Label 0')\n","    ax.scatter(df_trainn_Label_1['Attribute_1'], \n","                df_trainn_Label_1['Attribute_2'], \n","                c = 'r', marker = 'x', label = 'Label 1')\n","\n","    # set a title and labels\n","    ax.set_title('Decision Regsions of KNN for K={}'.format(K), fontsize=14)\n","    ax.set_xlabel(\"Attribute 1\", fontsize=12)\n","    ax.set_ylabel(\"Attribute 2\", fontsize=12)\n","\n","    # set dimensions of x and y axes \n","    ax.set_xlim(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5)\n","    ax.set_ylim( X[:, 1].min() - 0.5, X[:, 1].max() + 0.5)\n","    ax.grid(alpha = 0.2)\n","\n","    # set legend\n","    ax.legend(loc='upper left', fontsize=12)\n","\n","    # show the plot\n","    plt.show()\n","\n","    \n","#     print('K={}'.format(K))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sq5wSBbQp9RR","colab_type":"text"},"source":["If you couldn't see it before, it should now become relatvely more clear that the\n","two classes are arranged in a spiral."]},{"cell_type":"markdown","metadata":{"id":"AJI5fAIQreo4","colab_type":"text"},"source":["---\n","> **Q16:** Describe (and briefly explain) the effect of the following on the decision regions:<br>\n",">(a) K\n",">(b) changing the \"weights\" parameter of the KNN classifier from `uniform` to `distance` (in line 40 of the code). You may want to consult with the [documentation page of this on skitlearn:](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)<br>\n",">(c) changing the \"metric\" parameter of the KNN classifier from `eucledean` to `cityblock` and `cosine`. It is worthwhile to check the [documentation from sklearn on this one](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) too.\n"]},{"cell_type":"markdown","metadata":{"id":"_ZmdfpDWzuwC","colab_type":"text"},"source":[">**A16:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"bgdOB2AkXDXz","colab_type":"text"},"source":["### Picking the best K for KNN:"]},{"cell_type":"markdown","metadata":{"id":"rvZsbvKCXI5U","colab_type":"text"},"source":["In order to pick the best K, we first need a measure of evaluation, i.e., how good a classifier is:"]},{"cell_type":"code","metadata":{"id":"r8rPmQHOVgay","colab_type":"code","colab":{}},"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","KNN_classifier = KNeighborsClassifier(n_neighbors=K)\n","\n","KNN_classifier.fit(X, df_trainn_labels)\n","X_test = Xtestt[:,1:]\n","Y_test_pred = KNN_classifier.predict(X_test)\n","\n","\n","# Evaluating the Algorithm on test data:\n","\n","from sklearn.metrics import classification_report, confusion_matrix\n","print('Confusion matrix on the test data for KNN with K={}:\\n'.format(K))\n","print(confusion_matrix(df_testt_labels, Y_test_pred))\n","print('\\nClassification report on the test data for KNN with K={}:\\n'.format(K))\n","print(classification_report(df_testt_labels, Y_test_pred))\n","\n","# Now for educational purposes (!) let's evaluate on the train data as well:\n","Y_train_pred = KNN_classifier.predict(X)\n","print('\\n\\n'+'-'*60+'\\n\\n')\n","print('Confusion matrix on the train data for KNN with K={}:\\n'.format(K))\n","print(confusion_matrix(df_trainn_labels, Y_train_pred))\n","print('\\nClassification report on the train data for KNN with K={}:\\n'.format(K))\n","print(classification_report(df_trainn_labels, Y_train_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xGha7ZDWbvNU","colab_type":"text"},"source":["---\n",">**Q17:** Describe/interpret the numerical reports, and explain why using the train data is not helpful in finding the best value for K in KNN. "]},{"cell_type":"markdown","metadata":{"id":"Khh_oWUEcGr4","colab_type":"text"},"source":[">**A17:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"58SKmGgVIJul","colab_type":"text"},"source":["How do we choose the best K then? As with any hyper-parameter, with **validation**:\n","\n"]},{"cell_type":"code","metadata":{"id":"Rmf08RAhG6r-","colab_type":"code","colab":{}},"source":["# Investigating validation error rate for K between 1 and 40\n","error_test = []\n","# error_train = []\n","\n","for k in range(1, 40):\n","    knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', metric='euclidean')\n","    knn.fit(X, df_trainn_labels)\n","    pred_test_k = knn.predict(X_test)\n","    error_test.append(np.mean(pred_test_k != df_testt_labels))\n","#     pred_train_k = knn.predict(X)\n","#     error_train.append(np.mean(pred_train_k != df_trainn_labels))\n","\n","    \n","\n","plt.figure(figsize=(12, 6))\n","plt.plot(range(1, 40), error_test, color='red', linestyle='dashed', marker='o',\n","         markerfacecolor='blue', markersize=10, label='Validation error rate')\n","# plt.plot(range(1, 40), error_train, color='green', linestyle='dashed', marker='.',\n","#          markerfacecolor='black', alpha=0.4, markersize=10, label='train error rate')\n","plt.title('error rate vs K values (for choosing the best K)')\n","plt.xlabel('K', fontsize=16)\n","plt.ylabel('error rate', fontsize=16)    \n","plt.ylim(0,0.5)\n","plt.xlim(0,40)\n","plt.legend(fontsize=16)\n","plt.grid()\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIyuYH94cpx6","colab_type":"text"},"source":["---\n","### <font color='maroon'>**Exercise 7:** (a) What is the best value for $K$? Explain how you got to this solution. (b) Interpret the main trends in the plot. In partiular, specify which parts correspond to under-fitting and which part to over-fitting.    <ins>[0.25+0.25 = 0.5 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"GpOKvjb0VqeW","colab_type":"text"},"source":["---\n","### <font color='maroon'>**Exercise 8:** Given what you learned from this entire lab(!), provide one advantage and one disadvantage of using MaxEnt vs KNN. <ins>[0.25+0.25 = 0.5 mark]</ins></font>\n","---"]}]}