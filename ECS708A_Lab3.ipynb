{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ECS708A_Lab3.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xKNf2WkWpcWs"},"source":["# <font color='gray'> Lab 3: Classification II</font>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nC6ZwWkQn3J4"},"source":["\n","-\n","## Introduction \n","\n","\n","The aim of this lab is familiarise ourselves with the concepts of **priors**, **dimensionality** and **Bayes** in\n","classification problems. \n","\n","- This lab constitutes your third course-work activity.\n","- A report answering the <font color = 'red'>**questions in</font><font color = \"maroon\"> red**</font> only should be submitted by the 5th of April. \n","- The report should be a separate file in **pdf format** (so **NOT** *doc, docx, notebook* etc.), well identified with your name, student number, assignment number (for instance, Assignment 3), module code. \n","- No other means of submission other than the appropriate QM+ link is acceptable at any time (so NO email attachments, etc.)\n","- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MgbLOYNbun_k"},"source":["## **1. Data Proportions with `MaxEnt` + `Bayes`**\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0GpPnQGdC4wQ"},"source":["#### 0. Loading the dataset\n","\n","*   This first cell loads the `Iris` flower dataset. The Iris flower fataset is a classic dataset used to identify types of flowers based on features describing their petals. It also comes with a description of itself:\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3ZGLOPEnDSem","colab":{}},"source":["import numpy as np\n","from sklearn import datasets\n","\n","iris = datasets.load_iris()\n","#print(iris)\n","print(iris.DESCR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v1gyn6WnEQ2j"},"source":["*   As you can read, the full dataset contains four attributes (`sepal length`, `sepal width`, `petal length`, `petal width`, in centimeters) , but for simplicity (and to be able to plot them!) we will only work with the first two (`sepal length`, `sepal width`). "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZqmXUknCDrGq","colab":{}},"source":["X = iris.data[:, :2] \n","print(X[:4,:]) # printing the first few samples, to get a feeling about the data\n","print(iris.feature_names[:2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qSgswh5SGOuA"},"source":["*  This dataset is labeled. The labels are the type of Iris flower each sample is: `setosa`, `versicolor`, and `virginica`. The labels are represented by numbers: 0, 1 and 2, respectively:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mthT7vzQGosU","colab":{}},"source":["Y = iris.target\n","print(Y[:10]) # printing the first few labels\n","print(iris.target_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fmlwxmGD0iG0"},"source":["#### 1. Plotting the dataset\n","\n","*   Since we now have only two attributes, we can plot them in 2-D. To differentiate the labels, we will use different markers: "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NBdbzbC1CtDu","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","# marker_list = ['P', 'X', 'o']\n","marker_list = ['+', '.', 'x']\n","# marker_list = ['*', 's', 'D']\n","\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","for l in [0, 1, 2]:\n","  ax.scatter(X[Y == l, 0], X[Y == l, 1], \n","             marker=marker_list[l], s=70, color='black',\n","            #  color = plt.cm.Accent.colors[l], edgecolors='k',\n","             label='{:d} ({:s})'.format(l, iris.target_names[l]))\n","\n","ax.legend(fontsize=12)\n","ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","ax.grid(alpha=0.3)\n","ax.set_xlim(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5)\n","ax.set_ylim(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-T8C6SkQOXyj"},"source":["Let's split our data into train and test sets (you should know by now why we do this!). We will use a simple 50/50 split:"]},{"cell_type":"markdown","metadata":{"id":"XRlXOWIkxt7T","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KPlDiK_MOkKn","colab":{}},"source":["Xtr = X[::2,:] # train data set (features)\n","Ytr = Y[::2] # train data set (labels)\n","\n","Xte = X[1::2,:] # test data set (features)\n","Yte = Y[1::2] # test data set (labels)\n","#print (Xtr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oAJHPRWZ04s4"},"source":["#### 2. Train a Multi-class Logistic Regression classifier \n","\n","In the previous lab, we implemented a logistic regression classifier (a.k.a. `MaxEnt`) by ourseleves. Here, we will instead import it from the python's `scikit-learn` library. \n","(Link to `scikit-learn` [documentation](https://scikit-learn.org/stable/documentation.html), and its [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) page in particular).\n","Some of the code here is from [this example](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html) from scikit-learn's own documentation.\n","\n","To learn more about `multinomial logistic regression` visit [Binary vs. Multi-Class Logistic Regression (by Chris Yeh)](https://chrisyeh96.github.io/2018/06/11/logistic-regression.html)\n"," "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DkDyyEX0OKQX","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","log_reg_classifier = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n","log_reg_classifier.fit(Xtr, Ytr)\n","\n","x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n","y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n","\n","h = .02  # step size in the mesh\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","Z = log_reg_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n","\n","# First, plotting the decision regions:\n","Z = Z.reshape(xx.shape)\n","fig = plt.figure(1, figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","ax.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n","\n","\n","# Now plotting the training points:\n","for l in [0, 1, 2]:\n","  ax.scatter(Xtr[Ytr == l, 0], Xtr[Ytr == l, 1], \n","             marker=marker_list[l], color='black', s=70,\n","            label='{:d} ({:s})'.format(l, iris.target_names[l]))\n","   \n","\n","ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","ax.set_xlim(xx.min(), xx.max())\n","ax.set_ylim(yy.min(), yy.max())\n","ax.legend(fontsize=12, loc='upper right')\n","ax.grid(alpha=0.3)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9lWHK6KIXAGy"},"source":["We trained our model using training data only. In the above plot, we are displaying the training data and the resulting three decision regions. To get a visual feeling of how well our model is generalising to unseen data, let's display the test data as well. We are going to plot test samples in red, to differentiate them from the training data:\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6jVvquUwXHMZ","colab":{}},"source":["fig = plt.figure(1, figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","# Fitst plotting the decision regsions again:\n","plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n","\n","# plotting the train points in black and with reduced opacity:\n","for l in [0, 1, 2]:\n","  plt.scatter(Xtr[Ytr == l, 0], Xtr[Ytr == l, 1], \n","             marker=marker_list[l], color='black', s=70, alpha=0.5,)\n","# plotting the test points in red:\n","for l in [0, 1, 2]:\n","  plt.scatter(Xte[Yte == l, 0], Xte[Yte == l, 1], \n","             marker=marker_list[l], color='red', s=70, \n","              label='{:d} ({:s})'.format(l, iris.target_names[l]))\n","  \n","  \n","ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","ax.set_xlim(xx.min(), xx.max())\n","ax.set_ylim(yy.min(), yy.max())\n","ax.legend(fontsize=12, loc='upper right')\n","ax.grid(alpha=0.3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"q0LlVl5YYgk1"},"source":["---\n","> **Q0:** Using the code snippet below, compute the accuracy of our logistic model on both **train** and **test** data.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RlMtwEvzZgK-"},"source":["> **A0:**\n","--- Included code below and the test accuracy is 82%"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3NprU8LmYhDs","colab":{}},"source":["# first computing the train accuracy:\n","train_accuracy = np.average(Ytr==log_reg_classifier.predict(Xtr))\n","print('train accuracy (computed ourselves) = {}'.format(train_accuracy))\n","print('train accuracy (using an already available method of our classifier object) = {}'.format(log_reg_classifier.score(Xtr, Ytr)))\n","# now computing the test accuracy ...   \n","test_accuracy = np.average(Ytr==log_reg_classifier.predict(Xte))\n","print('test accuracy (computed ourselves) = {}'.format(test_accuracy))\n","print('test accuracy (using an already available method of our classifier object) = {}'.format(log_reg_classifier.score(Xte, Yte)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3b9DI_5RP3Si"},"source":["---\n","> **Q1:** How many instances of each class are there in the training data? (A sample code is provided for you below as a hint)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HWyYYP0MaZ5Z"},"source":["> **A1:**\n","---Prss: 25 instances are present for each class"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5EMRr70ga305","colab":{}},"source":["\n","print(sum(Ytr==0))\n","print(sum(Ytr==1))\n","print(sum(Ytr==2))\n","#print (sum(Yte==0))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XbqJiMEwR5NS"},"source":["And the confusion matrix (here, the true classes correspond to the rows of the confusion matrix, whereas the predicted classes correspond to the columns)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b5QRLIoeJRGx","colab":{}},"source":["# Computing the confusion matrix:\n","# first, from scikit-learn library:\n","\n","from sklearn.metrics import confusion_matrix\n","\n","train_confusion_matrix = confusion_matrix(y_true=Ytr, y_pred=log_reg_classifier.predict(Xtr))\n","print('train confusion matrix:\\n {}\\n'.format(train_confusion_matrix))\n","\n","test_confusion_matrix = confusion_matrix(y_true=Yte, y_pred=log_reg_classifier.predict(Xte))\n","print('test confusion matrix:\\n {}'.format(test_confusion_matrix))\n","\n","cm = train_confusion_matrix\n","train_confusion_matrix_normalised = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","cm = test_confusion_matrix\n","test_confusion_matrix_normalised = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","print('Normalised train confusion matrix:\\n {}\\n'.format(train_confusion_matrix_normalised))\n","print('Normalised test confusion matrix:\\n {}\\n'.format(test_confusion_matrix_normalised))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lqaB4drrMSCb"},"source":["If you would like to display the confusion matrix more nicely (taken from [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)):"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yc2aHuqWL8Ef","colab":{}},"source":["fig, ax = plt.subplots()\n","ax.set_aspect('equal')\n","ax.set_xlim(-0.5, 2.5)\n","ax.set_ylim(-0.5, 2.5)\n","\n","cm = test_confusion_matrix\n","classes = iris.target_names\n","im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n","ax.figure.colorbar(im, ax=ax)\n","ax.set(xticks=np.arange(cm.shape[1]),\n","       yticks=np.arange(cm.shape[0]),\n","       # ... and label them with the respective list entries\n","       xticklabels=classes, yticklabels=classes,\n","       title='Test Confusion Matrix',\n","       ylabel='True label',\n","       xlabel='Predicted label')\n","\n","# Rotate the tick labels and set their alignment.\n","plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","\n","# Loop over data dimensions and create text annotations.\n","thresh = cm.max() / 2.\n","for i in range(cm.shape[0]):\n","    for j in range(cm.shape[1]):\n","        ax.text(j, i, format(cm[i, j], 'd'),\n","                ha=\"center\", va=\"center\",\n","                color=\"white\" if cm[i, j] > thresh else \"black\")\n","fig.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qHfboAmvlSkn"},"source":["Since we are going to print the evaluation of our classifiers a few times, let's create a new function that will do this job:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zHv0zmuZlf0T","colab":{}},"source":["def print_classifier_report(Xtrain, ytrain, Xtest, ytest, classifier, plot_regions=False):\n","  train_confusion_matrix = confusion_matrix(y_true=ytrain, y_pred=classifier.predict(Xtrain))\n","  print('Train accuracy = {:.3f}'.format(classifier.score(Xtrain, ytrain)))\n","  print('Train confusion matrix:\\n {}\\n'.format(train_confusion_matrix))\n","\n","  test_confusion_matrix = confusion_matrix(y_true=ytest, y_pred=classifier.predict(Xtest))\n","  print('Test accuracy = {:.3f}'.format(classifier.score(Xtest, ytest)))\n","  print('Test confusion matrix:\\n {}\\n'.format(test_confusion_matrix))\n","\n","  cm = train_confusion_matrix\n","  train_confusion_matrix_normalised = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","  cm = test_confusion_matrix\n","  test_confusion_matrix_normalised = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","  print('Normalised train confusion matrix:\\n {}\\n'.format(train_confusion_matrix_normalised))\n","  print('Normalised test confusion matrix:\\n {}\\n'.format(test_confusion_matrix_normalised))\n","  \n","  if plot_regions:\n","\n","    fig = plt.figure(1, figsize=(7, 7))\n","    ax = fig.add_subplot(111)\n","    ax.set_aspect('equal')\n","\n","    # Fitst plotting the decision regsions again:\n","    x_min = min(Xtrain[:, 0].min(), Xtest[:, 0].min()) - .5 # min of the x axis\n","    x_max = max(Xtrain[:, 0].max(), Xtest[:, 0].max()) + .5 # max of the x axis\n","    y_min = min(Xtrain[:, 1].min(), Xtest[:, 1].min()) - .5 # min of the y axis\n","    y_max = max(Xtrain[:, 1].max(), Xtest[:, 1].max()) + .5 # max of the y axis\n","    h = .02  # step size in the mesh\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n","    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","    plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Pastel2)\n","\n","    # plotting the train points in black and with reduced opacity:\n","    marker_list = ['+', '.', 'x']\n","    for l in [0, 1, 2]:\n","      plt.scatter(Xtrain[ytrain == l, 0], Xtrain[ytrain == l, 1], \n","                 marker=marker_list[l], color='black', s=70, alpha=0.5,)\n","    # plotting the test points in red:\n","    for l in [0, 1, 2]:\n","      plt.scatter(Xtest[ytest == l, 0], Xtest[ytest == l, 1], \n","                 marker=marker_list[l], color='red', s=70, \n","                  label='{:d} ({:s})'.format(l, iris.target_names[l]))\n","\n","\n","    ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","    ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","    ax.set_xlim(xx.min(), xx.max())\n","    ax.set_ylim(yy.min(), yy.max())\n","    ax.legend(fontsize=12, loc='upper right')\n","    ax.grid(alpha=0.3)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CqYb90I5tImi"},"source":["And now just to check if the code works:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZM0mOlAeslid","colab":{}},"source":["print_classifier_report(Xtr, Ytr, Xte, Yte, log_reg_classifier, plot_regions=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WFcAzzTX096E"},"source":["### 3. Imbalanced Classes\n","We now assume that the classifier is trained based on data gathered in some geographic region A, where the proportions of flowers of each type are different. Specifically, we assume that flower type 2 (`virginica Iris`) is 5 times less common.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U0q9_JCeb1TL","colab":{}},"source":["index_A_tr = [i for i, x in enumerate(Xtr) if Ytr[i]!=2 or i % 5 == 0]\n","XtrImbalanced_A = Xtr[index_A_tr,:] # train data set (features)\n","YtrImbalanced_A = Ytr[index_A_tr] # train data set (labels)\n","\n","# just to check the imbalanced-ness of the new training data:\n","for l in [0, 1, 2]:\n","  print('Number of instances of class {} in the new training data: {}'.format(l, sum(YtrImbalanced_A == l)))\n","  \n","  \n","index_A_te = [i for i, x in enumerate(Xte) if Yte[i]!=2 or i % 5 == 0]\n","XteImbalanced_A = Xte[index_A_te,:] # test data set (features)\n","YteImbalanced_A = Yte[index_A_te] # test data set (labels)\n","\n","print()\n","# just to check the imbalanced-ness of the new test data:\n","for l in [0, 1, 2]:\n","  print('Number of instances of class {} in the new test data: {}'.format(l, sum(YteImbalanced_A == l)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aThsG_woYwrh"},"source":["*   We will now train a logistic classifier on this new train data which has an imbalanced proportion of classes, and investigate the new accuracy, new confusion matrix, and the new decision regions:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XSKNhZaZT8B2","colab":{}},"source":["log_reg_classifier_imbalanced_A = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n","log_reg_classifier_imbalanced_A.fit(XtrImbalanced_A, YtrImbalanced_A)\n","\n","print_classifier_report(XtrImbalanced_A, YtrImbalanced_A, XteImbalanced_A, YteImbalanced_A, log_reg_classifier_imbalanced_A, True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RDJRdCK7ZdCB"},"source":["---\n","> **Q2:** Compare the new decision boundary with the previous one. Describe the difference."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j8aNRg6DZklx"},"source":["> **A2:** \n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8X_eD4_la2ck"},"source":["*   Note\tthat\talthough\tthe\toverall\taccuracy\tis\thigh,\tone\tclass\tis\tbeing\theavily\tmis-classified.\t"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"So83LHGS1ZE1"},"source":["---\n","#### <font color='maroon'>**Exercise 1:** Which class is being heavily mis-classified? Why has this happened? <ins>[1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"OQnb21hrgAVZ","colab_type":"text"},"source":["Prss: Virginca is mis-classified heavily. Due to less training instances for Virginica class"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Akc9emKm1ePA"},"source":["---\n","#### <font color='maroon'>**Exercise 2:** Obtain the accuracy for this class from the test dataset and identify the other class that it is being confused with. <ins>[1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"oZ-Wnsswf0iy","colab_type":"text"},"source":["Prss: 3 instances out of 5 instances falls in other class Versicolar. It is 40% accuracy observed from the normalised test matrix"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6TpBxKYh14Xk"},"source":["Now let's assume our classifier (which was trained using data of region A) is taken to another geographic region B, where the flower proportions are different again, specifically flower type 1 (`versicolor Iris`) is 5 times less common.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s1i7fj6qfmRq","colab":{}},"source":["index_B_te = [i for i, x in enumerate(Xte) if Yte[i]!=1 or i % 5 == 0]\n","XteImbalanced_B = Xte[index_B_te,:] # test data set (features)\n","YteImbalanced_B = Yte[index_B_te] # test data set (labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ShFIckUKfqB9"},"source":["\n","*   To visually compare the flower frequencies:\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2URC95um1JCD","colab":{}},"source":["fig, ax = plt.subplots()\n","counts_A = np.bincount(YteImbalanced_A)\n","counts_B = np.bincount(YteImbalanced_B)\n","\n","xlocations = np.array([0, 1, 2])\n","ax.bar(xlocations-0.15, counts_A, width=0.3, align='center', label='A')\n","ax.bar(xlocations+0.15, counts_B, width=0.3, align='center', hatch='//', label='B')\n","\n","ax.set_xticks([0, 1, 2])\n","ax.set_xticklabels(iris.target_names)\n","ax.set_xlim(-1,3.0)\n","\n","ax.grid(axis='y', alpha=0.75)\n","ax.set_xlabel('Iris category')\n","ax.set_ylabel('Frequency')\n","ax.set_title('Comparing the frequency of flower types between regions A and B')\n","ax.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rIPJJfhaHOVs"},"source":["*   So how would our model that is trained in region A perform if it used in region B? Let's see:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Pt0FKGeIQwQV","colab":{}},"source":["print_classifier_report(XtrImbalanced_A, YtrImbalanced_A, XteImbalanced_B, YteImbalanced_B, log_reg_classifier_imbalanced_A, True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-UIku_xJHgl-"},"source":["*   Analyse the distribution of circle and cross points in the predictor space\n","and the resulting decision boundaries (note that if samples were\n","correctly classified, the virginica samples (crosses) should be in the gray region,  and the\n","versicolor samples (circles) should be in light green region).\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2KBLur4OfLGX"},"source":["---\n","#### <font color='maroon'>**Exercise 3:** What is the new accuracy for class 2 (virginica)? Compare this accuracy with the accuracy obtained in the previous section and explain any discrepancies. <ins>[1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"HhfLSqV6hbwZ","colab_type":"text"},"source":["44 percentage is the new accuracy for class2.  The accuracy is improved from 40% to 44% in the new test data. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CyP9b3kkf7YO"},"source":["## Naive Bayes\n","\n","In this cell we obtain a Naïve Bayes classifier from a dataset corresponding to the region A. \t\n","\n","Although Naive Bayes is simple enough that we can write our own code for it, let's just directly use scikit-learn, (as documented [here](https://scikit-learn.org/stable/modules/naive_bayes.html) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)):"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0Vf9xSusvAu6","colab":{}},"source":["from sklearn.naive_bayes import GaussianNB\n","gnb_A = GaussianNB()\n","gnb_A.fit(XtrImbalanced_A, YtrImbalanced_A)\n","\n","print_classifier_report(XtrImbalanced_A, YtrImbalanced_A, XteImbalanced_A, YteImbalanced_A, gnb_A, True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w-TKsGXfG1oy"},"source":["### Naive Bayes with priors\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"l4nGpC0PeOn3"},"source":["*   In Naïve Bayes classifiers, priors can be specified. At test time the prior is combined with the likelihood via Bayes theorem to classify the samples.\n","\n","*   By default, Bayes classifiers use the observed data frequency to estimate\n","the prior, although you can specify them yourself.\n","\n","*   Let's specify a balanced prior (even though the data is imbalanced) by passsing prior = [1/3,1/3,1/3]."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WnKrmbK_HBGX","colab":{}},"source":["gnb_A_with_uniform_priors = GaussianNB(priors=np.array([1/3,1/3,1/3]))\n","gnb_A_with_uniform_priors.fit(XtrImbalanced_A, YtrImbalanced_A)\n","\n","print_classifier_report(XtrImbalanced_A, YtrImbalanced_A, XteImbalanced_A, YteImbalanced_A, gnb_A_with_uniform_priors, True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DsBLSwEYI2eL"},"source":["---\n","> **Q3:** Compare the new decision boundary with the previous one. Describe the difference.\n","Compare your results against the ones obtained by letting the prior be determined from the data, by\n","analyzing any changes in the decision boundaries and in the confusion matrix."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MLY3lzTmJdjt"},"source":["> **A3:**\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y58Xj7jIMjl8"},"source":["Now, suppose as before that we only have access to data from region A, but we know that our model is going to be used in region B. We can reuse the likelihoods already learnt and set the \"priors\" of region B. \n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7BquMMFVxEhJ"},"source":["---\n","#### <font color='maroon'>**Exercise 4:** What prior should you use to get maximum accuracy in region B? What accuracy do you get by using this value? <ins>[1 mark]</ins></font>\n","---\n","Note that you should be able to get higher accuracy than by using of the MaxEnt classifier from\n","region A in B as we did before. Also keep in mind that a good prior should reflect the\n","relative frequency of flowers in region B.\n","\n","Use the following code block to answer this exercise."]},{"cell_type":"markdown","metadata":{"id":"r1Bj-wMJkgoC","colab_type":"text"},"source":["Prss:  The prior should be changed to (25/55,5/55,25/55). The accuracy increases to 81.8%"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hZr1xpheJmmV","colab":{}},"source":["# code block to be used to answer the above exercise\n","  \n","gnb_A_with_uniform_priors = GaussianNB(priors=np.array([25/55,5/55,25/55]))\n","gnb_A_with_uniform_priors.fit(XtrImbalanced_A, YtrImbalanced_A)\n","\n","print_classifier_report(XtrImbalanced_A, YtrImbalanced_A, XteImbalanced_B, YteImbalanced_B, gnb_A_with_uniform_priors, True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"V6ulkL8E6cLJ"},"source":["## **3. Scaling with the number of dimensions (a.k.a attributes, features)**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fS9zYBph7cDl"},"source":["Run the following cell. You will create a dataset consisting of 25 examples for  2 classes (so a total of 50 instances):"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2X9vk54xREM3","colab":{}},"source":["import numpy as np\n","\n","np.random.seed(2)\n","\n","#dim = 2     # Generate data with only 2 dimensions.\n","dim = 200   # Generate data with 200 dimensions.\n","m1, m2, s1, s2 = [1, 0, 1, 1]\n","numberInstances=25\n","\n","Ytr = np.append(np.ones(numberInstances, dtype = int), np.zeros(numberInstances, dtype = int), axis = 0)\n","Xtr = np.append(m1 + s1 * np.random.randn(numberInstances, dim),  m2 + s2 * np.random.randn(numberInstances, dim), axis = 0)\n","Yte = np.append(np.ones(numberInstances, dtype = int), np.zeros(numberInstances, dtype = int), axis = 0)\n","Xte = np.append(m1 + s1 * np.random.randn(numberInstances, dim),  m2 + s2 * np.random.randn(numberInstances, dim), axis = 0)\n","#print(Xtr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-b5nZYH07nXN"},"source":["As you can see in the following figure, the dataset is weakly separable.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NKrweWGp6e8s","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","# split the data in df_train based on their class labels\n","train_Label_0 = Xtr[Ytr == 0]\n","train_Label_1 = Xtr[Ytr == 1]\n","\n","# plot the data\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","ax.scatter(train_Label_0[:,0], train_Label_0[:,1], \n","            c = 'b', marker = 'o', label = 'Label 0')\n","ax.scatter(train_Label_1[:,0], train_Label_1[:,1], \n","            c = 'r', marker = 'x', label = 'Label 1')\n","\n","# set a title and labels\n","ax.set_title('Plot of the $training$ dataset', fontsize = 18)\n","ax.set_xlabel(\"Attribute 1\", fontsize = 14)\n","ax.set_ylabel(\"Attribute 2\", fontsize = 14)\n","\n","# set dimensions of x and y axes \n","ax.set_xlim(-4, 4)\n","ax.set_ylim(-4, 4)\n","ax.grid(alpha=0.2)\n","\n","# set legend\n","ax.legend(fontsize=14)\n","\n","# show the plot\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fDOA6d3f7zhi"},"source":["*   Run logistic regression. Note the train and test accuracy.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NnFqnNAg6htx","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","\n","log_reg_classifier = LogisticRegression(C=1e5, solver='lbfgs')\n","log_reg_classifier.fit(Xtr, Ytr)\n","\n","print('Train accuracy (Logistic Regression): {:.2f}'.format(log_reg_classifier.score(Xtr, Ytr)))\n","print('Test accuracy (Logistic Regression): {:.2f}'.format(log_reg_classifier.score(Xte, Yte)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YNiWnRsW7vae"},"source":["*   Run naïve Bayes. Note the train and test accuracy. They should be similar.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ojcvU9ep6mdh","colab":{}},"source":["from sklearn.naive_bayes import GaussianNB\n","\n","gnb_classifier = GaussianNB()\n","gnb_classifier.fit(Xtr, Ytr)\n","\n","print('Train accuracy (Naive Bayes): {:.2f}'.format(gnb_classifier.score(Xtr, Ytr)))\n","print('Test accuracy (Naive Bayes): {:.2f}'.format(gnb_classifier.score(Xte, Yte)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_876hURa7-Zn"},"source":["*   Now lets suppose that instead of 2 attributes, there are 200 attributes, each of\n","which are weakly informative."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b2j83WYr8Di1"},"source":["  Set dim = 200 in Cell 1 and re-run the remaining cells.\n","* Note the train and test accuracy of each approach"]},{"cell_type":"markdown","metadata":{"id":"mMW5Ghgam6lS","colab_type":"text"},"source":["Prss: Increasing the attributes increases the accuracy. When the data was 2 dimension it was not linearly separable when 200 attributes its easy to separate the model( Ex: paper 2d to multifold)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QD5z-FKmCzly"},"source":["---\n","#### <font color='maroon'>**Exercise 5:** Compare the performance of both classifiers in the 2-feature scenario with the performance in the 200-feature scenario and explain any differences you might observe. <ins>[1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-d-fsYxb5KTg"},"source":["## 4. Exploring ROC curves"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ds3GtA5z5Qj1"},"source":["*   In part 2, we worked with imbalanced datasets. Specifically, we\n","\n","---\n","\n","\n","\n","---\n","\n","\n","observed that minority classes are likely to be misclassified. One way to\n","address this is to adjust the boundary of the classifier.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LpbiVeVs5vGI"},"source":["*   The following cell generates samilar data that we used in the previous part (with 2 dimensions only) and will train maxEnt and Bayes models on its training data."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1hhQfoI_MgH_","colab":{}},"source":["import numpy as np\n","\n","np.random.seed(2)\n","\n","dim = 2     \n","m1, m2, s1, s2 = [1, 0, 1, 1]\n","numberInstances=50\n","\n","Ytr = np.append(np.ones(numberInstances, dtype = int), np.zeros(numberInstances, dtype = int), axis = 0)\n","Xtr = np.append(m1 + s1 * np.random.randn(numberInstances, dim),  m2 + s2 * np.random.randn(numberInstances, dim), axis = 0)\n","Yte = np.append(np.ones(numberInstances, dtype = int), np.zeros(numberInstances, dtype = int), axis = 0)\n","Xte = np.append(m1 + s1 * np.random.randn(numberInstances, dim),  m2 + s2 * np.random.randn(numberInstances, dim), axis = 0)\n","\n","\n","log_reg_classifier = LogisticRegression(solver='lbfgs')\n","log_reg_classifier.fit(Xtr, Ytr)\n","\n","gnb_classifier = GaussianNB()\n","gnb_classifier.fit(Xtr, Ytr)\n","\n","print('Done')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v6yhmVSh51HF"},"source":["*   In the following, we change the decision boundary by\n","adjusting a threshold that by default is 0.5. Try different values for the\n","threshold and observe how the confusion matrix and accuracy change. By\n","changing the threshold value (experiment with it!), you can make the classifier prefer class 1\n","over class 2, which could be useful in an application where they have\n","varying importance.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jm5aOFkONvXP","colab":{}},"source":["y_test_probilities_LogisticRegression = log_reg_classifier.predict_proba(Xte)[:,1]\n","y_test_probilities_NaiveBayes = gnb_classifier.predict_proba(Xte)[:,1]\n","\n","thr = 0.3\n","\n","from sklearn.metrics import recall_score, accuracy_score, precision_score, confusion_matrix\n","\n","y_test_pred_LR = y_test_probilities_LogisticRegression > thr\n","y_test_pred_NB = y_test_probilities_NaiveBayes > thr\n","\n","\n","print('with threshold {}, the accuracy score of Naive Bayes (on test data) is: \\t{:.2f}'.format(thr, accuracy_score(Yte,y_test_pred_NB)))\n","print('with threshold {}, the recall score of Naive Bayes (on test data) is: \\t{:.2f}'.format(thr, recall_score(Yte,y_test_pred_NB)))\n","print('with threshold {}, the precision score of Naive Bayes (on test data) is: \\t{:.2f}'.format(thr, precision_score(Yte,y_test_pred_NB)))\n","print('-'*20)\n","print('with threshold {}, the accuracy score of Logistic Regression (on test data) is: \\t{:.2f}'.format(thr, accuracy_score(Yte,y_test_pred_LR)))\n","print('with threshold {}, the recall score of Logistic Regression (on test data) is: \\t{:.2f}'.format(thr, recall_score(Yte,y_test_pred_LR)))\n","print('with threshold {}, the precision score of Logistic Regression (on test data) is: \\t{:.2f}'.format(thr, precision_score(Yte,y_test_pred_LR)))\n","\n","import pandas as pd\n","\n","print('\\n'+'='*30+'\\n')\n","print('with threshold {}, confusion matrix on test data for Naive Bayes classifier:'.format(thr))\n","print(pd.DataFrame(confusion_matrix(Yte, y_test_pred_NB),\n","                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n","\n","print('-'*20)\n","\n","print('with threshold {}, confusion matrix on test data for Logistic Regression classifier:'.format(thr))\n","print(pd.DataFrame(confusion_matrix(Yte, y_test_pred_LR),\n","                 columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Aol-PNTkaw9v"},"source":["---\n","> **Q4:** Describe and then briefly explain the trend in the accuracy, precision and recall of a classified with respect to the value of threshold\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5oQNIAPbbSxe"},"source":["> **A4:**\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P1epjOjY56y3"},"source":["*   The following code, [modified from here](https://towardsdatascience.com/fine-tuning-a-classifier-in-scikit-learn-66e048c21e65), generates and plots the ROC curve, which is just false-positive-rates and true positive-rates for different values of thresholds against each other, and reports the AUC:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hIRvV29R5Ma0","colab":{}},"source":["from sklearn.metrics import roc_curve, auc\n","\n","\n","plt.figure(figsize=(8,8))\n","plt.title('ROC Curve')\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.axis([-0.005, 1, 0, 1.005])\n","plt.xticks(np.arange(0,1, 0.05), rotation=90)\n","plt.xlabel(\"False Positive Rate\")\n","plt.ylabel(\"True Positive Rate (Recall)\")\n","    \n","\n","fpr, tpr, auc_thresholds = roc_curve(Yte, y_test_probilities_LogisticRegression)\n","print(auc(fpr, tpr))\n","plt.plot(fpr, tpr, linewidth=3, alpha=0.7, label='Logistic Regression')\n","\n","fpr, tpr, auc_thresholds = roc_curve(Yte, y_test_probilities_NaiveBayes)\n","print(auc(fpr, tpr)) \n","plt.plot(fpr, tpr, linewidth=3, alpha=0.7, label='Naive Bayes')\n","\n","plt.legend(loc='best')\n","\n","\n"],"execution_count":null,"outputs":[]}]}