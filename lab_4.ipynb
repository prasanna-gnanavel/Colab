{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"name":"lab_4.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"6P7xCmE5F04y"},"source":["# Lab session 4: data warehousing and online analytical processing"]},{"cell_type":"markdown","metadata":{"id":"Aecqc8P4F041"},"source":["## Introduction \n","\n","The purpose of this lab session is to provide you with an opportunity to gain experience with **data warehousing** and **online analytical processing (OLAP)**, and more specifically with the concepts of **data cubes**, **data cube measures**, **typical OLAP operations**, and **data cube computation**.\n","\n","This session starts with a tutorial that uses examples to introduce you to the practical knowledge that you will need for the corresponding assignment. We highly recommend that you read the following tutorials if you need a gentler introduction to the libraries that we use:\n","- [Cubes](https://cubes.readthedocs.io/en/v1.0.1/tutorial.html)\n","- [Numpy quickstart tutorial](https://numpy.org/devdocs/user/quickstart.html)\n","- [Numpy: basic broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n","- [Pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html)\n","- [Matplotlib](https://matplotlib.org/tutorials/introductory/pyplot.html)\n","- [Seaborn](https://seaborn.pydata.org/tutorial/relational.html)\n","- [Scikit-learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n"]},{"cell_type":"markdown","metadata":{"id":"imm694GAF043"},"source":["## Submission instructions\n","\n","- The last section of this notebook includes the second part of the assessed assignment for day 2.\n","- There will be 4 assessed assignments. Each assignment corresponds to 10% of your final grade.\n","- The exam corresponds to the remaining 60% of your final grade.\n","- **PLAGIARISM** <ins>leads to irreversible non-negotiable failure in the module</ins> (if you are unsure about what constitutes plagiarism, please ask). \n","- The deadline for submitting each assignment is defined on QM+.\n","- Penalties for late submissions will be applied in accordance with the School policy. The submission cut-off date is 7 days after the deadline.\n","- You should submit a **report** with your solutions to the assignment included in the last section of this notebook. The report should be a single **pdf** file. Other formats (such as doc, docx, or ipynb) are **not** acceptable. \n","- The report should be excellently organized and identified with your name, student number, assignment number, and module identifier. When applicable, question numbers should precede the corresponding answers. \n","- Please name your report file according to the following convention: Assignment[Number]-[Student Name]-[Student Number].pdf\n","- Submissions should be made through QM+. Submissions by e-mail will be ignored.\n","- Please always check whether the report file was uploaded correctly to QM+.\n","- Cases of **extenuating circumstances** have to go through the proper procedure in accordance with the School policy. Only cases approved by the School in due time will be considered."]},{"cell_type":"markdown","metadata":{"id":"ACVqzQGkF044"},"source":["## 1. Introduction to Cubes\n","\n","This chapter describes how to use ``cubes`` (http://cubes.databrewery.org/), a lightweight Python framework and set of tools for the development of reporting and analytical applications, online analytical processing (OLAP), multidimensional analysis, and browsing of aggregated data. ``cubes`` features:\n","- a logical view of analysed data\n","- OLAP and aggregated browsing (default backend is for relational database: ROLAP)\n","- hierarchical dimensions (attributes that have hierarchical dependencies, such as category-subcategory or country-region)\n","- multiple hierarchies in a dimension\n","- localizable metadata and data \n","- authentication and authorization of cubes and their data\n","- pluggable data warehouse\n","\n","``cubes`` is meant to be used by application builders that want to provide analytical functionality. ``cubes`` also relies on methods from SQLAlchemy (https://www.sqlalchemy.org/), an open-source SQL toolkit and object-relational mapper for Python.\n","\n","In order to install ``cubes`` in your virtual machine, run the following command:"]},{"cell_type":"code","metadata":{"id":"6uzsWPNyF045","executionInfo":{"status":"ok","timestamp":1605177898610,"user_tz":0,"elapsed":3255,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}},"outputId":"1fd66951-a29c-4943-f9f6-c8d13cc389ca","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install cubes"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: cubes in /usr/local/lib/python3.6/dist-packages (1.1)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from cubes) (2.6.0)\n","Requirement already satisfied: expressions>=0.2.3 in /usr/local/lib/python3.6/dist-packages (from cubes) (0.2.3)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from cubes) (2.8.1)\n","Requirement already satisfied: grako>=3.9.3 in /usr/local/lib/python3.6/dist-packages (from expressions>=0.2.3->cubes) (3.99.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->cubes) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DDmEVOVhF05A"},"source":["## 2. Data Preparation\n","\n","This example uses the CSV file ``data/IBRD_Balance_Sheet__FY2010.csv`` (International Bank for Reconstruction and Development Balance Sheet), which should be uploaded to the virtual machine. The CSV file includes records which are characterised by a Category (and subcategories), Line Item, Fiscal Year, and Amount (in millions of dollars). We first start with imports:"]},{"cell_type":"code","metadata":{"id":"5vj9tOLwF05C","executionInfo":{"status":"ok","timestamp":1605177904378,"user_tz":0,"elapsed":568,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}}},"source":["from sqlalchemy import create_engine\n","from cubes.tutorial.sql import create_table_from_csv"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Czqd2busF05I"},"source":["We can now load the data, create a table, and populate it with contents of the CSV file:"]},{"cell_type":"code","metadata":{"id":"WwV6ih7hF05J","executionInfo":{"status":"ok","timestamp":1605178011561,"user_tz":0,"elapsed":1398,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}}},"source":["engine = create_engine('sqlite:///data.sqlite')\n","create_table_from_csv(engine,\n","                      \"IBRD_Balance_Sheet__FY2010.csv\",\n","                      table_name=\"ibrd_balance\",\n","                      fields=[\n","                          (\"category\", \"string\"),\n","                          (\"category_label\", \"string\"),\n","                          (\"subcategory\", \"string\"),\n","                          (\"subcategory_label\", \"string\"),\n","                          (\"line_item\", \"string\"),\n","                          (\"year\", \"integer\"),\n","                          (\"amount\", \"integer\")],\n","                      create_id=True\n","                     )"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4B9EHryF05P"},"source":["## 3. Creating a data cube\n","\n","Everything in ``cubes`` happens in an *analytical workspace*. It contains cubes, maintains connections to the data stores (with cube data), provides connection to external cubes, and more.\n","\n","The first thing that we have to do is to specify a data store: a database which will host the cubeâ€™s data."]},{"cell_type":"code","metadata":{"id":"ngU9UQJXF05Q","executionInfo":{"status":"ok","timestamp":1605178016707,"user_tz":0,"elapsed":627,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}}},"source":["from cubes import Workspace\n","\n","workspace = Workspace()\n","workspace.register_default_store(\"sql\", url=\"sqlite:///data.sqlite\")"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6qoEC82fF05U"},"source":["The structure of data cubes (in terms of dimensions, measures, and aggregates) is specified in JSON files. We now import file 'data/tutorial_model.json' (as usual, this file should be uploaded to the virtual machine), which includes an example model of the data cube, dimension tables, and aggregate functions for the CSV file we loaded previously."]},{"cell_type":"code","metadata":{"id":"ynPcDlZYF05V","executionInfo":{"status":"ok","timestamp":1605178020801,"user_tz":0,"elapsed":501,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}}},"source":["workspace.import_model(\"tutorial_model.json\")"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EnVtZxa3F05Y"},"source":["**Please make sure to inspect the structure of the JSON file above. This will be relevant for one of the assignment questions.**\n","\n","We can now create a data cube based on the above data cube model and data table:"]},{"cell_type":"code","metadata":{"id":"uxhCIcl8F05Z","executionInfo":{"status":"ok","timestamp":1605178024531,"user_tz":0,"elapsed":546,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}}},"source":["cube = workspace.cube(\"ibrd_balance\")"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J2D5SLjxF05d"},"source":["## 4. Aggregations and OLAP operations\n","\n","A *browser* is an object that does the actual aggregations and other data queries for a cube. To obtain one:"]},{"cell_type":"code","metadata":{"id":"wh0MvwIdF05d","executionInfo":{"status":"ok","timestamp":1605178027138,"user_tz":0,"elapsed":559,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}}},"source":["browser = workspace.browser(cube)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jp9wIz6tF05h"},"source":["We can now compute aggregates of the data cube as specified by the data cube model. For computing the total count of records:"]},{"cell_type":"code","metadata":{"id":"bJvSMvDUF05i","executionInfo":{"status":"ok","timestamp":1605178029790,"user_tz":0,"elapsed":544,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}},"outputId":"7353a0d3-08a3-4617-f10f-798f686b1da7","colab":{"base_uri":"https://localhost:8080/"}},"source":["result = browser.aggregate()\n","result.summary[\"record_count\"]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["62"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"tVpXyCXHF05m"},"source":["For computing a sum of the amount:"]},{"cell_type":"code","metadata":{"id":"FdrQVjKIF05n","executionInfo":{"status":"ok","timestamp":1605178033293,"user_tz":0,"elapsed":681,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}},"outputId":"9bbbfaf4-ff70-4056-ddf0-7ab941f9f1f4","colab":{"base_uri":"https://localhost:8080/"}},"source":["result.summary[\"amount_sum\"]"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1116860"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"lbQXtbohF05r"},"source":["Now we can try to compute aggregates by year:"]},{"cell_type":"code","metadata":{"id":"kSS84DfkF05r","executionInfo":{"status":"ok","timestamp":1605178036087,"user_tz":0,"elapsed":559,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}},"outputId":"7903bb78-e2be-4cdc-90a0-a4ca8f2abbdc","colab":{"base_uri":"https://localhost:8080/"}},"source":["result = browser.aggregate(drilldown=[\"year\"])\n","for record in result:\n","    print(record)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["{'year': 2009, 'amount_sum': 550840, 'record_count': 31}\n","{'year': 2010, 'amount_sum': 566020, 'record_count': 31}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6t03b2rmF05v"},"source":["Or compute aggregates by item category:"]},{"cell_type":"code","metadata":{"id":"jaj-DEoMF05v","executionInfo":{"status":"ok","timestamp":1605178042159,"user_tz":0,"elapsed":671,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}},"outputId":"fc15c95c-2931-4136-cacc-c962dce797bb","colab":{"base_uri":"https://localhost:8080/"}},"source":["result = browser.aggregate(drilldown=[\"item\"])\n","for record in result:\n","    print(record)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["{'item.category': 'a', 'item.category_label': 'Assets', 'amount_sum': 558430, 'record_count': 32}\n","{'item.category': 'e', 'item.category_label': 'Equity', 'amount_sum': 77592, 'record_count': 8}\n","{'item.category': 'l', 'item.category_label': 'Liabilities', 'amount_sum': 480838, 'record_count': 22}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"J3bxvXWvF050"},"source":["We can also perform *slicing* and *dicing* operations on the data cube. The example below performs a slicing operation on the data cube by selecting only entries with the year being 2009, and displays aggregates according to the item category. Here, a *cell* defines a point of interest: a portion of the cube to be aggregated or browsed."]},{"cell_type":"code","metadata":{"id":"Wek5qDqUF050","executionInfo":{"status":"ok","timestamp":1605178069055,"user_tz":0,"elapsed":540,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}},"outputId":"339bb087-d29f-4f94-fcc7-d212bf2beb62","colab":{"base_uri":"https://localhost:8080/"}},"source":["import cubes\n","cuts = [cubes.PointCut(\"year\", [\"2009\"])]\n","cell = cubes.Cell(cube, cuts)\n","result = browser.aggregate(cell, drilldown=[\"item\"])\n","for record in result:\n","    print(record)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["{'item.category': 'a', 'item.category_label': 'Assets', 'amount_sum': 275420, 'record_count': 16}\n","{'item.category': 'e', 'item.category_label': 'Equity', 'amount_sum': 40037, 'record_count': 4}\n","{'item.category': 'l', 'item.category_label': 'Liabilities', 'amount_sum': 235383, 'record_count': 11}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ALf2IBNbF053"},"source":["It's worth noting that slicing operations in ``cubes`` can be created by either specifying a \"point cut\", which selects a single value of an attribute in a given dimension (using the ``cubes.PointCut`` function as above), or by specifying a \"range cut\", which selects a range of values for a given dimension. The range cut uses the ``cubes.RangeCut`` function, which takes as input the attribute name, the minimum value of the specified range, and the maximum value of the range.\n","\n","Similarly, we can perform a *dicing* operation on the data cube by performing a selection on two or more dimensions. The example below performs a dicing operation on the data cube, selecting entries with the year being 2009 and the item category being \"a\", and displays the aggregate results:"]},{"cell_type":"code","metadata":{"id":"ZUlkLhPNF054","executionInfo":{"status":"ok","timestamp":1605178106393,"user_tz":0,"elapsed":681,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}},"outputId":"d5b7308f-5a26-43b9-a632-58be0e4ce4c3","colab":{"base_uri":"https://localhost:8080/"}},"source":["cuts = [cubes.PointCut(\"year\", [\"2009\"]),cubes.PointCut(\"item\", [\"a\"])]\n","cell = cubes.Cell(cube, cuts)\n","result = browser.aggregate(cell,drilldown=[\"item\"])\n","result.summary"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'amount_sum': 275420, 'record_count': 16}"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"xxOTsFP1F057"},"source":["We can also *drill down* lower in the Category hierarchy. Here, we perform a dicing operation to select records with year being 2009 and item category being \"a\" (corresponding to assets), and show aggregates for each subcategory level."]},{"cell_type":"code","metadata":{"id":"6zOLsQ_CF057","executionInfo":{"status":"ok","timestamp":1605178133450,"user_tz":0,"elapsed":551,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}},"outputId":"62d4c1ca-abd7-4334-bc79-cd53dcc83043","colab":{"base_uri":"https://localhost:8080/"}},"source":["cuts = [cubes.PointCut(\"year\", [\"2009\"]),cubes.PointCut(\"item\", [\"a\"])]\n","cell = cubes.Cell(cube, cuts)\n","result = browser.aggregate(cell,drilldown=[\"item:subcategory\"])\n","for record in result:\n","    print(record)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'da', 'item.subcategory_label': 'Derivative Assets', 'amount_sum': 123065, 'record_count': 4}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'dfb', 'item.subcategory_label': 'Due from Banks', 'amount_sum': 3044, 'record_count': 2}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'i', 'item.subcategory_label': 'Investments', 'amount_sum': 41012, 'record_count': 1}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'lo', 'item.subcategory_label': 'Loans Outstanding', 'amount_sum': 103657, 'record_count': 1}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'nn', 'item.subcategory_label': 'Nonnegotiable', 'amount_sum': 1202, 'record_count': 1}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'oa', 'item.subcategory_label': 'Other Assets', 'amount_sum': 2247, 'record_count': 3}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'orcv', 'item.subcategory_label': 'Other Receivables', 'amount_sum': 984, 'record_count': 2}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 'rcv', 'item.subcategory_label': 'Receivables', 'amount_sum': 176, 'record_count': 1}\n","{'item.category': 'a', 'item.category_label': 'Assets', 'item.subcategory': 's', 'item.subcategory_label': 'Securities', 'amount_sum': 33, 'record_count': 1}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d2q2p8ewF059"},"source":["# Assignment 2 [Part 2/2]\n","\n","1. A data warehouse for a music streaming company consists of the dimensions song, user, time (time and date of when the user listened to a song), and the two measures count (how many times a user listened to the song) and fee (fee paid by the streaming company to the artist every time a user listens to that song). \n","  1. Draw a schema diagram for the above data warehouse using either a star, snowflake, or fact constellation schema. [0.5/5]\n","  2. Starting with the base cuboid [time, user, song], what specific OLAP operations should be performed in order to list the total fee collected for a given song for a given month of a given year (e.g., October 2020)? [1/5]\n","\n","\n","2. Suppose that we have access to a data cube that contains information on rainfall for specific regions. The data cube has dimensions region, precipitation, and time.\n","  1. Assuming that we would like to compute the total amount of rainfall for a given region and month, which *data cube measure* would we use? To which category of data cube measures does this particular measure fall into? [0.25/5]\n","  2. Assuming that we would like to compute the average rainfall for a given region and month, which *data cube measure* would we use? To which category of data cube measures does this particular measure fall into? [0.25/5]\n","    \n","\n","3. Using the same CSV file and data cube in the tutorial above, modify the \"tutorial_model.json\" file to include aggregate measures for the minimum and maximum amount in the data cube. Using these implemented aggregate measures, produce the values for the minimum and maximum amount in the data per year. Make sure to show your work in the report. [1.0/5]\n","\n","\n","4. Using the CSV file ``data/country-income.csv``, perform the following:\n","  1. Load the CSV file using Cubes, create a JSON file for the data cube model, and create a data cube for the data. Use as dimensions the region, age, and online shopper fields. Use as measure the income. Define aggregate functions in the data cube model for the total, average, minimum, and maximum income. In your report, show the relevant scripts and files. [1.0/5]\n","  2. Using the created data cube and data cube model, produce aggregate results for the whole data cube; results per region; results per online shopping activity; and results for all people aged between 40 and 50. [1/5]\n","  "]},{"cell_type":"code","metadata":{"id":"f2idQaw9F05-"},"source":[""],"execution_count":null,"outputs":[]}]}