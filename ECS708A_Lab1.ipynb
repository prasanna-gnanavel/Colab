{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ECS708A_Lab1.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qdHbgJ4EHmij"},"source":["# <font color='gray'> Lab 1: Regression </font>"]},{"cell_type":"markdown","metadata":{"id":"Ln31kjtrKhgK"},"source":["## Introduction \n","\n","The aim of this lab is to get familiar with **regression problems**, the concepts of **under/over-fitting**, and **cross-validation**. \n","\n","- This lab constitutes your first course-work activity.\n","- A report answering the <font color = 'red'>**questions in</font><font color = \"maroon\"> red**</font> only should be submitted by the 8th of March. \n","- The report should be a separate file in **pdf format** (so **NOT** *doc, docx, notebook* etc.), well identified with your name, student number, assignment number (for instance, Assignment 1), module code. \n","- No other means of submission other than the appropriate QM+ link is acceptable at any time (so NO email attachments, etc.)\n","- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n"]},{"cell_type":"markdown","metadata":{"id":"w7L4DNPvPxf9"},"source":["## 0. Regression and Over-fitting\n"]},{"cell_type":"markdown","metadata":{"id":"Nm294F7-Zmrw"},"source":["The\t point\t of\t this\texercise\tis\t to\t understand\t\n",">i)\t**over-fitting**;\n","<br>\n",">ii)\thow **over-fitting affects\t train\t and\t test\t error** differently,\t and;\n","<br>\n","iii)\t how\t it\t depends\t on\t the **complexity of the model**."]},{"cell_type":"markdown","metadata":{"id":"k3RNGFfzWci8"},"source":["0.  First, let's \"load\" our data. The first exercise is very simple, so we are going to give the data explicitly as follows:"]},{"cell_type":"code","metadata":{"id":"MSPDh4hhR841"},"source":["# traing data\n","xTrain =[0.3000, -0.7700, 0.9000, -0.0400, 0.7400, -0.5800, -0.9200, -0.2100, -0.5400, 0.6800]\n","yTrain =[1.1492, 0.3582, 1.9013, 0.9487, 1.3096, 0.9646, 0.1079, 1.1262, 0.6131, 1.0951]\n"," \n","# testing date\n","xTest =[0.1100, 0.2300, 0.1800, 0.3500, 0.5200, -0.4400, -0.6900, -0.2400, 0.4300, -0.4100, 0.3300, 0.8800, -0.4300, 0.5600, 0.6600, -0.0100, -0.8300, 0.5700, 0.3400, 0.6700]\n","yTest =[1.0569, 1.0647, 0.9575, 1.2097, 0.8371, 0.8573, 0.6128, 1.1087, 0.9253, 0.9788, 1.0590, 1.6263, 0.7660, 1.0799, 1.3341, 0.6867, 0.3657, 1.1747, 1.0440, 1.1315]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XGNzyWu4XJul"},"source":[" 1. We have loaded four variables. What are they? Let's explore one of them:"]},{"cell_type":"code","metadata":{"id":"T80DcTU4Xwet"},"source":["print(xTrain)\n","print(len(xTrain))\n","print(len(xTest))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0IxN5LBXXy_d"},"source":["---\n","> **Q0:** Note that the dimensions of the `xTrain` are 10x1. \n","This means we are dealing with a ?-dimensional regression problem with ?? instances."]},{"cell_type":"markdown","metadata":{"id":"gRYOXxLl3rMz"},"source":["Total instances is 10"]},{"cell_type":"markdown","metadata":{"id":"lPPui6Q2-Fi2"},"source":["2. Let's plot our train and test data-sets. The x-axis will be our predictor attribute and the y-axis is the label (target). The train and test data-sets are plotted with different markers:"]},{"cell_type":"code","metadata":{"id":"2n98UA8Y-hYd"},"source":["import matplotlib.pyplot as plt\n","\n","# plt.figure(figsize=(7,7))\n","plt.plot(xTrain, yTrain, 'o', label=\"Train Data\")\n","plt.plot(xTest, yTest, '*', label=\"Test Data\")\n","plt.xlabel(\"x (attribute)\", fontsize=18)\n","plt.ylabel(\"y (label)\", fontsize=18)\n","plt.xlim(-1,1)\n","plt.ylim(-0.5,2.5)\n","# plt.gca().set_aspect('equal')\n","plt.grid(alpha=0.2)\n","plt.legend(fontsize=12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SBSeW7KAEmbi"},"source":["3. The following code **fits** a 1$^{st}$ order polynomial (i.e., a simple line) to the presented data using least squares. This means that it finds the line that predicts the label in the train data with minimum mean squared error (MSE). We also print the corresponding equation with the estimated values for the parameters of that equation (i.e., the learned model) along with the MSE in the train and test data. "]},{"cell_type":"code","metadata":{"id":"4vDmxUwQddIy","cellView":"both"},"source":["import numpy as np\n","\n","order = 1\n","p = np.poly1d(np.polyfit(xTrain, yTrain, order))\n","t = np.linspace(-1, 1, 201)\n","\n","plt.plot(xTrain, yTrain, 'o', label=\"Train Data\")\n","plt.plot(xTest, yTest, '*', label=\"Test Data\")\n","plt.plot(t, p(t), '-', label=\"Fitted Model\", linewidth=2)\n","plt.xlabel(\"x (attribute)\", fontsize=18)\n","plt.ylabel(\"y (label)\", fontsize=18)\n","plt.xlim(-1,1)\n","plt.ylim(-0.5,2.5)\n","# plt.gca().set_aspect('equal')\n","plt.grid(alpha=0.2)\n","plt.legend(fontsize=12)\n","plt.show()\n","\n","print('The learned model is:\\n {}\\n---'.format(p))\n","    \n","MSE_Train = sum((p(x) - y)**2 for x, y in zip(xTrain, yTrain))/len(yTrain)\n","print('Train MSE: {:8.5f}'.format(MSE_Train))\n","\n","MSE_Test = sum((p(x) - y)**2 for x, y in zip(xTest, yTest))/len(yTest)\n","print('Test MSE: {:8.5f}'.format(MSE_Test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uBC-BGDG3WPN"},"source":["---\n","> **Q1:** There is an intentional mistake in the code: the mean-squared-error (MSE) for the test dataset is computed wrongly. Fix it! \n","\n","  *Hint 1: you can get the idea from how the MSE for the train data is computed, and write the code for the MSE_Test accordingly.*\n","\n","  *Hint 2: if you are still in doubt, look at the code cell in the next task, you can actually find it there, but use it as a double-check.*\n","\n","  *Hint 3: note that indentation is very important in Python. There are no `begin` and `end` or curly brackets { } in Python, and code blocks are demarcated by their indentation (loops, if statements, body of functions, etc).*"]},{"cell_type":"markdown","metadata":{"id":"UoVzphxEIGIY"},"source":["> **Q2:** Which function in the above code is responsible for finding the best model for a given order (i.e., fit the model)? \n"]},{"cell_type":"markdown","metadata":{"id":"8w-qJxp5IyKa"},"source":["The numpy polyfit code is responsible to find the best model for a given order. Poly1d forms the equation with the weightage produced by polyfit function.\n","\n","\n","np.polyfit(xTrain, yTrain, order)"]},{"cell_type":"markdown","metadata":{"id":"biRUSEDKIztq"},"source":["> **Q3:** Edit the code so that you find the best model for higher degrees of polynomials. That is, fit 2$^{nd}$, 3$^{rd}$, ..., 9$^{th}$ order polynomials to the training data. What do you observe about the different fits in terms of their match to training and test data, respectively? "]},{"cell_type":"markdown","metadata":{"id":"j_R2CndbJsbk"},"source":["> **A3:** \n","--- "]},{"cell_type":"markdown","metadata":{"id":"TMPpV1UZLrLY"},"source":["4. Let's put all of the polynomial orders side-by-side for an easier comparison! Run the following code block **twice** (to get the tabular view). You can then change between the tabs. Note the train MSE and test MSE for each fit.\n"]},{"cell_type":"code","metadata":{"id":"gwPZ3bnF_dpw"},"source":["# Don't forget to run this cell twice to get the tabs (some weird bug with the \n","# colab interface it seems!) If you don't get the tab view, it is fine, you can\n","# stil analyse the outputs!\n","\n","from google.colab import widgets\n","\n","orders = range(1,10)\n","t = np.linspace(-1, 1, 201)\n","\n","tb = widgets.TabBar([str(order) for order in orders])\n","for order in orders:\n","  with tb.output_to(str(order), select= (order < 2)):\n","    p = np.poly1d(np.polyfit(xTrain, yTrain, int(order)))\n","\n","    plt.plot(xTrain, yTrain, 'o', label='Train data')\n","    plt.plot(xTest, yTest, '*', alpha=0.6, label='Test data')\n","    plt.plot(t, p(t), '-', alpha=0.8, label='Learned model', linewidth=2)\n","    plt.xlabel(\"x\", fontsize=14)\n","    plt.ylabel(\"y\",fontsize=14)\n","    plt.xlim(-1,1)\n","    plt.ylim(-0.5,2.5)\n","    # plt.gca().set_aspect('equal')\n","    plt.grid(alpha=0.2)\n","    plt.legend(loc='upper left', fontsize=12)\n","    plt.show()\n","    \n","    print('The learned (fitted) model is:\\n {}\\n---'.format(p))\n","\n","    MSE_Train = sum((p(x) - y)**2 for x, y in zip(xTrain, yTrain))/len(yTrain)\n","    print('Train MSE: {:8.5f}'.format(MSE_Train))\n","\n","    MSE_Test = sum((p(x) - y)**2 for x, y in zip(xTest, yTest))/len(yTest)\n","    print('Test MSE: {:8.5f}'.format(MSE_Test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zlPPflCKPK83"},"source":["5. Let's make the comparison of MSE values more systematic: let's save the MSE of each model for both train and test data into two lists and plot them. The x-axis is the order of the polynomial of the fitted model, a measure of complexity (flexibility) of the model:"]},{"cell_type":"code","metadata":{"id":"TJcBEZe_WTxx"},"source":["orders = range(1,10)\n","MSE_Trains = dict.fromkeys(orders)\n","MSE_Tests = dict.fromkeys(orders)\n","print(MSE_Trains)\n","for order in orders:\n","  p = np.poly1d(np.polyfit(xTrain, yTrain, int(order)))\n","  MSE_Trains[order] = sum((p(x) - y)**2 for x, y in zip(xTrain, yTrain))/len(yTrain)\n","  MSE_Tests[order] = sum((p(x) - y)**2 for x, y in zip(xTest, yTest))/len(yTest)\n","  \n","plt.plot(orders, list(MSE_Trains.values()), '--o', label='MSE Train', linewidth=2)\n","plt.plot(orders, list(MSE_Tests.values()), '--*', label='MSE Test', linewidth=2)\n","\n","plt.title('''Fig 0: Comparison of Train and Test MSE (no regularisation).\\n\n","You should be able to describe and explain the general trend in each graph, \n","and identify the cases of underfitting and overfitting.\\n''', fontsize=14)\n","plt.xlabel(\"Order (degree of the fitted polynomial)\", fontsize=12)\n","plt.ylabel(\"Mean Squared Error (MSE)\", fontsize=12)\n","plt.xlim(1,9)\n","plt.ylim(0,0.1)\n","plt.grid(alpha=0.2)\n","plt.legend(fontsize=14)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xc53RPzaPndH"},"source":["\n","\n","#### <font color=\"maroon\"><b>Exercise 0: </b> Which polynomial has the lowest `train` MSE? Which one has the lowest `test` MSE? <b>[0.5 mark]</b> </font>\n"]},{"cell_type":"markdown","metadata":{"id":"uMayN6xKQCdF"},"source":["The lowest MSE is for order number 9\n","\n","The lowest test MSE is for order number 3 "]},{"cell_type":"markdown","metadata":{"id":"G37HPZaxPnJn"},"source":["####<font color=\"maroon\"><b>Exercise 1: </b> What trend do you observe when you analyse the dependence of train and test MSE on the polynomial order? First describe the observed trends, and then explain them. <b>[1 mark]</b>  </font>"]},{"cell_type":"markdown","metadata":{"id":"Y-94hEkkYMou"},"source":["Prss: The test and train data have low MSE at order number 3\n","\n","The train MSE value starts decreasing from order no 4 to 8. It reaches zero at order number 9. We observer overfitting from order number 4 onwards as variation between test data and train data start increasing.\n","\n","Uderfitting:In order number 1 and 2 MSE values are high in both test and train data.\n"]},{"cell_type":"markdown","metadata":{"id":"NsNx0hmqUtHq"},"source":["####<font color=\"maroon\"><b>Exercise 2: </b> Identify the models that are suffering from <u>under-fitting</u> and the ones suffering from <u>over-fitting</u>. Justify your choice based on your observations and use the theory that we have learnt to explain it. <b>[0.5 mark]</b>  </font>"]},{"cell_type":"markdown","metadata":{"id":"fDWqaIr7ZhGw"},"source":["The train MSE value starts decreasing from order no 4 to 8. It reaches zero at order number 9. We observer overfitting from order number 4 onwards as variation between test data and train data start increasing.\n","\n","Uderfitting:In order number 1 and 2 MSE values are high in both test and train data.\n","\n","At order number 3 MMSE is lowest and not much variation between train data and test data."]},{"cell_type":"markdown","metadata":{"id":"1noA8yFATSK2"},"source":["####<font color=\"maroon\"><b>Exercise 3: </b> Which model would you pick as the best one amongst these 9 models? What are the parameter(s) and hyper-parameter(s) of your chosen model? <b>[0.5 mark]</b>  </font>\n","---\n","---"]},{"cell_type":"markdown","metadata":{"id":"X7t9bpyGZ9mE"},"source":["Prss: \n","The order number 3 chosen.\n","\n","Parameters are :\n","The learned (fitted) model is:\n","        3           2\n","1.326 x - 0.09076 x - 0.1449 x + 1.004\n","\n","Hyperparameter: \n","Train MSE:  0.01976\n","Test MSE:  0.01632P"]},{"cell_type":"markdown","metadata":{"id":"1o_AbBOg-97W"},"source":["## 1. Regularisation "]},{"cell_type":"markdown","metadata":{"id":"6IjtVOUOWlOl"},"source":["The aim of this exercise is to see the behind-the-scenes code for linear and polynomial regression that corresponds to the \n","equations\tseen in the lecture. Moreover, we will see the fantastic role that regularisation techniques can play in fighting against over-fitting and getting models with improved generalisation (lower variance)."]},{"cell_type":"markdown","metadata":{"id":"-KnxdjqkXtSg"},"source":["0. The following script defines a function that we will use to generate training and test samples from an underlying true model that we want to learn. The true model is a polynomial, and we generate our dataset instances by sampling from this true model and adding some noise to it. (Note that this is of course an artificial set-up, with the purpose of this lab exercise, to see the effect of regularisation. In a practical scenario, we don't know the true model and our task is to indeed \"learn\" it) \n","\n","  Run the following code cell to load the function **definition** (note that there will not be any output). \n","\n"]},{"cell_type":"code","metadata":{"id":"0ZR50MWx_BZr"},"source":["# You don't have to be concerned about the detail of the following function!\n","\n","def utilCreatePolyData(nTrain=10, \n","                       nTest=20,\n","                       PolynomialCoefficients=[1,0,0,1],\n","                       rndseed=0):\n","  '''\n","  Function to generate some artificial samples for our test and train data.\n","  The data is generated by sampling from a true underlying model as a polynomial \n","  and adding some noise (Gaussian with mean zero and `sigma`=0.1) to them.\n","  \n","  Parameters:\n","    nTrain (int) : number of the training data points to generate\n","    nTest (int) : number of the test data points to generate\n","    PolynomialCoefficients (list) : the coefficients of the true polynomial\n","    rndseed (int) : seed for the random number generator --for repeatability--\n","  \n","  Retunrns:\n","    xTrain, yTrain (list) : training attributes and labels (with length of `nTrain`)\n","    xTest, yTest (list) : test attributes and labels (with length of `nTest`)\n","    xTrue, yTrue (list) : samples from the true underlying model (length of 201)\n","  '''\n","  \n","  np.random.seed(rndseed) \n","  \n","  # just to make sure we have enough samples to choose from:\n","  total_number_of_samples = max(201, 10*(nTrain+nTest))\n","  \n","  xTrue  = np.linspace(start=-1, stop=1, num=total_number_of_samples)\n","  yTrue  = np.polynomial.polynomial.polyval(xTrue, PolynomialCoefficients)\n","  sigma    = 0.1\n","  idx    = np.random.permutation(np.arange(total_number_of_samples))\n","  idxTrain = idx[0:nTrain]\n","  xTrain = xTrue[idxTrain] \n","  yTrain = yTrue[idxTrain] + np.random.randn(nTrain)*sigma\n","  \n","  idxTest = idx[nTrain:(nTrain+nTest)]\n","  xTest = xTrue[idxTest]\n","  yTest = yTrue[idxTest] + np.random.randn(nTest)*sigma\n","\n","  return xTrain, yTrain, xTest, yTest, xTrue, yTrue\n","\n","# # for testing the function:\n","# xTrain, yTrain, xTest, yTest, xTrue, yTrue = utilCreatePolyData(nTrain=3, nTest=2)\n","# print(xTrain, yTrain, xTest, yTest)\n","# print( xTrue[:4], yTrue[:4])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J-jKca1Nd0RE"},"source":["1. The following script defines another useful function that extends the attributes with polynomials of that attribute for a given order. For instance, if the samples only have one attribute (so `X` is `N-by-1` where `N` is the number of items in our dataset), then the output of `util_makeGramMatPoly(X, 2)` is an `N-by-3` matrix, where the first column is just all constant ones, the second column is the same attributes as in `X`, and the third column is the square of the values in X. Similarly, `util_makeGramMatPoly(X, 3)` will be an `N-by-4` matrix, where the first column is just all ones, the second column is the original attributes in X, the third column is the squares, and the fourth is their cubes. Recall that this was our trick to apply the same technique for finding the best multivariate linear regression to polynomial regression.\n","\n","  This function is also implemented in the `scikitlearn` library as `sklearn.preprocessing.PolynomialFeatures`. You can read more about it along with examples [on its documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html/). If you are interested in its actual implementation, as with any other Python library, you can check its [source code](https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/preprocessing/data.py#L1326)."]},{"cell_type":"code","metadata":{"id":"OxfyCo0TyS8K"},"source":["# Again, you don't have to be concerned with the detail of this function!\n","\n","from scipy.special import comb as choose\n","from itertools import chain, combinations_with_replacement\n","def util_makeGramMatPoly(X, order):\n","  '''\n","  Returns a new matrix whose columns are the columns of the input matrix `X`, as \n","  well as all the monomials of those columns, up to the degree `order`.\n","\n","  For example, if the columns of X represent attributes of `age` and `weight`,\n","  i.e., if `X = [age, weight]`, then the output of `util_makeGramMatPoly(X, 2)` \n","  will be `[1, age, weight, age**2, age*weight, weight**2]`.\n","\n","  Parameters:\n","    X : numpy array, shape (n_samples, n_features)\n","      The matrix of dataset (size: `N*p`) where `N` is the number of samples \n","      (instances), and each one is described with `p` features (attributes).\n","    \n","    order : int\n","      The maximum degree (maximum power) of the monomial to be built from the \n","      columns of X. \n","\n","  Returns:\n","    X_Extended : numpy array, shape (n_samples, n_extended_features) \n","      The extended dataset extended with monomial features, prepared for \n","      polynomial regression. The number of extended features is the number of \n","      possible ways to construct monomials of up to degree `order` from the \n","      columns of X, including zero powers.`\n","  '''\n","\n","  \n","  if len(np.shape(X)) == 1:\n","    X = np.reshape(X,(-1,1))\n","  \n","  n_samples, n_features = X.shape\n","\n","  # I sat down and worked out the number in closed-form  -- just for fun! -- \n","  n_output_features = int((order+1)*choose(order+n_features, order+1)/n_features)\n","\n","  X_Extended = np.empty(shape=(n_samples, n_output_features))\n","  combinations = chain.from_iterable(combinations_with_replacement(range(n_features), i)\n","                                     for i in range(0, order+1))\n","  for i, comb in enumerate(combinations):\n","    X_Extended[:, i] = X[:, comb].prod(axis=1)\n","                           \n","  return X_Extended\n","\n","\n","# for testing the function:\n","X = np.array([[1,2],[3,4],[-1,5]])\n","print('Example Input:\\n')\n","print(X)\n","print('\\nIts Output:\\n')\n","print(util_makeGramMatPoly(X, 3))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fnwTf5emAHts"},"source":["2. In the following script, we first generate training and test\n","samples from a cubic polynomial (as the true underlying model, with some noise added to it). We then fit polynomials of different degrees\n","to the train data and evaluate it on the test data. There are two differences here compared to the previous section:\n","\n","- We allow the loss function (to quantify the goodness of fit) to have a regularisation term as well as just the mean-squared-error. Recall that this term penalises the use of weights (except for the intercept).  \n","\n","- Instead of using an available high-level function to do the fitting (finding the best model), we directly implement the analytical solution that we presented during the lecture, i.e. the \"closed form\" solution for the weights of the model that yield the minimum mean squred error. \n"]},{"cell_type":"markdown","metadata":{"id":"7ZvEgw28oydu"},"source":["---\n","> **Q4:** Look over the code and identify the line that computes the best weight vector and compare it with the solution to polynomial regression equation in the lecture notes. Do they match? \n","Note that LAMBDA (i.e., $\\lambda$) is the regularisation weight which controls the strength (importance) of the regularisation term in the overall loss function.\n"]},{"cell_type":"markdown","metadata":{"id":"PBXyk1bjrgsS"},"source":["**bold text**> **A4:**  Test"]},{"cell_type":"markdown","metadata":{"id":"hvoGPdw0BuZB"},"source":["> **Q5:** Now if you are up for a rather difficult question: can you explain the reason behind the following line in the code: ```I[0,0] = 0```"]},{"cell_type":"markdown","metadata":{"id":"9om0OKcKCRFP"},"source":["> **A5:** \n","---"]},{"cell_type":"code","metadata":{"id":"L9u59vHAH2DX"},"source":["message = \"Machine Learning Lab 1: Polynomial Regression with Regularisation\"\n","print(message)\n","print('-'*len(message)+'\\n')\n","\n","from scipy.linalg import pinv\n","# pinv: takes the (pseudo)-inverse (generalised inverse) of a matrix.\n","# the same function is also available from numpy: numpy.linalg.pinv\n","\n","\n","grid = widgets.Grid(1, 3)\n","\n","# first, to generate our train and test data sets:\n","xTrain, yTrain, xTest, yTest, xTrue, yTrue = utilCreatePolyData(nTrain=100,\n","                                                                rndseed=0)\n","# Note: change the \"rndseed\" to some other integer, say your Student ID, to get \n","# fresh data, different from others!\n","#print(\"machi next line is xtrue\")\n","#print(xTrue) \n","\n","'''Perform regression with different order polynomials'''\n","\n","LAMBDA = 0.2 # This is the regularisation strength.\n","\n","for i, order in enumerate([1,3,9]):\n","    phiTrain = util_makeGramMatPoly(xTrain, order)\n","    phiTest  = util_makeGramMatPoly(xTest, order)  \n","    phiAll   = util_makeGramMatPoly(xTrue, order)\n","    I        = np.eye(order+1)\n","    print(I)\n","    I[0,0] = 0\n","    \n","    w_ml     = np.matmul(pinv(LAMBDA*I + np.matmul(phiTrain.T, phiTrain)), np.matmul(phiTrain.T, yTrain))\n","    \n","    \n","    with grid.output_to(0, i):\n","      plt.plot(xTrain, yTrain,'.', alpha=0.8, label='Train data')\n","      plt.plot(xTest, yTest,'*', alpha=0.8, label='Test data')\n","      plt.plot(xTrue, np.matmul(phiAll, w_ml), '-', \n","               alpha=0.8, label='Learned model', linewidth=2)\n","      plt.plot(xTrue, yTrue,'--', label='True model', linewidth=2)\n","\n","      \n","      plt.title('Order = {}, $\\lambda$={}'.format(order, LAMBDA))\n","      plt.xlabel(\"x\", fontsize=14)\n","      plt.ylabel(\"y\", fontsize=14)\n","      plt.xlim(-1.1,1.1)\n","      plt.ylim(-0.1,2.1)\n","      plt.grid(alpha=0.2)\n","      plt.legend(loc='upper left', fontsize=12)\n","      plt.show()\n","      ERROR_TRAIN = np.matmul(phiTrain, w_ml) - yTrain\n","      ERROR_TEST = 0\n","      print('Train MSE = {:8.4f}'.format(np.dot(ERROR_TRAIN.T,ERROR_TRAIN)/len(ERROR_TRAIN)))\n","      print('Test MSE = {:8.4f}'.format(0))\n","      print('\\nFitted Model:\\n{}\\n---'.format(np.poly1d(np.flip(w_ml), variable='x')))\n","      print(\"w'*w = {:8.4f}\".format(np.dot(w_ml.T, w_ml)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iba-JUSupw2O"},"source":["---\n","> **Q6:** There is an intentional mistake in the code: the MSE for the test data is not computed correctly. Fix it. Same hints as before applies here (look how the MSE is computed for the train data, and accordingly write the code for the test data. Once done, peek ahead in the next code cell for double-checking where the code is provided for you.)"]},{"cell_type":"markdown","metadata":{"id":"ma7eAbDCsIhJ"},"source":["> **Q7:** Try different values for $\\lambda$ and observe the fits as well as the reported train and test errors, e.g., for the choice of LAMBDA in $(0, 0.001, 0.01, 0.1, 1, 10)$. You will need to re-run the script for each new value of LAMBDA. In particular, take note of the effect of $\\lambda$ on cases where we have under-fitting and cases where we have  over-fitting in the absence of regularisation (i.e., when $\\lambda=0$)."]},{"cell_type":"markdown","metadata":{"id":"s7xUX7FQ161_"},"source":["> **A7:** \n","---"]},{"cell_type":"markdown","metadata":{"id":"aedshp_z2ZMK"},"source":["ll3. For convenience, the following script puts the different models in a grid to allow easier comparison (we are doing a grid-search!). Here, we have two **hyper-parameters** (what are they?) \n","\n","  For each case (for each combination of the hyper-parameters), the best model is fit and its performance on train and test dataset are reported. Another value that is reported is the sum of the squares of the weights are also reported, i.e., $\\sum_{i=1}^{p}w^2_i$, which is equal to $\\vec{w}^{T}.\\vec{w}$."]},{"cell_type":"code","metadata":{"id":"MFkHWZdphzzG"},"source":["def GridPlotDifferentOrdersDiffernetRegularisation(LAMBDAS=[0, 0.001, 0.01, 0.1, 1, 10],\n","                                                   ORDERS= [1,2,3,9],\n","                                                   N_TrainPoints=10):\n","\n","  grid = widgets.Grid(len(LAMBDAS)+1, len(ORDERS)+1)\n","\n","  for j, order in enumerate(ORDERS):\n","    with grid.output_to(0, j+1):\n","      print('order={:d}'.format(order))\n","\n","\n","  xTrain, yTrain, xTest, yTest, xTrue, yTrue = utilCreatePolyData(nTrain=N_TrainPoints)\n","  # Parameter here specifies how many points to generate.\n","\n","  for i, LAMBDA in enumerate(LAMBDAS):\n","    with grid.output_to(i+1, 0):\n","      print('lambda={:5.4f}'.format(LAMBDA))\n","\n","    for j, order in enumerate(ORDERS):\n","      phiTrain = util_makeGramMatPoly(xTrain, order)\n","      phiTest  = util_makeGramMatPoly(xTest, order)  \n","      I        = np.eye(order+1)\n","      I[0, 0] = 0\n","      w_ml     = np.matmul(pinv(LAMBDA*I + np.matmul(phiTrain.T, phiTrain)),np.matmul(phiTrain.T, yTrain))\n","\n","      with grid.output_to(i+1, j+1):\n","        plt.plot(xTrain, yTrain,'.', alpha=0.8, label='Train data')\n","        plt.plot(xTest, yTest,'*', alpha=0.8, label='Test data')\n","        plt.plot(xTrue, np.polynomial.polynomial.polyval(xTrue, w_ml),'-',\n","                   linewidth=2, alpha=0.8, label='Learned model')\n","        plt.plot(xTrue, yTrue, '--', label='True model', linewidth=2)\n","        \n","\n","        plt.xlabel(\"x\", fontsize=14)\n","        plt.ylabel(\"y\", fontsize=14)\n","        plt.xlim(-1.1,1.1)\n","        plt.ylim(0,2)\n","        plt.grid(alpha=0.2)\n","        plt.legend(loc='upper left', fontsize=12)\n","        plt.show()\n","\n","        ERROR_TRAIN = np.matmul(phiTrain, w_ml) - yTrain\n","        ERROR_TEST = np.matmul(phiTest, w_ml) - yTest\n","        print('MSE Train = {:8.4f}'.format(np.dot(ERROR_TRAIN.T,ERROR_TRAIN)/len(ERROR_TRAIN)))\n","        print('MSE Test = {:8.4f}'.format(np.dot(ERROR_TEST.T,ERROR_TEST)/len(ERROR_TEST)))\n","        print(\"w'*w = {:8.4f}\".format(np.dot(w_ml.T, w_ml)))\n","        \n","# run it with default values:        \n","GridPlotDifferentOrdersDiffernetRegularisation()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cdRkWGLX6CWv"},"source":["---\n","---\n","####<font color=\"maroon\"><b>Exercise 5: Focus on order=9. Describe AND explain the trend of each of the metrics below with respect to increasing values of $\\lambda$ (that is, first describe what the effect of increasing $\\lambda$ from zero upward is it on the parameter in question and then explain briefly and clearly the reasons behind it):   [1 mark]\n","  (a) TRAIN MSE <br>\n","  (b) $\\vec{w}^T.\\vec{w}$<br> \n","  (c) TEST MSE\n","</b>\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"NB77xyaQbblp"},"source":["The Train MSE is zero when  λ is zero. The value increases and gets adjusted when the  λ increases. The train MSE is zero at order number 9 because the model memorise the train labels and works well for train data set, the same model results larger error for test data sets. \n","\n","The Test MSE is much higher when  λ is zero because the train model is memorised the train data for order 9. The Test MSE drops closer to 0.08 after applying  λ function.  The value gets adjusted as  λ increases. \n","\n","The weights are being adjusted with every increase log function at log 0.01 its at optimum level.\n","\n","\n","\n","\n","\n","\n","\n","\n","The λ(log)is applied in trained data. As the log value increases the values in the attributes decreases exponentialy with power of λ. Ex: log 2 on the value 9 is 3.\n","\n","At λ 0.01 the train and test has minimum variation in MSE. \n","\n","\n","The weights are being adjusted with every increase log function at log 0.01 its at optimum level.\n","\n","Test MSE is at high when log was zero , it  become lowest at 0.01 log\n","\n"]},{"cell_type":"markdown","metadata":{"id":"io9Q-yBB9bwA"},"source":["####<font color=\"maroon\"><b>Exercise 6: Now suppose that instead of 10 training instances, we had access to 100 train instances. Run the following script and inspect the change in the test error. Describe and explain the effect of having more training data on the test error (test MSE) and over-fitting. </b> <b>[1 mark]</b> </font>"]},{"cell_type":"markdown","metadata":{"id":"3u-mB8-rlPYI"},"source":["The increases in sample size(instances) the MSE value has improved for both train and test data without applying log function.\n","\n","Based on the observation order number 9 has lowest train and Test MSE. \n","\n","There is no overfitting observed with increasing in the sample size."]},{"cell_type":"code","metadata":{"id":"JSwD4RS9HhWN"},"source":["GridPlotDifferentOrdersDiffernetRegularisation(N_TrainPoints=100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77P4iHK1RnIm"},"source":["---\n","---"]},{"cell_type":"markdown","metadata":{"id":"Hg7aJRlwJeiG"},"source":["## 2. Validation (and Cross-Validation)\n"]},{"cell_type":"markdown","metadata":{"id":"gyFC4eQBRrNV"},"source":["The point of this exercise is to clarify the roles of train, validation, and testing data in machine learning.\n","\n","\n","So far, we have been training a model (finding the best fit, the best parameters, for a given set of hyper-parameters) on `train` data, and then evaluating it on `test` data, since they were unseen by the model. However, if we then pick the best model (the best hyper-parameter combination) among our fitted models based on the performances on the test data, we are at the risk of over-fitting the hyper-parameters to the test data! That is, we may think that this is the best hyper-parameter combination, but in fact, it just happens to be doing the best on the particular test data we have. Although this is much less serious than over-fitting to the train data, importantly, we do not have an untainted measure of the performance of the ``best'' model when faced with truly new and unseen data, i.e., how well does your model *generalise* (because we have tainted our test data by peeking into it for tuning our hyper-parameter). \n","\n","Recognising this problem, the best practice is to partition our dataset into not just two subsets (train and test), but to 3 subsets: (**train**, **validation**, and **test**). \n","\n","The process of finding the best model and getting an honest (untainted) measure of its performance is as follows:\n","\n","0.   Split the dataset into 3 subsets: `train`, `validation`, and `test`;\n","1.   For a given set of hyper-parameters, find the best fit using the `train` dataset; then apply the model to the `validation` dataset to get a measure of its performance;\n","2.   Vary the hyper-parameters (i.e., do a \"grid-search\") and pick the hyper-parameter combination that performs the best on the `validation` dataset.\n","3.   Then in order to get a performance measure of this selected model, apply it to the `test` dataset.\n","\n","There can be some variation of this **model selection** process: \n","\n","- (v1) In step 0 above, the partition of the dataset between `train` and `validation` was fixed when searching for the best hyper-parameter; instead, it can be randomised each time after step 1, i.e., for each new candidate for hyper-parameter values.\n","\n","- (v2) Once the hyper-parameters are selected using validation (after step 2 above), then the train and validation datasets can be combined, and a model is fit to this larger dataset using this hyper-parameter values (and then applied to the test dataset, as before, to get a measure of its performance when facing new data)."]},{"cell_type":"markdown","metadata":{"id":"qCdey7PJcXnw"},"source":["---\n","> **Q8:** As a thought-provoking (challenging) question, for each of the variations above, which we just labelled (v1) and (v2), try to argue its pros and cons, i.e., discuss its potential advantages and disadvantages of using that variation. "]},{"cell_type":"markdown","metadata":{"id":"i-PDwcHsgHu7"},"source":["> **A8:** \n","---"]},{"cell_type":"markdown","metadata":{"id":"8oOBFZvSR_vz"},"source":["0. This script contains a simple validation procedure to select the parameter lambda of a regularisation problem. The validation procedure splits the input data into two subsets: trainset and valset. It then searches within a range of values (hyper-parameters) and decides the best one to use based on the resulting validation MSE. Note that\n","`np.logspace(start=-3, stop=1, num=5)` creates the array of $[10^{-3}, 10^{-2} , 10^{-1} , 10^0 , 10^1 ]$."]},{"cell_type":"code","metadata":{"id":"flTfVNSTRtc3"},"source":["# First, starting with our artificial example: \n","xTrain, yTrain, xTest, yTest, xTrue, yTrue = utilCreatePolyData(nTrain=100, \n","                                                                 PolynomialCoefficients=[1,-1,0,0,0,1],\n","                                                                )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vTDVyi1tJ2xE"},"source":["LAMBDAS = np.logspace(start=-3, stop=1, num=5)\n","LAMBDAS = np.insert(LAMBDAS, 0, 0)\n","order = 9\n","\n","bestLAMBDA = -1\n","minerr = np.Infinity\n","nTrain = len(yTrain)\n","RNDM = np.random.RandomState(seed=0)\n","\n","for LAMBDA in LAMBDAS:\n","  # Split the train samples into training and validation dataset\n","  ids = RNDM.permutation(np.arange(nTrain))\n","  num_train = int(np.ceil(nTrain*0.75))\n","  xtrainset = xTrain[ids[:num_train]]\n","  ytrainset = yTrain[ids[:num_train]]\n","  xvalset   = xTrain[ids[num_train:]]\n","  yvalset   = yTrain[ids[num_train:]]\n","  phitrain  = util_makeGramMatPoly(xtrainset, order)\n","  phival    = util_makeGramMatPoly(xvalset, order)\n","  phitest   = util_makeGramMatPoly(xTest, order)\n","  I         = np.eye(order+1)\n","  I[0, 0]   = 0\n","\n","  # This line learns the regression model for a given lambda.\n","  w_map     = np.matmul(pinv(LAMBDA*I + np.matmul(phitrain.T,phitrain)),\n","                        np.matmul(phitrain.T,ytrainset)) \n","    \n","  # The next line will help decide which is the best lambda.   \n","  #--- FIX ME: COMMENT OUT TWO OF THE LINES BELOW, SO EXACTLY ONE IS ENABLED ---  \n","  #SSE = np.sum(np.power(ytrainset - np.matmul(phitrain, w_map), 2))\n","  #SSE = np.sum(np.power(yvalset - np.matmul(phival, w_map), 2)) \n","  SSE = np.sum(np.power(yTest - np.matmul(phitest, w_map), 2))\n","    \n","  print('Lambda: {0:1.5f}, CV SSE: {1:1.5f}.'.format(LAMBDA, SSE), end='')\n","  if SSE < minerr:\n","    print(' * New best')\n","    bestLAMBDA = LAMBDA\n","    minerr = SSE\n","  else:\n","    print()\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pjCkjKutTxH-"},"source":["1. The incomplete part of this script is missing which portion of the data we\n","should use as the criteria for selecting lambda. It could be Train, Val or Test.\n","Fix this section by commenting out two lines, leaving the correct line in, and\n","run the code."]},{"cell_type":"markdown","metadata":{"id":"qS4OmjlGULCM"},"source":["---\n","---\n","####<font color=\"maroon\"><b>Exercise 7: Which is the correct way to complete the script and calculate the variable SSE? Explain what each of the three options to calculate SSE would do and justify your choice. What is the resulting train and test errors? </b> <b>[0.5 mark]</b> </font>\n","---\n","---"]},{"cell_type":"markdown","metadata":{"id":"pTgCzTWel2v6"},"source":["The uncomment the SSE to get the SSE values at different lambda values.\n","\n","Train data SSE\n","\n","Lambda: 0.00000, CV SSE: 0.83345. * New best\n","Lambda: 0.00100, CV SSE: 0.69638. * New best\n","Lambda: 0.01000, CV SSE: 0.71330.\n","Lambda: 0.10000, CV SSE: 0.83372.\n","Lambda: 1.00000, CV SSE: 0.88272.\n","Lambda: 10.00000, CV SSE: 3.02197\n","\n","Validation SSE\n","\n","Lambda: 0.00000, CV SSE: 0.14715. * New best\n","Lambda: 0.00100, CV SSE: 0.28123.\n","Lambda: 0.01000, CV SSE: 0.26907.\n","Lambda: 0.10000, CV SSE: 0.13880. * New best\n","Lambda: 1.00000, CV SSE: 0.33508.\n","Lambda: 10.00000, CV SSE: 0.95373.\n","\n","\n","\n","\n","Test SSE\n","\n","Lambda: 0.00000, CV SSE: 0.18934. * New best\n","Lambda: 0.00100, CV SSE: 0.17033. * New best\n","Lambda: 0.01000, CV SSE: 0.17288.\n","Lambda: 0.10000, CV SSE: 0.19468.\n","Lambda: 1.00000, CV SSE: 0.32222.\n","Lambda: 10.00000, CV SSE: 0.96854.\n","\n","\n","The lambda 0.10 model is the loswest SSE in validation"]},{"cell_type":"markdown","metadata":{"id":"Nc1_jy51huje"},"source":["### Show the results"]},{"cell_type":"code","metadata":{"id":"J3TQwLQRRrpo"},"source":["phi        = util_makeGramMatPoly(xTrain, order)\n","best_w_map = np.matmul(pinv(bestLAMBDA*I+np.matmul(phi.T, phi)), \n","                       np.matmul(phi.T, yTrain)) \n","\n","phitest    = util_makeGramMatPoly(xTest, order) \n","train_MSE = np.sum(np.power(yTrain - np.matmul(phi, best_w_map), 2)) \n","test_MSE  = np.sum(np.power(yTest - np.matmul(phitest, best_w_map), 2)) \n","\n","\n","\n","plt.plot(xTrain, yTrain,'.', alpha=0.4, label='Train data')\n","plt.plot(xTest, yTest,'*', alpha=0.6, label='Test data')\n","plt.plot(xTrue, yTrue, '--', label='True model')\n","plt.plot(xTrue, np.polynomial.polynomial.polyval(xTrue, best_w_map),'-',\n","           alpha=0.8, label='Learned model')\n","\n","plt.xlabel(\"x\", fontsize=14)\n","plt.ylabel(\"y\", fontsize=14)\n","plt.grid(alpha=0.2)\n","plt.legend(fontsize=12)\n","plt.show()\n","\n"],"execution_count":null,"outputs":[]}]}