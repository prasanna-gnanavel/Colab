{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Introduction to keras.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"iuX2BBPtfafg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"outputId":"f8a86816-c2b1-4b7a-d254-f0e3d7ec9efe","executionInfo":{"status":"ok","timestamp":1586798940488,"user_tz":-60,"elapsed":1577016,"user":{"displayName":"Prasanna Gnanavel","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYxK2caUPvzD-2OIGQBTruCTOrsIAsstjM88N9=s64","userId":"18249743406817165445"}}},"source":["#This is a keras official example, xplain line-by-line\n","'''Trains a simple convnet on the MNIST dataset.\n","Gets to 99.25% test accuracy after 12 epochs\n","(there is still a lot of margin for parameter tuning).\n","16 seconds per epoch on a GRID K520 GPU.\n","'''\n","\n","from __future__ import print_function\n","import keras\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten #[batchsize, w, h, channel]=>[batchsize, channel*w*h]\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","\n","batch_size = 128\n","num_classes = 10\n","epochs = 12\n","\n","# input image dimensions\n","img_rows, img_cols = 28, 28\n","\n","# the data, split between train and test sets\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","if K.image_data_format() == 'channels_first':\n","    #Pytorch\n","    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    #Tensorflow\n","    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)\n","\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","x_train /= 255\n","x_test /= 255\n","print('x_train shape:', x_train.shape)\n","print(x_train.shape[0], 'train samples')\n","print(x_test.shape[0], 'test samples')\n","\n","# convert class vectors to binary class matrices\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)\n","\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3),\n","                 activation='relu',\n","                 input_shape=input_shape))\n","model.add(Conv2D(64, (3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.25))\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","              optimizer=keras.optimizers.Adadelta(),\n","              metrics=['accuracy'])\n","\n","model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          verbose=1,\n","          validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss:', score[0])\n","print('Test accuracy:', score[1])"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n","11493376/11490434 [==============================] - 2s 0us/step\n","x_train shape: (60000, 28, 28, 1)\n","60000 train samples\n","10000 test samples\n","Train on 60000 samples, validate on 10000 samples\n","Epoch 1/12\n","60000/60000 [==============================] - 135s 2ms/step - loss: 0.2608 - accuracy: 0.9208 - val_loss: 0.0642 - val_accuracy: 0.9801\n","Epoch 2/12\n","60000/60000 [==============================] - 131s 2ms/step - loss: 0.0902 - accuracy: 0.9729 - val_loss: 0.0416 - val_accuracy: 0.9855\n","Epoch 3/12\n","60000/60000 [==============================] - 131s 2ms/step - loss: 0.0666 - accuracy: 0.9801 - val_loss: 0.0383 - val_accuracy: 0.9867\n","Epoch 4/12\n","60000/60000 [==============================] - 130s 2ms/step - loss: 0.0536 - accuracy: 0.9840 - val_loss: 0.0321 - val_accuracy: 0.9888\n","Epoch 5/12\n","60000/60000 [==============================] - 130s 2ms/step - loss: 0.0472 - accuracy: 0.9856 - val_loss: 0.0285 - val_accuracy: 0.9898\n","Epoch 6/12\n","60000/60000 [==============================] - 134s 2ms/step - loss: 0.0410 - accuracy: 0.9873 - val_loss: 0.0298 - val_accuracy: 0.9895\n","Epoch 7/12\n","60000/60000 [==============================] - 130s 2ms/step - loss: 0.0375 - accuracy: 0.9886 - val_loss: 0.0301 - val_accuracy: 0.9901\n","Epoch 8/12\n","60000/60000 [==============================] - 129s 2ms/step - loss: 0.0343 - accuracy: 0.9893 - val_loss: 0.0281 - val_accuracy: 0.9908\n","Epoch 9/12\n","60000/60000 [==============================] - 128s 2ms/step - loss: 0.0334 - accuracy: 0.9898 - val_loss: 0.0254 - val_accuracy: 0.9914\n","Epoch 10/12\n","60000/60000 [==============================] - 127s 2ms/step - loss: 0.0309 - accuracy: 0.9911 - val_loss: 0.0308 - val_accuracy: 0.9911\n","Epoch 11/12\n","60000/60000 [==============================] - 128s 2ms/step - loss: 0.0275 - accuracy: 0.9916 - val_loss: 0.0283 - val_accuracy: 0.9907\n","Epoch 12/12\n","60000/60000 [==============================] - 131s 2ms/step - loss: 0.0268 - accuracy: 0.9920 - val_loss: 0.0269 - val_accuracy: 0.9925\n","Test loss: 0.026937208870885115\n","Test accuracy: 0.9925000071525574\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M-rxZhv7fafr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"to45Pahqfafz","colab_type":"code","colab":{},"outputId":"834d433d-1a5b-40bc-bd87-ed380e6b1ebe"},"source":["#Autoencoder high dimension => low dimension  => high dimension instead of PCA for reduction"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'channels_last'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"6lhRb22bfaf7","colab_type":"code","colab":{},"outputId":"0895d6c3-a888-45a5-ff17-ba38bf69bcfa"},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","import keras\n","from keras.layers import Activation, Dense, Input\n","from keras.layers import Conv2D, Flatten\n","from keras.layers import Reshape, Conv2DTranspose\n","from keras.models import Model\n","from keras import backend as K\n","from keras.datasets import mnist\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","np.random.seed(1337)\n","\n","# MNIST dataset\n","(x_train, _), (x_test, _) = mnist.load_data()\n","\n","image_size = x_train.shape[1]\n","x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n","x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255\n","\n","# Generate corrupted MNIST images by adding noise with normal dist\n","# centered at 0.5 and std=0.5\n","noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n","x_train_noisy = x_train + noise\n","noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n","x_test_noisy = x_test + noise\n","\n","x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n","x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n","\n","# Network parameters\n","input_shape = (image_size, image_size, 1)\n","batch_size = 128\n","kernel_size = 3\n","latent_dim = 16\n","# Encoder/Decoder number of CNN layers and filters per layer\n","layer_filters = [32, 64]\n","\n","# Build the Autoencoder Model\n","# First build the Encoder Model\n","inputs = Input(shape=input_shape, name='encoder_input')\n","x = inputs\n","# Stack of Conv2D blocks\n","# Notes:\n","# 1) Use Batch Normalization before ReLU on deep networks\n","# 2) Use MaxPooling2D as alternative to strides>1\n","# - faster but not as good as strides>1\n","for filters in layer_filters:\n","    x = Conv2D(filters=filters,\n","               kernel_size=kernel_size,\n","               strides=2,\n","               activation='relu',\n","               padding='same')(x)\n","\n","# Shape info needed to build Decoder Model\n","shape = K.int_shape(x)\n","\n","# Generate the latent vector\n","x = Flatten()(x)\n","latent = Dense(latent_dim, name='latent_vector')(x)\n","\n","# Instantiate Encoder Model\n","encoder = Model(inputs, latent, name='encoder')\n","encoder.summary()\n","\n","# Build the Decoder Model\n","latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n","x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n","x = Reshape((shape[1], shape[2], shape[3]))(x)\n","\n","# Stack of Transposed Conv2D blocks\n","# Notes:\n","# 1) Use Batch Normalization before ReLU on deep networks\n","# 2) Use UpSampling2D as alternative to strides>1\n","# - faster but not as good as strides>1\n","for filters in layer_filters[::-1]:\n","    x = Conv2DTranspose(filters=filters,\n","                        kernel_size=kernel_size,\n","                        strides=2,\n","                        activation='relu',\n","                        padding='same')(x)\n","\n","x = Conv2DTranspose(filters=1,\n","                    kernel_size=kernel_size,\n","                    padding='same')(x)\n","\n","outputs = Activation('sigmoid', name='decoder_output')(x)\n","\n","# Instantiate Decoder Model\n","decoder = Model(latent_inputs, outputs, name='decoder')\n","decoder.summary()\n","\n","# Autoencoder = Encoder + Decoder\n","# Instantiate Autoencoder Model\n","autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n","autoencoder.summary()\n","\n","autoencoder.compile(loss='mse', optimizer='adam')\n","\n","# Train the autoencoder\n","autoencoder.fit(x_train_noisy,\n","                x_train,\n","                validation_data=(x_test_noisy, x_test),\n","                epochs=30,\n","                batch_size=batch_size)\n","\n","# Predict the Autoencoder output from corrupted test images\n","x_decoded = autoencoder.predict(x_test_noisy)\n","\n","# Display the 1st 8 corrupted and denoised images\n","rows, cols = 10, 30\n","num = rows * cols\n","imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\n","imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n","imgs = np.vstack(np.split(imgs, rows, axis=1))\n","imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n","imgs = np.vstack([np.hstack(i) for i in imgs])\n","imgs = (imgs * 255).astype(np.uint8)\n","plt.figure()\n","plt.axis('off')\n","plt.title('Original images: top rows, '\n","          'Corrupted Input: middle rows, '\n","          'Denoised Input:  third rows')\n","plt.imshow(imgs, interpolation='none', cmap='gray')\n","Image.fromarray(imgs).save('corrupted_and_denoised.png')\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From C:\\Users\\Lincoln\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Model: \"encoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","encoder_input (InputLayer)   (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 7, 7, 64)          18496     \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 3136)              0         \n","_________________________________________________________________\n","latent_vector (Dense)        (None, 16)                50192     \n","=================================================================\n","Total params: 69,008\n","Trainable params: 69,008\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"decoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","decoder_input (InputLayer)   (None, 16)                0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 3136)              53312     \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 7, 7, 64)          0         \n","_________________________________________________________________\n","conv2d_transpose_1 (Conv2DTr (None, 14, 14, 64)        36928     \n","_________________________________________________________________\n","conv2d_transpose_2 (Conv2DTr (None, 28, 28, 32)        18464     \n","_________________________________________________________________\n","conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         289       \n","_________________________________________________________________\n","decoder_output (Activation)  (None, 28, 28, 1)         0         \n","=================================================================\n","Total params: 108,993\n","Trainable params: 108,993\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"autoencoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","encoder_input (InputLayer)   (None, 28, 28, 1)         0         \n","_________________________________________________________________\n","encoder (Model)              (None, 16)                69008     \n","_________________________________________________________________\n","decoder (Model)              (None, 28, 28, 1)         108993    \n","=================================================================\n","Total params: 178,001\n","Trainable params: 178,001\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:From C:\\Users\\Lincoln\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","Train on 60000 samples, validate on 10000 samples\n","Epoch 1/30\n","60000/60000 [==============================] - 110s 2ms/step - loss: 0.0623 - val_loss: 0.0329\n","Epoch 2/30\n","60000/60000 [==============================] - 101s 2ms/step - loss: 0.0263 - val_loss: 0.0223\n","Epoch 3/30\n","60000/60000 [==============================] - 114s 2ms/step - loss: 0.0212 - val_loss: 0.0198\n","Epoch 4/30\n","60000/60000 [==============================] - 129s 2ms/step - loss: 0.0193 - val_loss: 0.0187\n","Epoch 5/30\n","60000/60000 [==============================] - 121s 2ms/step - loss: 0.0184 - val_loss: 0.0179\n","Epoch 6/30\n","60000/60000 [==============================] - 134s 2ms/step - loss: 0.0177 - val_loss: 0.0175\n","Epoch 7/30\n","60000/60000 [==============================] - 148s 2ms/step - loss: 0.0173 - val_loss: 0.0171\n","Epoch 8/30\n","60000/60000 [==============================] - 144s 2ms/step - loss: 0.0169 - val_loss: 0.0170\n","Epoch 9/30\n","60000/60000 [==============================] - 147s 2ms/step - loss: 0.0166 - val_loss: 0.0166\n","Epoch 10/30\n","60000/60000 [==============================] - 147s 2ms/step - loss: 0.0163 - val_loss: 0.0166\n","Epoch 11/30\n","60000/60000 [==============================] - 137s 2ms/step - loss: 0.0161 - val_loss: 0.0164\n","Epoch 12/30\n","60000/60000 [==============================] - 126s 2ms/step - loss: 0.0159 - val_loss: 0.0163\n","Epoch 13/30\n","60000/60000 [==============================] - 123s 2ms/step - loss: 0.0157 - val_loss: 0.0162\n","Epoch 14/30\n","60000/60000 [==============================] - 132s 2ms/step - loss: 0.0155 - val_loss: 0.0161\n","Epoch 15/30\n","60000/60000 [==============================] - 123s 2ms/step - loss: 0.0154 - val_loss: 0.0160\n","Epoch 16/30\n","60000/60000 [==============================] - 126s 2ms/step - loss: 0.0152 - val_loss: 0.0160\n","Epoch 17/30\n","60000/60000 [==============================] - 150s 3ms/step - loss: 0.0151 - val_loss: 0.0159\n","Epoch 18/30\n","60000/60000 [==============================] - 134s 2ms/step - loss: 0.0150 - val_loss: 0.0159\n","Epoch 19/30\n","60000/60000 [==============================] - 138s 2ms/step - loss: 0.0149 - val_loss: 0.0159\n","Epoch 20/30\n","60000/60000 [==============================] - 120s 2ms/step - loss: 0.0148 - val_loss: 0.0158\n","Epoch 21/30\n","60000/60000 [==============================] - 118s 2ms/step - loss: 0.0147 - val_loss: 0.0156\n","Epoch 22/30\n","60000/60000 [==============================] - 124s 2ms/step - loss: 0.0147 - val_loss: 0.0158\n","Epoch 23/30\n","60000/60000 [==============================] - 135s 2ms/step - loss: 0.0146 - val_loss: 0.0157\n","Epoch 24/30\n","60000/60000 [==============================] - 149s 2ms/step - loss: 0.0145 - val_loss: 0.0156\n","Epoch 25/30\n","60000/60000 [==============================] - 139s 2ms/step - loss: 0.0145 - val_loss: 0.0157\n","Epoch 26/30\n","60000/60000 [==============================] - 106s 2ms/step - loss: 0.0144 - val_loss: 0.0156\n","Epoch 27/30\n","60000/60000 [==============================] - 104s 2ms/step - loss: 0.0144 - val_loss: 0.0157\n","Epoch 28/30\n","60000/60000 [==============================] - 112s 2ms/step - loss: 0.0143 - val_loss: 0.0156\n","Epoch 29/30\n","60000/60000 [==============================] - 111s 2ms/step - loss: 0.0143 - val_loss: 0.0155\n","Epoch 30/30\n","60000/60000 [==============================] - 121s 2ms/step - loss: 0.0142 - val_loss: 0.0156\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ewpWI0OZfagD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}