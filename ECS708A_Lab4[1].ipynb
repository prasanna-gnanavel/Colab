{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"private_outputs":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xKNf2WkWpcWs"},"source":["# <font color='gray'> Lab 4: Unsupervised Learning</font>"]},{"cell_type":"markdown","metadata":{"id":"nC6ZwWkQn3J4"},"source":["## Introduction \n","\n","\n","The aim of this lab is to:\n","(i)  **get experience with unsupervised clustering using K-means**; and (ii) **explore the notions of accuracy and confusion matrix further**, and (iii) **familiarise ourselves with high-dimentional data and dimensionality reduction techniques**. \n","\n","- This lab constitutes your fourth course-work activity.\n","- A report answering the <font color = 'red'>**questions in</font><font color = \"maroon\"> red**</font> only should be submitted by the 19th of April. \n","- The report should be a separate file in **pdf format** (so **NOT** *doc, docx, notebook* etc.), well identified with your name, student number, assignment number, module code. \n","- No other means of submission other than the appropriate QM+ link is acceptable at any time (so NO email attachments, etc.)\n","- **PLAGIARISM** <ins>is an irreversible non-negotiable failure in the course</ins> (if in doubt of what constitutes plagiarism, ask!). \n"]},{"cell_type":"markdown","metadata":{"id":"_4k8ZphQqATM"},"source":["## **1. K-Means Clustering**\n","\n","In this exercise, we will explore the *k*-means clustering algorithm, the evaluation of clustering quality, and the relation between clustering and classification."]},{"cell_type":"markdown","metadata":{"id":"TGfPeI0xceao"},"source":["#### 0. Loading the dataset\n","\n","*   This first cell loads the `Iris` flower dataset that you have already worked with in *Lab 3*. The Iris flower dataset is a classic dataset used to identify types of flowers based on features describing their petals. \n","\n","* The following cell will show you a plot with 2 of the 4 dimensions of this data (flower petal geometry) coloured by the type of flower. \n","\n","* In this lab, instead of learning to classifying it, we will cluster it.\n","\n"]},{"cell_type":"code","metadata":{"id":"ZKHVdJ_CcsQ4"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","\n","# import some data to play with\n","iris = datasets.load_iris()\n","print(iris)\n","X = iris.data[:, :2]  # we only take the first two features.\n","Y = iris.target\n","\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","colors = (\"red\", \"green\", \"blue\")\n","marker_list = ['+', 'o', 'x']\n","\n","\n","for l in [0, 1, 2]:\n","  ax.scatter(X[Y == l, 0], X[Y == l, 1],\n","             marker=marker_list[l], s=7,\n","             c=colors[l], edgecolors='none',\n","             label='{:d} ({:s})'.format(l, iris.target_names[l]))\n","\n","ax.legend(fontsize=12)\n","ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","ax.grid(alpha=0.3)\n","ax.set_xlim(X[:, 0].min() - 0.5, X[:, 0].max() + 0.5)\n","ax.set_ylim(X[:, 1].min() - 0.5, X[:, 1].max() + 0.5)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u__3tp-nlFZT"},"source":["#### 1. Setup *k*-means when *k = 2*\n","\n","* This cell initializes *k*-means.\n","\n","*Recall that in K-means, data points are associated to the cluster centre that they are closest to.*\n","\n","We first start choosing two random cluster centres:"]},{"cell_type":"code","metadata":{"id":"k6EEOWhGiXOI"},"source":["# change this if you want to start from a different randomisation seed\n","np.random.seed(seed=9)\n","k = 2# set the k value of k-means\n","centers2 = np.random.normal(size=[k, 2]) + np.ones((k,1)) * np.mean(X, axis=0)\n","\n","#print(np.random.normal(size=[k, 2]))\n","#print (\"next\")\n","print(np.mean(X, axis=0))\n","print(\"next\")\n","#print ( np.ones((2,1)))\n","#print (np.mean(X, axis=0))\n","\n","print(centers2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"19MkuNvrQpcJ"},"source":["* Now, let's plot these two centres within our dataset. You should see a figure showing the flower data-points, and two randomly initialised cluster centres. Note that as this is unsupervised learning, we don't use any labels and hence, the colours of datapoints shown in the previous plot are removed."]},{"cell_type":"code","metadata":{"id":"TycLa1dum_9-"},"source":["fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","\n","# Plot the datapoints\n","ax.scatter(X[:, 0], X[:, 1], c = 'w', edgecolor = 'k')\n","\n","# Plot the centres that we obtained (randomly) for k-means in the previous cell\n","ax.scatter(centers2[0, 0], centers2[0, 1], color='red', marker = 'X', s= 100)\n","ax.scatter(centers2[1, 0], centers2[1, 1], color='blue', marker = 'X', s = 100)\n","\n","ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","ax.grid(alpha=0.3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0LF9DGhtNVBX"},"source":["---\n","> **Q0:** How do you think the data points shown will be grouped by using the distance to the cluster centres as the similarity criterion?"]},{"cell_type":"markdown","metadata":{"id":"JVeaQTlONWEO"},"source":["> **A0:**\n","---\n","\n","Prss: It creates 2 centroids and try to minimum distance from each centroids. All the data points will be clustered towards red centroid which is closest to all the data points. "]},{"cell_type":"markdown","metadata":{"id":"hSuP9HAaW4h4"},"source":["#### 2. Assign points to the two clusters\n","\n","* This does the first \"E-step\" of *k*-means. You should see points now assigned to their nearest cluster centre. We also compute the summed distance of every point to its nearest cluster centre."]},{"cell_type":"code","metadata":{"id":"Crr4dkisV9oj"},"source":["from scipy.spatial import distance\n","\n","# Find the euclidean distance between every point and every cluster.\n","distanceMatrix2 = distance.cdist(X, centers2, 'euclidean')\n","\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","overalDistToClusters2 = 0.0\n","totaldist=0\n","# Make the labe of each point be the closest cluster.\n","for index in range(len(X)):\n","  if distanceMatrix2[index][0] < distanceMatrix2[index][1]:\n","    ax.scatter(X[index, 0], X[index, 1], edgecolor='red', c = 'w')\n","    overalDistToClusters2 += distanceMatrix2[index][0]\n","    totaldist=overalDistToClusters2+totaldist\n","  else:\n","    ax.scatter(X[index, 0], X[index, 1], edgecolor='blue', c = 'w')\n","    overalDistToClusters2 += distanceMatrix2[index][1]\n","print(totaldist)\n","\n","\n","# Plot the centers that we calculated for k-means in the previous cell\n","ax.scatter(centers2[0, 0], centers2[0, 1], color='red', marker = 'X', s= 100)\n","ax.scatter(centers2[1, 0], centers2[1, 1], color='blue', marker = 'X', s = 100)\n","\n","ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","ax.grid(alpha=0.3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"joJsdL7-Nkst"},"source":["---\n","> **Q1:** What is the total distance of instances to their cluster centres?"]},{"cell_type":"markdown","metadata":{"id":"48KPHV9mrKb1"},"source":["> **A1:**\n","---The toal distance of instances all together is 9194.307512943604"]},{"cell_type":"code","metadata":{"id":"30VRpFV9DAIO"},"source":["#np.random.seed(seed=2)\n","#if you uncomment the above seed code, then each run you will get different centre points \n","newCenters2 = np.random.normal(size=[k, 2]) + np.ones((2,1)) * np.mean(X, axis=0)\n","ncenters2 = np.random.normal(size=[k, 2]) + np.ones((2,1)) * np.mean(X, axis=0)\n","print(newCenters2)\n","print(ncenters2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qq1DCZk3ime9"},"source":["#### 3. Update two cluster centres\n","\n","* The next step in *k*-means is to update the cluster centers so that they move to the middle (mean of) their assigned points. \n","\n","* It' a good exercise to try and program this yourself.\n","\n","*Hint: if you cannot write the code after some effort, you can see the code being done for k=3 in a later cell (after step 6)!*"]},{"cell_type":"code","metadata":{"id":"qoWT1xfchTzY"},"source":["# update the cluster centers based on their assigned points\n","newCenters2 = np.zeros((2, 2))\n","\n","\n","# write your code here!\n","\n","tol = 0.000001\n","max_iteration = 100\n","difference = np.inf\n","iteration = 0\n","overalDistToClusters3 = [np.inf]\n","\n","k = 2 # set the k value of k-means\n","\n","newCenters2 = np.random.normal(size=[k, 2]) + np.ones((2,1)) * np.mean(X, axis=0)\n","print(newCenters2)\n","\n","\n","edgecolorlist_1 = ['red', 'blue']\n","\n","while difference>tol and iteration<max_iteration:\n","    \n","    fig = plt.figure(figsize=(7, 7))\n","    ax = fig.add_subplot(111)\n","    ax.set_aspect('equal')\n","    ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","    ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","    ax.grid(alpha=0.3)\n","    ax.set_xlim(4, 8)\n","    ax.set_ylim(1, 4.5)\n","    \n","    overalDistToClusters3_new = 0.0\n","    distanceMatrix2 = distance.cdist(X, newCenters2, 'euclidean')\n","    whichCenterNearest = np.argsort(distanceMatrix2, axis=1)[:, 0]\n","\n","    for index in range(len(X)):\n","        overalDistToClusters3_new += distanceMatrix2[index][whichCenterNearest[index]]\n","    difference = overalDistToClusters3[-1] - overalDistToClusters3_new\n","    overalDistToClusters3.append(overalDistToClusters3_new)\n","    \n","    for i in range(k):\n","        indx = whichCenterNearest==i\n","        if indx.any():\n","            newCenters2[i,:] = np.mean(X[indx,:], axis = 0)\n","    \n","        ax.scatter(X[indx, 0], X[indx, 1], edgecolor=edgecolorlist_1[i], c = 'w')\n","        ax.scatter(newCenters2[i, 0], newCenters2[i, 1], color=edgecolorlist_1[i], marker = 'X', s= 100, alpha=0.9) \n","\n","    iteration +=1\n","    ax.set_title('iteration = {:d}, difference = {:.4f}'.format(iteration, difference))\n","\n","#print(overalDistToClusters3_new)\n","plt.show()\n","print(newCenters2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DQk1VKNbrGbN"},"source":["---\n","> **Q2:** What are the new centers of clusters?"]},{"cell_type":"markdown","metadata":{"id":"WywbV88erNjl"},"source":["> **A2:**\n","---\n","The new centers of cluset is \n","\n","[[6.4775     2.935     ]\n","\n","\n"," [5.11857143 3.19714286]]"]},{"cell_type":"markdown","metadata":{"id":"z9bqjAuBrAuw"},"source":["* Now, let's plot the new centers of clusters within the data-points."]},{"cell_type":"code","metadata":{"id":"OPZZbTFAq8Aj"},"source":["fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","#newCenters2 = np.random.normal(size=[k, 2]) + np.ones((2,1)) * np.mean(X, axis=0)\n","newCenters2=newCenters2\n","# Make the labe of each point be the closest cluster.\n","totaldist=0\n","for index in range(len(X)):\n","  if distanceMatrix2[index][0] < distanceMatrix2[index][1]:\n","    ax.scatter(X[index, 0], X[index, 1], edgecolor='red', c = 'w')\n","  else:\n","    ax.scatter(X[index, 0], X[index, 1], edgecolor='blue', c = 'w')\n","  overalDistToClusters2 += distanceMatrix2[index][0]\n","  totaldist=overalDistToClusters2+totaldist\n","print(totaldist)\n","# Plot the centers that we calculated for k-means in the previous cell\n","ax.scatter(newCenters2[0, 0], newCenters2[0, 1], color='red', marker = 'X', s= 100)\n","ax.scatter(newCenters2[1, 0], newCenters2[1, 1], color='blue', marker = 'X', s = 100)\n","\n","ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","ax.grid(alpha=0.3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m011tw8ssO2r"},"source":["---\n","> **Q3:** What is the total distance of datapoints to the **new** clusters? (you may need to write a small code for this)"]},{"cell_type":"markdown","metadata":{"id":"8hBgyL4_sUeD"},"source":["> **A3:**\n","---34999.11475723345"]},{"cell_type":"markdown","metadata":{"id":"IGabWllIspms"},"source":["---\n","> **Q4:** Did the overall clustering quality improve (did the total distance decrease)?"]},{"cell_type":"markdown","metadata":{"id":"RkbKSjY8srKY"},"source":["> **A4:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"xAVeQQJnxWFR"},"source":["#### 4. Setup *k*-means when *k = 3*\n","\n","* This cell initializes *k*-means for k=3:"]},{"cell_type":"code","metadata":{"id":"WTGetKL2xi5M"},"source":["np.random.seed(seed = 3)\n","k = 3 # set the k value of k-means\n","\n","centers3 = np.random.normal(size=[k, 2]) + np.ones((3,1)) * np.mean(X, axis=0)\n","\n","print(centers3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cBaWMEZ50Fm_"},"source":["#### 5. Assign points to the three clusters\n","\n","* Similar to earliest cell, you should see points now assigned to nearest cluster."]},{"cell_type":"code","metadata":{"id":"MTRxZtNk0usP"},"source":["from scipy.spatial import distance\n","tol = 0.000001\n","max_iteration = 100\n","difference = np.inf\n","iteration = 0\n","overalDistToClusters3 = [np.inf]\n","\n","# Find the euclidean distance between every point and every cluster.\n","distanceMatrix3 = distance.cdist(X, centers3, 'euclidean')\n","\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","ax.set_aspect('equal')\n","\n","overalDistToClusters3 = 0.0\n","\n","# Make the labe of each point be the closest cluster.\n","whichCenterNearest = np.argsort(distanceMatrix3, axis=1)[:, 0]\n","\n","for index in range(len(X)):  \n","  overalDistToClusters3 += distanceMatrix3[index][whichCenterNearest[index]]\n","  if whichCenterNearest[index] == 0:\n","    ax.scatter(X[index, 0], X[index, 1], edgecolor='red', c = 'w')\n","  if whichCenterNearest[index] == 1:\n","    ax.scatter(X[index, 0], X[index, 1], edgecolor='blue', c = 'w')\n","  if whichCenterNearest[index] == 2:\n","    ax.scatter(X[index, 0], X[index, 1], edgecolor='green', c = 'w')\n","    \n","# Plot the centers that we calculated for k-means in the previous cell\n","ax.scatter(centers3[0, 0], centers3[0, 1], color='red', marker = 'X', s= 100)\n","ax.scatter(centers3[1, 0], centers3[1, 1], color='blue', marker = 'X', s = 100)\n","ax.scatter(centers3[2, 0], centers3[2, 1], color='green', marker = 'X', s = 100)\n","\n","ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","ax.grid(alpha=0.3)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYiA-RswtULy"},"source":["#### 6. Iterative updates of cluster centers when *k = 3*\n","\n","* Now, let's iteratively update the cluster centers. The stopping criterion is satisfied when either: \n","> (1) the difference between total distances of two consecutive iterations is less than `0.000001`; or,<br>\n","> (2) number of iterations is bigger than `max_iteration`."]},{"cell_type":"code","metadata":{"id":"uQcXDwm2CMy6"},"source":["# Find the euclidean distance between every point and every cluster.\n","\n","tol = 0.000001\n","max_iteration = 100\n","difference = np.inf\n","iteration = 0\n","overalDistToClusters3 = [np.inf]\n","\n","np.random.seed(seed = 3)\n","k = 7 #set the k value of k-means\n","\n","centers3 = np.random.normal(size=[k, 2]) + np.ones((k,1)) * np.mean(X, axis=0)\n","print (centers3)\n","\n","edgecolorlist = ['red', 'blue', 'green','black','magenta','yellow','cyan']\n","\n","while difference>tol and iteration<max_iteration:\n","    \n","    fig = plt.figure(figsize=(7, 7))\n","    ax = fig.add_subplot(111)\n","    ax.set_aspect('equal')\n","    ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","    ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","    ax.grid(alpha=0.3)\n","    ax.set_xlim(4, 8)\n","    ax.set_ylim(1, 4.5)\n","    \n","    overalDistToClusters3_new = 0.0\n","    distanceMatrix3 = distance.cdist(X, centers3, 'euclidean')\n","    whichCenterNearest = np.argsort(distanceMatrix3, axis=1)[:, 0]\n","\n","    for index in range(len(X)):\n","        overalDistToClusters3_new += distanceMatrix3[index][whichCenterNearest[index]]\n","    difference = overalDistToClusters3[-1] - overalDistToClusters3_new\n","    overalDistToClusters3.append(overalDistToClusters3_new)\n","    \n","    for i in range(k):\n","        indx = whichCenterNearest==i\n","        if indx.any():\n","            centers3[i,:] = np.mean(X[indx,:], axis = 0)\n","    \n","        ax.scatter(X[indx, 0], X[indx, 1], edgecolor=edgecolorlist[i], c = 'w')\n","        ax.scatter(centers3[i, 0], centers3[i, 1], color=edgecolorlist[i], marker = 'X', s= 100, alpha=0.9) \n","\n","    iteration +=1\n","    ax.set_title('iteration = {:d}, difference = {:.4f}'.format(iteration, difference))\n","\n","\n","#print ((overalDistToClusters3))\n","\n","print( overalDistToClusters3[1:])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jBUFNQajdWxh"},"source":["You should observe that in each iteration, the cluster quality visibly improves. To see this is one figure, let's plot the total distance from cluster centers versus iterations:"]},{"cell_type":"code","metadata":{"id":"QNIi0kkNL7o7"},"source":["\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","\n","ax.set_xlabel(\"Iteration\", fontsize=14)\n","ax.set_ylabel(\"Total cost\", fontsize=14)\n","ax.grid(alpha=0.3)\n","\n","# log y axis\n","# ax.semilogy(range(1,len(overalDistToClusters3)), overalDistToClusters3[1:], '--', marker = 'o')\n","\n","ax.plot(range(1,len(overalDistToClusters3)), overalDistToClusters3[1:], '--', marker = 'o')\n","ax.set_ylim([0, overalDistToClusters3[1]*1.05])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHHbL9hmfT2O"},"source":["#### 7. Repeat the experiment varying the value of K (the number of clusters) from 2 to 7.\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"3Lu3mEaUhjJe"},"source":["---\n","#### <font color='maroon'>**Exercise 1:** What do you observe about the dependence of the **final** cluster quality on the number of clusters K used? Why? <ins>[1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"gtTk1rNcZvPO"},"source":["Prss: As K increased to 7, the total disctance from the cluster has decreases. the final cluster cost is 45"]},{"cell_type":"code","metadata":{"id":"4WMQ8chKeWtD"},"source":["# Find the euclidean distance between every point and every cluster.\n","\n","tol = 0.000001\n","max_iteration = 100\n","difference = np.inf\n","iteration = 0\n","overalDistToClusters3 = [np.inf]\n","\n","np.random.seed(seed = 3)\n","k = 3 # set the k value of k-means\n","\n","centers3 = np.random.normal(size=[k, 2]) + np.ones((k,1)) * np.mean(X, axis=0)\n","print(centers3)\n","edgecolorlist = ['red', 'blue', 'green', 'orange','black','purple','pink']\n","\n","while difference>tol and iteration<max_iteration:\n","    \n","    fig = plt.figure(figsize=(7, 7))\n","    ax = fig.add_subplot(111)\n","    ax.set_aspect('equal')\n","    ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","    ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","    ax.grid(alpha=0.3)\n","    ax.set_xlim(4, 8)\n","    ax.set_ylim(1, 4.5)\n","    \n","    overalDistToClusters3_new = 0.0\n","    distanceMatrix3 = distance.cdist(X, centers3, 'euclidean')\n","    whichCenterNearest = np.argsort(distanceMatrix3, axis=1)[:, 0]\n","\n","    for index in range(len(X)):\n","        overalDistToClusters3_new += distanceMatrix3[index][whichCenterNearest[index]]\n","    difference = overalDistToClusters3[-1] - overalDistToClusters3_new\n","    overalDistToClusters3.append(overalDistToClusters3_new)\n","    \n","    for i in range(k):\n","        indx = whichCenterNearest==i\n","        if indx.any():\n","            centers3[i,:] = np.mean(X[indx,:], axis = 0)\n","    \n","        ax.scatter(X[indx, 0], X[indx, 1], edgecolor=edgecolorlist[i], c = 'w')\n","        ax.scatter(centers3[i, 0], centers3[i, 1], color=edgecolorlist[i], marker = 'X', s= 100, alpha=0.9) \n","\n","    iteration +=1\n","    ax.set_title('iteration = {:d}, difference = {:.4f}'.format(iteration, difference))\n","\n","\n","print ((overalDistToClusters3))\n","\n","print( overalDistToClusters3[1:])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aai0fd9bfDnU"},"source":["fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","\n","ax.set_xlabel(\"Iteration\", fontsize=14)\n","ax.set_ylabel(\"Total cost\", fontsize=14)\n","ax.grid(alpha=0.3)\n","\n","# log y axis\n","# ax.semilogy(range(1,len(overalDistToClusters3)), overalDistToClusters3[1:], '--', marker = 'o')\n","\n","ax.plot(range(1,len(overalDistToClusters3)), overalDistToClusters3[1:], '--', marker = 'o')\n","ax.set_ylim([0, overalDistToClusters3[1]*1.05])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yVG4ERl1h9UL"},"source":["#### 8. Set K back to 3. Find where in the code we are setting the randomisation seed: changing the seed will pick different initial centres. Try different parameters (positive integers) for the seed. What do you observe?\n"]},{"cell_type":"markdown","metadata":{"id":"MkDZPiB6tGn_"},"source":["---\n","#### <font color='maroon'>**Exercise 2:** Find a seed that gives  a different final quality of clusters (in terms of total distance). Include the values of the seed, the final distance and the picture of the cluster with your answer. <ins>[1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"VcioMu_jhmLU"},"source":["Change the seed to different number below"]},{"cell_type":"code","metadata":{"id":"157vtTs5gOac"},"source":["# Find the euclidean distance between every point and every cluster.\n","\n","tol = 0.000001\n","max_iteration = 100\n","difference = np.inf\n","iteration = 0\n","overalDistToClusters3 = [np.inf]\n","\n","np.random.seed(seed = 8)\n","k = 6 # set the k value of k-means\n","\n","centers3 = np.random.normal(size=[k, 2]) + np.ones((6,1)) * np.mean(X, axis=0)\n","\n","edgecolorlist = ['red', 'blue', 'green', 'orange','black','purple']#,'pink']\n","\n","while difference>tol and iteration<max_iteration:\n","    \n","    fig = plt.figure(figsize=(7, 7))\n","    ax = fig.add_subplot(111)\n","    ax.set_aspect('equal')\n","    ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","    ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","    ax.grid(alpha=0.3)\n","    ax.set_xlim(4, 8)\n","    ax.set_ylim(1, 4.5)\n","    \n","    overalDistToClusters3_new = 0.0\n","    distanceMatrix3 = distance.cdist(X, centers3, 'euclidean')\n","    whichCenterNearest = np.argsort(distanceMatrix3, axis=1)[:, 0]\n","\n","    for index in range(len(X)):\n","        overalDistToClusters3_new += distanceMatrix3[index][whichCenterNearest[index]]\n","    difference = overalDistToClusters3[-1] - overalDistToClusters3_new\n","    overalDistToClusters3.append(overalDistToClusters3_new)\n","    \n","    for i in range(k):\n","        indx = whichCenterNearest==i\n","        if indx.any():\n","            centers3[i,:] = np.mean(X[indx,:], axis = 0)\n","    \n","        ax.scatter(X[indx, 0], X[indx, 1], edgecolor=edgecolorlist[i], c = 'w')\n","        ax.scatter(centers3[i, 0], centers3[i, 1], color=edgecolorlist[i], marker = 'X', s= 100, alpha=0.9) \n","\n","    iteration +=1\n","    ax.set_title('iteration = {:d}, difference = {:.4f}'.format(iteration, difference))\n","\n","\n","plt.show()\n","\n","fig = plt.figure(figsize=(7, 7))\n","ax = fig.add_subplot(111)\n","\n","ax.set_xlabel(\"Iteration\", fontsize=14)\n","ax.set_ylabel(\"Total cost\", fontsize=14)\n","ax.grid(alpha=0.3)\n","\n","# log y axis\n","# ax.semilogy(range(1,len(overalDistToClusters3)), overalDistToClusters3[1:], '--', marker = 'o')\n","\n","ax.plot(range(1,len(overalDistToClusters3)), overalDistToClusters3[1:], '--', marker = 'o')\n","ax.set_ylim([0, overalDistToClusters3[1]*1.05])\n","\n","plt.show()\n","\n","print ((overalDistToClusters3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XuAtMgjVvZ_O"},"source":["#### 9. Try to change the definition of distance used for clustering. For instance, try the distance types of `cityblock` or `cosine`."]},{"cell_type":"markdown","metadata":{"id":"r6aX2sAevsbd"},"source":[" ---\n","> **Q5:** What do you observe about difference in the resulting clustering?"]},{"cell_type":"markdown","metadata":{"id":"iCY6FEcgv07W"},"source":["> **A5:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"7pBLQvDKwTkq"},"source":["#### 10. The goal of clustering is to discover clusters in the feature space. Since we have the labels for each sample, we can use them to investigate whether the clusters identify regions in space that are associated to one of the labels.\n","\n","The following code compares every possible assignment of clusters to true class labels. By doing so, we are evaluating a possible match between grouping of data points into clusters and the true labels."]},{"cell_type":"code","metadata":{"id":"GG1MXlnWMaIl"},"source":["from itertools import permutations\n","\n","def compute_clustering_accuracy(whichCenterNearest,  Y):\n","\n","  accuracies = []\n","  for mapping in permutations(range(3)):\n","    whichCenterNearestmapped = [mapping[x] for x in whichCenterNearest]\n","    accuracies.append(np.mean(whichCenterNearestmapped==Y))\n","  accuracy = max(accuracies)\n","  return accuracy\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B-vXNWl0w7Bl"},"source":[" ---\n","> **Q6:** Use the above function with proper parameters to answer this question: What clustering accuracy do you get? (Notice the difference between\n","accuracy, where more is better, and quality, where lower distance to the cluster centre is better!)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ho2c6hxdxW4J"},"source":["> **A6:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"O55VcjQbxsde"},"source":["#### 11. Run the following code. In the cluster display figure, you should see that the cluster  boundaries shown are no longer as clean as before. The colors  indicating cluster are mixed up. Why is this?"]},{"cell_type":"code","metadata":{"id":"2jElHTwKiQSg"},"source":["newX = iris.data\n","d = len(newX[0])\n","#print(d)\n","# newX[0] this will take all 4 attributes of the flower\n","#print(newX[0])\n","np.random.seed(seed = 3)\n","k = 3 # set the k value of k-means\n","\n","centers3 =  np.random.normal(size=[k, d]) + np.ones((k,1)) * np.mean(newX, axis=0)\n","print(centers3)\n","# Find the euclidean distance between every point and every cluster.\n","tol = 0.000001\n","max_iteration = 100\n","difference = np.inf\n","iteration = 0\n","overalDistToClusters3 = [np.inf]\n","\n","edgecolorlist = ['red', 'blue', 'green','cyan']\n","\n","while difference>tol and iteration<max_iteration:\n","    \n","    fig = plt.figure(figsize=(7, 7))\n","    ax = fig.add_subplot(111)\n","    ax.set_aspect('equal')\n","    ax.set_xlabel(iris.feature_names[0], fontsize=14)\n","    ax.set_ylabel(iris.feature_names[1], fontsize=14)\n","    ax.grid(alpha=0.3)\n","    ax.set_xlim(4, 8)\n","    ax.set_ylim(1, 4.5)\n","    \n","    overalDistToClusters3_new = 0.0\n","    distanceMatrix3 = distance.cdist(newX, centers3, 'euclidean')\n","    whichCenterNearest = np.argsort(distanceMatrix3, axis=1)[:, 0]\n","\n","    for index in range(len(X)):\n","        overalDistToClusters3_new += distanceMatrix3[index][whichCenterNearest[index]]\n","    difference = overalDistToClusters3[-1] - overalDistToClusters3_new\n","    overalDistToClusters3.append(overalDistToClusters3_new)\n","    \n","    for i in range(k):\n","        indx = whichCenterNearest==i\n","        if indx.any():\n","            centers3[i,:] = np.mean(newX[indx,:], axis = 0)\n","    \n","        ax.scatter(newX[indx, 0], newX[indx, 1], edgecolor=edgecolorlist[i], c = 'w')\n","        ax.scatter(centers3[i, 0], centers3[i, 1], color=edgecolorlist[i], marker = 'X', s= 100, alpha=0.9) \n","\n","    iteration +=1\n","    ax.set_title('iteration = {:d}, difference = {:.4f}'.format(iteration, difference))\n","\n","print(overalDistToClusters3_new)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9C6qmGCrx59n"},"source":["---\n","#### <font color='maroon'>**Exercise 2:** Has the clustering accuracy improved from before? Why? [1 mark] </ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"Zzi1zjBDzgGt"},"source":["Edit the visualization to visualize\n","three of the K-means clustering dimensions instead of just 2"]},{"cell_type":"markdown","metadata":{"id":"XCFzThh-kwKd"},"source":["PRss: The number of attributes increased from 2 to 4"]},{"cell_type":"markdown","metadata":{"id":"ZNe6BQVjYPK2"},"source":["## **2. Dimensionality Reduction using PCA**\n"]},{"cell_type":"markdown","metadata":{"id":"r-7mKu6dY1Z0"},"source":["**1. Load the dataset**\n","\n","*   This first cell loads the `Yale_64x64` dataset, which is a face database. Details are here: http://vision.ucsd.edu/content/yale-face-database.\n","\n","The Yale_64*64.mat file is in lab4.zip file in QMplus\n"]},{"cell_type":"code","metadata":{"id":"3ZGLOPEnDSem"},"source":["import numpy as np\n","from scipy.io import loadmat\n","\n","# load images from the mat file\n","mat_contents = loadmat('Yale_64x64.mat')\n","faces = mat_contents[\"fea\"]\n","labels = mat_contents[\"gnd\"]\n","print(labels.base)\n","print(faces.shape)\n","print(labels.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ExIOvtmSgE6"},"source":["* The loaded face database is now in a numpy matrix called \"faces\". As you can see from its shape, it has 165 rows, and 4096 columns. \n","Each row corresponds to a face image. Each column represents a (grayscale) pixel of the image. \n","\n","  Images are 64x64 pixles, so each row of our dataset has 4096 columns."]},{"cell_type":"markdown","metadata":{"id":"Hi_IgvWwSOUb"},"source":["* We can display the images by *reshaping* each of the 1x4096 vectors to a 64x64 matrix. This can be done easily by using `np.reshape`, as follows:"]},{"cell_type":"code","metadata":{"id":"xhjq_-b8S7WN"},"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(15,15))\n","\n","for i in range(165):\n","  img = np.reshape(faces[i,:], (64, 64)).T\n","  ax = fig.add_subplot(15, 11, i+1)\n","  plt.imshow(img, cmap=plt.get_cmap('gray'))\n","  ax.tick_params(labelleft=False, labelbottom=False)\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VW1oaNpFVDHd"},"source":["---\n","> **Q7:** Beside the `faces` variable, we also loaded a `labels` variable (whose size is 165x1). What information does it hold?"]},{"cell_type":"markdown","metadata":{"id":"p75skgJsVRTD"},"source":["> **A7:**\n","---prss: Labels are pictures of individuals. 11 pictures of each individuals"]},{"cell_type":"markdown","metadata":{"id":"dCV87zBu2poC"},"source":["**2. Find a basis for the faces (eigenvectors of the covariance matrix).**\n","\n","The following cell takes the SVD (Singular Value Decomposition) of the data. The `U` output of the SVD is the eigenvectors of the\n","data covariance required for PCA. This contains the basis vectors / prototype faces in terms of\n","which all other faces are to be encoded (every face will be a linear combination of these\n","prototypes). The basis vactors constitute our principal components. Notice that some basis vectors encode lighting, others features such as glasses and a mustache!"]},{"cell_type":"code","metadata":{"id":"wat-N8AEy6yZ"},"source":["# SVD gives Eigenvectors and Eigenvals:\n","U, S, Vh = np.linalg.svd(faces.T, full_matrices=True) \n","\n","print(U.shape)\n","\n","# We also compute a picture in which each pixel is the \"average\" of that pixel \n","# across all 165 images:\n","meanFaces = np.mean(faces, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t_1cKLCOvMa5"},"source":["Now let's plot the basis as well as the average face. \n","Recall that the interpretation of the principal components is the following: the first principal component explains the highest variance, then the second one explains the remaining variance, and so on and so forth. We have only depicted the first 15 principal components. "]},{"cell_type":"code","metadata":{"id":"OwZgCdCd46zT"},"source":["img = np.reshape(meanFaces, (64, 64)).T\n","fig = plt.figure(figsize=(15,15))\n","fig.add_subplot(4, 4, 1)\n","plt.imshow(img, cmap=plt.get_cmap('gray'))\n","plt.axis('off')\n","\n","  \n","for image in range(15):\n","  img = np.reshape(U[:, image], (64, 64)).T\n","  fig.add_subplot(4, 4, image + 2)\n","  plt.imshow(img, cmap=plt.get_cmap('gray'))\n","  plt.axis('off')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I32d3kQH80Fb"},"source":["**3. PCA Encoding and Reconstruction.**\n","\n","* In the next cell, we are using the PCA encoding of each face to reconstruct it (we only use the first 25 principal components).\n","\n","* Recall that for a D-column input matrix/database, the goal of PCA is to construct a K < D\n","column encoding that most accurately encodes the data in D.\n","\n","* You can see the original database `faces`, and the compressed database `pcaFaces`. You\n","can compare their size with `faces.shape` and `pcaFaces.shape`. You can see the\n","compressed version has only K=25 columns compared to the original D=4096 columns."]},{"cell_type":"code","metadata":{"id":"GOtKBngm_lJ1"},"source":["from numpy import linalg as LA\n","import matplotlib.gridspec as gridspec\n","from google.colab import widgets\n","\n","\n","nPCA = 400\n"," # how many pricipal components to use\n","N = faces.shape[0] # the number of total images\n","\n","# PCA Encoding. Raw images (faces) => compressed images (pcaFaces).\n","pcaFaces = np.matmul(U[:, 0:nPCA].T, faces.T - np.repeat(meanFaces, N).reshape(len(meanFaces), N))\n","\n","# PCA Decoding. Compressed images pcaFaces => Raw images reconstrFaces.\n","reconstrFaces = np.matmul(U[:, 0:nPCA], pcaFaces) + np.repeat(meanFaces, N).reshape(len(meanFaces), N)\n","\n","\n","grid = widgets.Grid(rows=4, columns=3)\n","\n","for row in range(4):\n","  for col in range(3):\n","    with grid.output_to(row, col):\n","      fig = plt.figure(figsize=(5,5))\n","      ax = fig.add_subplot(1, 2, 1)\n","      image_index = row*3 + col + 1\n","      img = np.reshape(faces[image_index * 10 - 1], (64, 64)).T\n","      plt.imshow(img, cmap=plt.get_cmap('gray'))\n","      plt.xlabel('original')\n","      ax.set_xticks([])\n","      ax.set_yticks([])\n","      ax.set_xticklabels([])\n","      ax.set_yticklabels([])\n","      \n","      img = np.reshape(reconstrFaces[:, image_index * 10 - 1], (64, 64)).T\n","      ax = fig.add_subplot(1,2,2)\n","      plt.imshow(img, cmap=plt.get_cmap('gray'))\n","      plt.xlabel('reconstructed')\n","      ax.set_xticks([])\n","      ax.set_yticks([])\n","      ax.set_xticklabels([])\n","      ax.set_yticklabels([])\n","\n","\n","\n","print(faces.shape)\n","print(pcaFaces.shape)\n","\n","print('original size: {} (KB)'.format(faces.size*8/1000))\n","print('reduced size: {} (KB)'.format(pcaFaces.size*8/1000))\n","error = LA.norm(reconstrFaces.flatten()-faces.T.flatten())**2/LA.norm(faces.flatten())**2\n","print('Reconstruction error for nPCA={} is: {}'.format(nPCA, error))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1v-nYBgWAzL"},"source":["* Starting from 1 dimensional encoding, increase the dimensions (set variable `nPCA`) and re-run the cell, observing how the facial encoding fidelity increases. For what number of encoding dimensions can you see features like glasses and facial expressions?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2-wNNfbATHAg"},"source":["---\n","> **Q8:** Following the above procedure, find out how many PCs (Principal Components) do you need to reach an encoding fidelity of 99%? (<1% reconstruction error). Compare the size in bytes of original and PCA data at this point.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XoS0yjFdT1rg"},"source":["> **A8:**\n","---"]},{"cell_type":"markdown","metadata":{"id":"sexvFPB3TLP4"},"source":["* Note that when using the full number (4096) of PCs, the encoding fidelity is 100%. Using all the PCs conveys exactly the same information as the original data. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"FspkczEIb6Gw"},"source":["**4. Using eigenvalues:**\n","\n","The PCA process produces eigenvectors and eigenvalues. The eigenvectors give the new basis\n","(i.e., define the new database columns), and the eigenvalues explain how useful each column is\n","for encoding the data."]},{"cell_type":"markdown","metadata":{"id":"T0uaMI-DcYmF"},"source":["* Let's look at the (square of the) eigenvalues of the basis:"]},{"cell_type":"code","metadata":{"id":"e-SYhXZ3ch1r"},"source":["eigvals = np.square(S)\n","plt.plot(eigvals, linewidth=3)\n","plt.grid()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5oT8fChckBW"},"source":["The x-axis identifies each dimension in the PCA space, and the Y-axis is the eigenvalue / information content.\n","\n","* The first few dimensions are by far the most informative.\n"]},{"cell_type":"markdown","metadata":{"id":"Rz4o3paCf_h1"},"source":["Make this a cumulative plot to see how much of the overall variance is explained (encoded by) per each number of dimensions, i.e., how much each new dimension contributes to the reconstruction fidelity: "]},{"cell_type":"code","metadata":{"id":"4MrTfCPugeTJ"},"source":["plt.plot(np.cumsum(eigvals)/sum(eigvals), linewidth=3)\n","plt.xlabel('Dimensions')\n","plt.ylabel('Reconstruction Accuracy')\n","plt.grid()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKWhMwodWdL1"},"source":["---\n","#### <font color='maroon'>**Exercise 4:** Use the data plotted above to find out what number of PCs are required to explain 99% of the data variance (achieve 99% reconstruction accuracy). What number is this? Does it match the value obtained from **Q8**? Provide a short discussion. <ins>[1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"9o-Aq_3exTGc"},"source":["Based on graph above 55 dimentions provide accuracy of 99%.\n","\n","Yes, Q8 also gives 99% accuracy with 55 dimesnsion"]},{"cell_type":"markdown","metadata":{"id":"UiV7FU6Ui64H"},"source":["**5. Classification using reduced dimensions:**\n"]},{"cell_type":"markdown","metadata":{"id":"tj7tL58OW-2o"},"source":["Now lets try to recognize the faces using a simple KNN classifier.\n","* There are 15 people in this dataset. The following cell splits the data into train and test, and runs a 1-NN classifer on the test (using the train data):\n"]},{"cell_type":"code","metadata":{"id":"VZCpJqwVjHF5"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","nPCA = 42\n","# how many pricipal components to use\n","N = faces.shape[0] # the number of total images\n","\n","# PCA Encoding. Raw images (faces) => compressed images (pcaFaces).\n","pcaFaces = np.matmul(U[:, 0:nPCA].T, faces.T - np.repeat(meanFaces, N).reshape(len(meanFaces), N))\n","\n","xTr = pcaFaces.T[::2,:]\n","yTr = labels[::2]\n","\n","xTe = pcaFaces.T[1::2,:]\n","yTe = labels[1::2]\n","\n","neigh = KNeighborsClassifier(n_neighbors=1).fit(xTr, yTr.ravel()) \n","print('Accuracy score of 1-NN when nPCA = {} is {:.3f}'.format(nPCA, neigh.score(xTe,yTe)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlCPxvPJjJsd"},"source":["* Change nPCA. Observe that classification using different numbers of PCA dimensions\n","produces different results."]},{"cell_type":"markdown","metadata":{"id":"IGdLjSRhWdxf"},"source":["---\n","#### <font color='maroon'>**Exercise 5:** Which number of PCA dimensions gets the maximum face recognition accuracy? Is it better or worse than the accuracy obtained when classifying the raw images? Why? (What factors contribute to this?) Provide a brief discussion.<ins> [1 mark]</ins></font>\n","---"]},{"cell_type":"markdown","metadata":{"id":"wLBW4bUqUNQ3"},"source":["PRss: The PCA dimension at 42 gets a maximum face recognition accuracy 70.7%.\n","\n","The accuracy has improved from 67% at 4096 nPCA to 70.7% at 42 npca because in raw images there were unimportant features which not helping to classify the pictures."]},{"cell_type":"markdown","metadata":{"id":"tI6GM4Xlil6j"},"source":["* *Note: Accuracy was used in two different contexts in the above: (i) reconstruction accuracy (unsupervised learning task: How accurately an image is encoded after compressing away some of the columns with PCA) , and (ii) face recognition accuracy in the last task (Supervised learning task: How accurately can we recognize faces given raw image or PCA compressed image).*"]}]}